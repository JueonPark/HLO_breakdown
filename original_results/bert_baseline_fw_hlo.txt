HloModule cluster_0__XlaCompiledKernel_true__XlaHasReferenceVars_false__XlaNumConstantArgs_173__XlaNumResourceArgs_52_.2592

%max_half_.303 (x.304: f16[], y.305: f16[]) -> f16[] {
  %x.304 = f16[] parameter(0)
  %y.305 = f16[] parameter(1)
  ROOT %maximum.306 = f16[] maximum(f16[] %x.304, f16[] %y.305)
}

%add_float_.313 (x.314: f32[], y.315: f32[]) -> f32[] {
  %x.314 = f32[] parameter(0)
  %y.315 = f32[] parameter(1)
  ROOT %add.316 = f32[] add(f32[] %x.314, f32[] %y.315)
}

%max_half_.479 (x.480: f16[], y.481: f16[]) -> f16[] {
  %x.480 = f16[] parameter(0)
  %y.481 = f16[] parameter(1)
  ROOT %maximum.482 = f16[] maximum(f16[] %x.480, f16[] %y.481)
}

%add_float_.489 (x.490: f32[], y.491: f32[]) -> f32[] {
  %x.490 = f32[] parameter(0)
  %y.491 = f32[] parameter(1)
  ROOT %add.492 = f32[] add(f32[] %x.490, f32[] %y.491)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_softmax_Sum-reduction.1341 (x.1342: f32[], y.1343: f32[]) -> f32[] {
  %x.1342 = f32[] parameter(0)
  %y.1343 = f32[] parameter(1)
  ROOT %add.1344 = f32[] add(f32[] %x.1342, f32[] %y.1343)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_softmax_Sum-reduction.1599 (x.1600: f32[], y.1601: f32[]) -> f32[] {
  %x.1600 = f32[] parameter(0)
  %y.1601 = f32[] parameter(1)
  ROOT %add.1602 = f32[] add(f32[] %x.1600, f32[] %y.1601)
}

%Mean-reduction.942 (x.943: f32[], y.944: f32[]) -> f32[] {
  %x.943 = f32[] parameter(0)
  %y.944 = f32[] parameter(1)
  ROOT %add.945 = f32[] add(f32[] %x.943, f32[] %y.944)
}

%fused_computation (param_0: f32[], param_1.1314: f32[], param_2.886: f32[], param_3.686: f32[], param_4.536: f32[], param_5.457: f32[], param_6.524: f32[], param_7.448: f32[]) -> (f32[], f32[], f32[], f32[]) {
  %param_0 = f32[] parameter(0)
  %param_1.1314 = f32[] parameter(1)
  %constant_70 = f32[] constant(0.0625)
  %multiply.326 = f32[] multiply(f32[] %param_1.1314, f32[] %constant_70), metadata={op_type="Mean" op_name="model/bert_pretrain_loss_and_metric_layer/Mean"}
  %param_3.686 = f32[] parameter(3)
  %constant_71 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %compare.118 = pred[] compare(f32[] %param_3.686, f32[] %constant_71), direction=EQ, metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %param_2.886 = f32[] parameter(2)
  %divide.88 = f32[] divide(f32[] %param_2.886, f32[] %param_3.686), metadata={op_type="DivNoNan" op_name="model/bert_pretrain_loss_and_metric_layer/div_no_nan"}
  %select.114 = f32[] select(pred[] %compare.118, f32[] %constant_71, f32[] %divide.88), metadata={op_type="DivNoNan" op_name="model/bert_pretrain_loss_and_metric_layer/div_no_nan"}
  %add.8 = f32[] add(f32[] %multiply.326, f32[] %select.114), metadata={op_type="AddV2" op_name="model/bert_pretrain_loss_and_metric_layer/add"}
  %broadcast.223 = f32[16]{0} broadcast(f32[] %add.8), dimensions={}
  %reduce.112 = f32[] reduce(f32[16]{0} %broadcast.223, f32[] %constant_71), dimensions={0}, to_apply=%Mean-reduction.942, metadata={op_type="Mean" op_name="Mean"}
  %multiply.95 = f32[] multiply(f32[] %reduce.112, f32[] %constant_70), metadata={op_type="Mean" op_name="Mean"}
  %add.7 = f32[] add(f32[] %param_0, f32[] %multiply.95), metadata={op_type="AssignAddVariableOp" op_name="AssignAddVariableOp"}
  %param_4.536 = f32[] parameter(4)
  %add.456.clone.1 = f32[] add(f32[] %param_4.536, f32[] %select.114), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_2"}
  %param_5.457 = f32[] parameter(5)
  %add.85.clone.1 = f32[] add(f32[] %param_5.457, f32[] %multiply.326), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_6"}
  %param_6.524 = f32[] parameter(6)
  %param_7.448 = f32[] parameter(7)
  %constant_66_clone_1 = f32[] constant(1e-05), metadata={op_type="AddV2" op_name="model/bert_pretrain_loss_and_metric_layer/add_1"}
  %add.10.clone.1 = f32[] add(f32[] %param_3.686, f32[] %constant_66_clone_1), metadata={op_type="AddV2" op_name="model/bert_pretrain_loss_and_metric_layer/add_1"}
  %divide.0.clone.1 = f32[] divide(f32[] %param_7.448, f32[] %add.10.clone.1), metadata={op_type="RealDiv" op_name="model/bert_pretrain_loss_and_metric_layer/truediv"}
  %add.9.clone.1 = f32[] add(f32[] %param_6.524, f32[] %divide.0.clone.1), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp"}
  ROOT %tuple.30 = (f32[], f32[], f32[], f32[]) tuple(f32[] %add.7, f32[] %add.456.clone.1, f32[] %add.85.clone.1, f32[] %add.9.clone.1)
}

%min_S64.804 (lhs.805: s64[], rhs.806: s64[]) -> s64[] {
  %lhs.805 = s64[] parameter(0)
  %rhs.806 = s64[] parameter(1)
  ROOT %minimum.807 = s64[] minimum(s64[] %lhs.805, s64[] %rhs.806)
}

%fused_computation.3 (param_0.1186: f32[16,76], param_1.1325: f32[30522], param_2.894: f16[1216,30528]) -> s64[16,76] {
  %param_2.894 = f16[1216,30528]{1,0} parameter(2)
  %slice.37 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_2.894), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_1.1325 = f32[30522]{0} parameter(1)
  %convert.671 = f16[30522]{0} convert(f32[30522]{0} %param_1.1325), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.1800 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.671), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.461 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.37, f16[1216,30522]{1,0} %broadcast.1800), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.670 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.461), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %bitcast.472 = f32[16,76,30522]{2,1,0} bitcast(f32[1216,30522]{1,0} %convert.670), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_1"}
  %param_0.1186 = f32[16,76]{1,0} parameter(0)
  %broadcast.226 = f32[16,76,30522]{2,1,0} broadcast(f32[16,76]{1,0} %param_0.1186), dimensions={0,1}, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %compare.1 = pred[16,76,30522]{2,1,0} compare(f32[16,76,30522]{2,1,0} %bitcast.472, f32[16,76,30522]{2,1,0} %broadcast.226), direction=EQ, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %iota.0 = s64[16,76,30522]{2,1,0} iota(), iota_dimension=2, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %constant_73 = s64[] constant(9223372036854775807), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %broadcast.225 = s64[16,76,30522]{2,1,0} broadcast(s64[] %constant_73), dimensions={}, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %select.20 = s64[16,76,30522]{2,1,0} select(pred[16,76,30522]{2,1,0} %compare.1, s64[16,76,30522]{2,1,0} %iota.0, s64[16,76,30522]{2,1,0} %broadcast.225), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  ROOT %reduce.114 = s64[16,76]{1,0} reduce(s64[16,76,30522]{2,1,0} %select.20, s64[] %constant_73), dimensions={2}, to_apply=%min_S64.804, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
}

%add_float_.763 (x.764: f32[], y.765: f32[]) -> f32[] {
  %x.764 = f32[] parameter(0)
  %y.765 = f32[] parameter(1)
  ROOT %add.766 = f32[] add(f32[] %x.764, f32[] %y.765)
}

%fused_computation.7 (param_0.1063: f32[1216], param_1.1166: s32[16,76], param_2.706: f32[1216], param_3.484: f32[30522], param_4.339: f16[1216,30528]) -> f32[1216] {
  %param_1.1166 = s32[16,76]{1,0} parameter(1)
  %convert.271 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %param_1.1166), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_2"}
  %convert.270 = s64[16,76]{1,0} convert(f32[16,76]{1,0} %convert.271), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %bitcast.318 = s64[1216]{0} bitcast(s64[16,76]{1,0} %convert.270), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %broadcast.608 = s64[1216,30522]{1,0} broadcast(s64[1216]{0} %bitcast.318), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.6 = s64[1216,30522]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.35 = pred[1216,30522]{1,0} compare(s64[1216,30522]{1,0} %broadcast.608, s64[1216,30522]{1,0} %iota.6), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_418 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.607 = f32[1216,30522]{1,0} broadcast(f32[] %constant_418), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_76 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.606 = f32[1216,30522]{1,0} broadcast(f32[] %constant_76), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.50 = f32[1216,30522]{1,0} select(pred[1216,30522]{1,0} %compare.35, f32[1216,30522]{1,0} %broadcast.607, f32[1216,30522]{1,0} %broadcast.606), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_416 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.604 = s64[1216]{0} broadcast(s64[] %constant_416), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.34 = pred[1216]{0} compare(s64[1216]{0} %broadcast.604, s64[1216]{0} %bitcast.318), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_414 = s64[] constant(30522), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.603 = s64[1216]{0} broadcast(s64[] %constant_414), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.33 = pred[1216]{0} compare(s64[1216]{0} %bitcast.318, s64[1216]{0} %broadcast.603), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %and.93 = pred[1216]{0} and(pred[1216]{0} %compare.34, pred[1216]{0} %compare.33), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.602 = f32[1216]{0} broadcast(f32[] %constant_76), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_413 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.601 = f32[1216]{0} broadcast(f32[] %constant_413), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.49 = f32[1216]{0} select(pred[1216]{0} %and.93, f32[1216]{0} %broadcast.602, f32[1216]{0} %broadcast.601), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.599 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %select.49), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %add.93 = f32[1216,30522]{1,0} add(f32[1216,30522]{1,0} %select.50, f32[1216,30522]{1,0} %broadcast.599), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %negate.0 = f32[1216,30522]{1,0} negate(f32[1216,30522]{1,0} %add.93), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_4.339 = f16[1216,30528]{1,0} parameter(4)
  %slice.23 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_4.339), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_3.484 = f32[30522]{0} parameter(3)
  %convert.437 = f16[30522]{0} convert(f32[30522]{0} %param_3.484), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.1321 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.437), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.300 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.23, f16[1216,30522]{1,0} %broadcast.1321), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.436 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.300), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %param_2.706 = f32[1216]{0} parameter(2)
  %broadcast.1320 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_2.706), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.83 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %convert.436, f32[1216,30522]{1,0} %broadcast.1320), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.1063 = f32[1216]{0} parameter(0)
  %broadcast.227 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_0.1063), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.1 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %subtract.83, f32[1216,30522]{1,0} %broadcast.227), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.97 = f32[1216,30522]{1,0} multiply(f32[1216,30522]{1,0} %negate.0, f32[1216,30522]{1,0} %subtract.1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %reduce.116 = f32[1216]{0} reduce(f32[1216,30522]{1,0} %multiply.97, f32[] %constant_76), dimensions={1}, to_apply=%add_float_.763, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
}

%model_bert_pretrain_loss_and_metric_layer_Sum_6-reduction.862 (x.863: f32[], y.864: f32[]) -> f32[] {
  %x.863 = f32[] parameter(0)
  %y.864 = f32[] parameter(1)
  ROOT %add.865 = f32[] add(f32[] %x.863, f32[] %y.864)
}

%fused_computation.8 (param_0.13: f32[], param_1.637: s32[16,1], param_2.253: s64[16]) -> f32[] {
  %param_0.13 = f32[] parameter(0)
  %param_2.253 = s64[16]{0} parameter(2)
  %convert.28 = s32[16]{0} convert(s64[16]{0} %param_2.253), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_11"}
  %param_1.637 = s32[16,1]{1,0} parameter(1)
  %bitcast.168 = s32[16]{0} bitcast(s32[16,1]{1,0} %param_1.637), metadata={op_type="Squeeze" op_name="model/bert_pretrain_loss_and_metric_layer/Squeeze"}
  %compare.2 = pred[16]{0} compare(s32[16]{0} %convert.28, s32[16]{0} %bitcast.168), direction=EQ, metadata={op_type="Equal" op_name="model/bert_pretrain_loss_and_metric_layer/Equal_1"}
  %convert.27 = f32[16]{0} convert(pred[16]{0} %compare.2), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_12"}
  %constant_77 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.117 = f32[] reduce(f32[16]{0} %convert.27, f32[] %constant_77), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum_6-reduction.862, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_6"}
  ROOT %add.11 = f32[] add(f32[] %param_0.13, f32[] %reduce.117), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_4"}
}

%min_S64.851 (lhs.852: s64[], rhs.853: s64[]) -> s64[] {
  %lhs.852 = s64[] parameter(0)
  %rhs.853 = s64[] parameter(1)
  ROOT %minimum.854 = s64[] minimum(s64[] %lhs.852, s64[] %rhs.853)
}

%fused_computation.9 (param_0.1094: f32[16], param_1.1196: f32[2], param_2.748: f16[16,8]) -> s64[16] {
  %param_2.748 = f16[16,8]{1,0} parameter(2)
  %slice.27 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_2.748), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_1.1196 = f32[2]{0} parameter(1)
  %convert.506 = f16[2]{0} convert(f32[2]{0} %param_1.1196), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.1412 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.506), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.318 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.27, f16[16,2]{1,0} %broadcast.1412), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.29 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.318), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %param_0.1094 = f32[16]{0} parameter(0)
  %broadcast.337 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_0.1094), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.3 = pred[16,2]{1,0} compare(f32[16,2]{1,0} %convert.29, f32[16,2]{1,0} %broadcast.337), direction=EQ, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %iota.1 = s64[16,2]{1,0} iota(), iota_dimension=1, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %constant_78 = s64[] constant(9223372036854775807), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %broadcast.228 = s64[16,2]{1,0} broadcast(s64[] %constant_78), dimensions={}, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %select.22 = s64[16,2]{1,0} select(pred[16,2]{1,0} %compare.3, s64[16,2]{1,0} %iota.1, s64[16,2]{1,0} %broadcast.228), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  ROOT %reduce.118 = s64[16]{0} reduce(s64[16,2]{1,0} %select.22, s64[] %constant_78), dimensions={1}, to_apply=%min_S64.851, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
}

%model_bert_pretrain_loss_and_metric_layer_Mean-reduction.927 (x.928: f32[], y.929: f32[]) -> f32[] {
  %x.928 = f32[] parameter(0)
  %y.929 = f32[] parameter(1)
  ROOT %add.930 = f32[] add(f32[] %x.928, f32[] %y.929)
}

%fused_computation.10 (param_0.1102: f32[16], param_1.1207: s32[16,1], param_2.760: f32[16], param_3.542: f32[2], param_4.397: f16[16,8]) -> f32[] {
  %param_1.1207 = s32[16,1]{1,0} parameter(1)
  %convert.498 = f32[16,1]{1,0} convert(s32[16,1]{1,0} %param_1.1207), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_5"}
  %convert.497 = s64[16,1]{1,0} convert(f32[16,1]{1,0} %convert.498), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %bitcast.366 = s64[16]{0} bitcast(s64[16,1]{1,0} %convert.497), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %broadcast.1392 = s64[16,2]{1,0} broadcast(s64[16]{0} %bitcast.366), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.14 = s64[16,2]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.91 = pred[16,2]{1,0} compare(s64[16,2]{1,0} %broadcast.1392, s64[16,2]{1,0} %iota.14), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_968 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1391 = f32[16,2]{1,0} broadcast(f32[] %constant_968), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_80 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.1390 = f32[16,2]{1,0} broadcast(f32[] %constant_80), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.90 = f32[16,2]{1,0} select(pred[16,2]{1,0} %compare.91, f32[16,2]{1,0} %broadcast.1391, f32[16,2]{1,0} %broadcast.1390), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_966 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1389 = s64[16]{0} broadcast(s64[] %constant_966), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.90 = pred[16]{0} compare(s64[16]{0} %broadcast.1389, s64[16]{0} %bitcast.366), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_965 = s64[] constant(2), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1388 = s64[16]{0} broadcast(s64[] %constant_965), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.89 = pred[16]{0} compare(s64[16]{0} %bitcast.366, s64[16]{0} %broadcast.1388), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %and.101 = pred[16]{0} and(pred[16]{0} %compare.90, pred[16]{0} %compare.89), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1387 = f32[16]{0} broadcast(f32[] %constant_80), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_964 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1386 = f32[16]{0} broadcast(f32[] %constant_964), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.89 = f32[16]{0} select(pred[16]{0} %and.101, f32[16]{0} %broadcast.1387, f32[16]{0} %broadcast.1386), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1385 = f32[16,2]{1,0} broadcast(f32[16]{0} %select.89), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %add.311 = f32[16,2]{1,0} add(f32[16,2]{1,0} %select.90, f32[16,2]{1,0} %broadcast.1385), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %negate.1 = f32[16,2]{1,0} negate(f32[16,2]{1,0} %add.311), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %param_4.397 = f16[16,8]{1,0} parameter(4)
  %slice.33 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_4.397), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_3.542 = f32[2]{0} parameter(3)
  %convert.519 = f16[2]{0} convert(f32[2]{0} %param_3.542), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.1424 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.519), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.324 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.33, f16[16,2]{1,0} %broadcast.1424), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.518 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.324), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %param_2.760 = f32[16]{0} parameter(2)
  %broadcast.1422 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_2.760), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.91 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %convert.518, f32[16,2]{1,0} %broadcast.1422), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.1102 = f32[16]{0} parameter(0)
  %broadcast.229 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_0.1102), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.2 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %subtract.91, f32[16,2]{1,0} %broadcast.229), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.98 = f32[16,2]{1,0} multiply(f32[16,2]{1,0} %negate.1, f32[16,2]{1,0} %subtract.2), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %bitcast.169 = f32[32]{0} bitcast(f32[16,2]{1,0} %multiply.98)
  ROOT %reduce.119 = f32[] reduce(f32[32]{0} %bitcast.169, f32[] %constant_80), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Mean-reduction.927, metadata={op_type="Mean" op_name="model/bert_pretrain_loss_and_metric_layer/Mean"}
}

%All_40-reduction.2342 (x.2343: pred[], y.2344: pred[]) -> pred[] {
  %x.2343 = pred[] parameter(0)
  %y.2344 = pred[] parameter(1)
  ROOT %and.2345 = pred[] and(pred[] %x.2343, pred[] %y.2344)
}

%fused_computation.11 (param_0.619: pred[], param_1.643: pred[], param_2.256: pred[], param_3.171: pred[], param_4.78: pred[], param_5.56: f32[2], param_6.49: pred[], param_7.44: pred[], param_8.47: pred[], param_9.25: pred[], param_10.12: pred[], param_11.9: pred[], param_12.11: pred[], param_13.10: pred[], param_14.11: pred[], param_15.12: pred[], param_16.14: pred[], param_17.8: pred[], param_18.4: pred[], param_19.4: pred[], param_20.4: pred[], param_21.4: pred[], param_22.4: pred[], param_23.4: pred[], param_24.4: pred[], param_25.4: pred[], param_26.4: pred[], param_27.4: pred[], param_28.4: pred[], param_29.4: pred[], param_30.4: pred[], param_31.4: pred[], param_32.4: pred[], param_33.4: pred[], param_34.4: pred[], param_35.4: pred[], param_36.4: pred[], param_37.4: pred[], param_38.4: pred[], param_39.4: pred[], param_40.4: pred[], param_41.4: pred[], param_42.4: pred[], param_43.4: pred[], param_44.4: pred[], param_45.4: pred[]) -> pred[] {
  %param_44.4 = pred[] parameter(44)
  %param_45.4 = pred[] parameter(45)
  %and.89 = pred[] and(pred[] %param_44.4, pred[] %param_45.4)
  %param_43.4 = pred[] parameter(43)
  %and.88 = pred[] and(pred[] %and.89, pred[] %param_43.4)
  %param_42.4 = pred[] parameter(42)
  %and.87 = pred[] and(pred[] %and.88, pred[] %param_42.4)
  %param_41.4 = pred[] parameter(41)
  %and.86 = pred[] and(pred[] %and.87, pred[] %param_41.4)
  %param_40.4 = pred[] parameter(40)
  %and.85 = pred[] and(pred[] %and.86, pred[] %param_40.4)
  %param_39.4 = pred[] parameter(39)
  %and.84 = pred[] and(pred[] %and.85, pred[] %param_39.4)
  %param_38.4 = pred[] parameter(38)
  %and.83 = pred[] and(pred[] %and.84, pred[] %param_38.4)
  %param_37.4 = pred[] parameter(37)
  %and.82 = pred[] and(pred[] %and.83, pred[] %param_37.4)
  %param_36.4 = pred[] parameter(36)
  %and.81 = pred[] and(pred[] %and.82, pred[] %param_36.4)
  %param_35.4 = pred[] parameter(35)
  %and.80 = pred[] and(pred[] %and.81, pred[] %param_35.4)
  %param_34.4 = pred[] parameter(34)
  %and.79 = pred[] and(pred[] %and.80, pred[] %param_34.4)
  %param_33.4 = pred[] parameter(33)
  %and.78 = pred[] and(pred[] %and.79, pred[] %param_33.4)
  %param_32.4 = pred[] parameter(32)
  %and.77 = pred[] and(pred[] %and.78, pred[] %param_32.4)
  %param_31.4 = pred[] parameter(31)
  %and.76 = pred[] and(pred[] %and.77, pred[] %param_31.4)
  %param_30.4 = pred[] parameter(30)
  %and.75 = pred[] and(pred[] %and.76, pred[] %param_30.4)
  %param_29.4 = pred[] parameter(29)
  %and.74 = pred[] and(pred[] %and.75, pred[] %param_29.4)
  %param_28.4 = pred[] parameter(28)
  %and.73 = pred[] and(pred[] %and.74, pred[] %param_28.4)
  %param_27.4 = pred[] parameter(27)
  %and.72 = pred[] and(pred[] %and.73, pred[] %param_27.4)
  %param_26.4 = pred[] parameter(26)
  %and.71 = pred[] and(pred[] %and.72, pred[] %param_26.4)
  %param_25.4 = pred[] parameter(25)
  %and.70 = pred[] and(pred[] %and.71, pred[] %param_25.4)
  %param_24.4 = pred[] parameter(24)
  %and.69 = pred[] and(pred[] %and.70, pred[] %param_24.4)
  %param_23.4 = pred[] parameter(23)
  %and.68 = pred[] and(pred[] %and.69, pred[] %param_23.4)
  %param_22.4 = pred[] parameter(22)
  %and.67 = pred[] and(pred[] %and.68, pred[] %param_22.4)
  %param_21.4 = pred[] parameter(21)
  %and.66 = pred[] and(pred[] %and.67, pred[] %param_21.4)
  %param_20.4 = pred[] parameter(20)
  %and.65 = pred[] and(pred[] %and.66, pred[] %param_20.4)
  %param_19.4 = pred[] parameter(19)
  %and.64 = pred[] and(pred[] %and.65, pred[] %param_19.4)
  %param_18.4 = pred[] parameter(18)
  %and.63 = pred[] and(pred[] %and.64, pred[] %param_18.4)
  %param_17.8 = pred[] parameter(17)
  %and.62 = pred[] and(pred[] %and.63, pred[] %param_17.8)
  %param_16.14 = pred[] parameter(16)
  %and.61 = pred[] and(pred[] %and.62, pred[] %param_16.14)
  %param_15.12 = pred[] parameter(15)
  %and.60 = pred[] and(pred[] %and.61, pred[] %param_15.12)
  %param_14.11 = pred[] parameter(14)
  %and.59 = pred[] and(pred[] %and.60, pred[] %param_14.11)
  %param_13.10 = pred[] parameter(13)
  %and.58 = pred[] and(pred[] %and.59, pred[] %param_13.10)
  %param_12.11 = pred[] parameter(12)
  %and.57 = pred[] and(pred[] %and.58, pred[] %param_12.11)
  %param_11.9 = pred[] parameter(11)
  %and.56 = pred[] and(pred[] %and.57, pred[] %param_11.9)
  %param_10.12 = pred[] parameter(10)
  %and.55 = pred[] and(pred[] %and.56, pred[] %param_10.12)
  %param_9.25 = pred[] parameter(9)
  %and.54 = pred[] and(pred[] %and.55, pred[] %param_9.25)
  %param_8.47 = pred[] parameter(8)
  %and.53 = pred[] and(pred[] %and.54, pred[] %param_8.47)
  %param_7.44 = pred[] parameter(7)
  %and.52 = pred[] and(pred[] %and.53, pred[] %param_7.44)
  %param_6.49 = pred[] parameter(6)
  %and.51 = pred[] and(pred[] %and.52, pred[] %param_6.49)
  %param_5.56 = f32[2]{0} parameter(5)
  %is-finite.0 = pred[2]{0} is-finite(f32[2]{0} %param_5.56), metadata={op_type="IsFinite" op_name="IsFinite_40"}
  %constant_81 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.120 = pred[] reduce(pred[2]{0} %is-finite.0, pred[] %constant_81), dimensions={0}, to_apply=%All_40-reduction.2342, metadata={op_type="All" op_name="All_40"}
  %and.50 = pred[] and(pred[] %and.51, pred[] %reduce.120)
  %param_4.78 = pred[] parameter(4)
  %and.49 = pred[] and(pred[] %and.50, pred[] %param_4.78)
  %param_3.171 = pred[] parameter(3)
  %and.48 = pred[] and(pred[] %and.49, pred[] %param_3.171)
  %param_2.256 = pred[] parameter(2)
  %and.47 = pred[] and(pred[] %and.48, pred[] %param_2.256)
  %param_1.643 = pred[] parameter(1)
  %and.46 = pred[] and(pred[] %and.47, pred[] %param_1.643)
  %param_0.619 = pred[] parameter(0)
  %and.45 = pred[] and(pred[] %and.46, pred[] %param_0.619), metadata={op_type="All" op_name="All_46"}
  %convert.30 = f32[] convert(pred[] %and.45), metadata={op_type="Cast" op_name="Cast"}
  %constant_82 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %compare.4 = pred[] compare(f32[] %convert.30, f32[] %constant_82), direction=EQ, metadata={op_type="Equal" op_name="Equal"}
}

%add_float_.1128 (x.1129: f32[], y.1130: f32[]) -> f32[] {
  %x.1129 = f32[] parameter(0)
  %y.1130 = f32[] parameter(1)
  ROOT %add.1131 = f32[] add(f32[] %x.1129, f32[] %y.1130)
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_sub_Sum-reduction.1904 (x.1905: f32[], y.1906: f32[]) -> f32[] {
  %x.1905 = f32[] parameter(0)
  %y.1906 = f32[] parameter(1)
  ROOT %add.1907 = f32[] add(f32[] %x.1905, f32[] %y.1906)
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum_1-reduction.1895 (x.1896: f32[], y.1897: f32[]) -> f32[] {
  %x.1896 = f32[] parameter(0)
  %y.1897 = f32[] parameter(1)
  ROOT %add.1898 = f32[] add(f32[] %x.1896, f32[] %y.1897)
}

%fused_computation.18 (param_0.1212: f16[1216,768], param_1.1399: f16[1216,768], param_2.938: f32[1216], param_3.742: f32[1216], param_4.611: f16[1216,768], param_5.536: f32[1216], param_6.521: f32[768], param_7.446: f32[1216], param_8.382: f16[1216,768]) -> (f32[768], f16[1216,768], f32[768], f32[768]) {
  %constant_299_clone_1 = f16[] constant(0.13416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %broadcast.271.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_299_clone_1), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %param_1.1399 = f16[1216,768]{1,0} parameter(1)
  %multiply.366.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_1.1399, f16[1216,768]{1,0} %param_1.1399)
  %param_8.382 = f16[1216,768]{1,0} parameter(8)
  %convert.163.clone.1 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_8.382), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1/Cast"}
  %constant_890_clone_1 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1288.clone.1 = f32[1216]{0} broadcast(f32[] %constant_890_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %param_7.446 = f32[1216]{0} parameter(7)
  %constant_885_clone_1 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1287.clone.1 = f32[1216]{0} broadcast(f32[] %constant_885_clone_1), dimensions={}
  %multiply.711.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_7.446, f32[1216]{0} %broadcast.1287.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %add.281.clone.1 = f32[1216]{0} add(f32[1216]{0} %broadcast.1288.clone.1, f32[1216]{0} %multiply.711.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %rsqrt.85.clone.1 = f32[1216]{0} rsqrt(f32[1216]{0} %add.281.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %broadcast.496.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %rsqrt.85.clone.1), dimensions={0}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %param_6.521 = f32[768]{0} parameter(6)
  %broadcast.494.clone.1 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_6.521), dimensions={1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.364.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.496.clone.1, f32[1216,768]{1,0} %broadcast.494.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.261.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.163.clone.1, f32[1216,768]{1,0} %multiply.364.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1/Mul"}
  %constant_300_clone_1 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.269.clone.1 = f32[1216,1]{1,0} broadcast(f32[] %constant_300_clone_1), dimensions={}
  %broadcast.268.clone.1 = f32[1216,1]{1,0} broadcast(f32[] %constant_885_clone_1), dimensions={}
  %bitcast.283.clone.1 = f32[1216,1]{1,0} bitcast(f32[1216]{0} %rsqrt.85.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %multiply.260.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %bitcast.283.clone.1, f32[1216,1]{1,0} %bitcast.283.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %multiply.259.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %multiply.260.clone.1, f32[1216,1]{1,0} %bitcast.283.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %param_5.536 = f32[1216]{0} parameter(5)
  %constant_301_clone_1 = f32[] constant(-0.5)
  %broadcast.267.clone.1 = f32[1216]{0} broadcast(f32[] %constant_301_clone_1), dimensions={}
  %multiply.258.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_5.536, f32[1216]{0} %broadcast.267.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %bitcast.282.clone.1 = f32[1216,1]{1,0} bitcast(f32[1216]{0} %multiply.258.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %multiply.257.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %multiply.259.clone.1, f32[1216,1]{1,0} %bitcast.282.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %multiply.256.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %broadcast.268.clone.1, f32[1216,1]{1,0} %multiply.257.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv"}
  %multiply.255.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %broadcast.269.clone.1, f32[1216,1]{1,0} %multiply.256.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %bitcast.281.clone.1 = f32[1216]{0} bitcast(f32[1216,1]{1,0} %multiply.255.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.266.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %bitcast.281.clone.1), dimensions={0}
  %param_4.611 = f16[1216,768]{1,0} parameter(4)
  %convert.419.clone.1 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_4.611), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %param_3.742 = f32[1216]{0} parameter(3)
  %multiply.709.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_3.742, f32[1216]{0} %broadcast.1287.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.1283.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.709.clone.1), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %subtract.79.clone.1 = f32[1216,768]{1,0} subtract(f32[1216,768]{1,0} %convert.419.clone.1, f32[1216,768]{1,0} %broadcast.1283.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.253.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.266.clone.1, f32[1216,768]{1,0} %subtract.79.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mul_1"}
  %add.45.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.261.clone.1, f32[1216,768]{1,0} %multiply.253.clone.1), metadata={op_type="AddN" op_name="AddN_1"}
  %param_2.938 = f32[1216]{0} parameter(2)
  %multiply.252.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %broadcast.1287.clone.1, f32[1216]{0} %param_2.938), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.264.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.252.clone.1), dimensions={0}
  %add.44.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %add.45.clone.1, f32[1216,768]{1,0} %broadcast.264.clone.1), metadata={op_type="AddN" op_name="AddN_1"}
  %convert.162.clone.1 = f16[1216,768]{1,0} convert(f32[1216,768]{1,0} %add.44.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast/Cast"}
  %constant_303_clone_1 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.498.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_303_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.365.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_1.1399, f16[1216,768]{1,0} %broadcast.498.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.251.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %convert.162.clone.1, f16[1216,768]{1,0} %multiply.365.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_3/Mul_1"}
  %constant_306_clone_1 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.501.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_306_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %param_0.1212 = f16[1216,768]{1,0} parameter(0)
  %multiply.250.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.1212, f16[1216,768]{1,0} %param_0.1212), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/TanhGrad"}
  %subtract.10.clone.1 = f16[1216,768]{1,0} subtract(f16[1216,768]{1,0} %broadcast.501.clone.1, f16[1216,768]{1,0} %multiply.250.clone.1), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/TanhGrad"}
  %multiply.249.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.251.clone.1, f16[1216,768]{1,0} %subtract.10.clone.1), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/TanhGrad"}
  %constant_305_clone_1 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.499.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_305_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2"}
  %multiply.248.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.249.clone.1, f16[1216,768]{1,0} %broadcast.499.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2/Mul"}
  %multiply.247.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.366.clone.1, f16[1216,768]{1,0} %multiply.248.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul"}
  %multiply.246.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %broadcast.271.clone.1, f16[1216,768]{1,0} %multiply.247.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %multiply.245.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %convert.162.clone.1, f16[1216,768]{1,0} %broadcast.498.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_3/Mul"}
  %add.88.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %broadcast.501.clone.1, f16[1216,768]{1,0} %param_0.1212), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %multiply.244.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.245.clone.1, f16[1216,768]{1,0} %add.88.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul/Mul"}
  %add.43.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %multiply.246.clone.1, f16[1216,768]{1,0} %multiply.244.clone.1), metadata={op_type="AddN" op_name="AddN_2"}
  %add.42.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %add.43.clone.1, f16[1216,768]{1,0} %multiply.248.clone.1), metadata={op_type="AddN" op_name="AddN_2"}
  %convert.34 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %add.42.clone.1), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %constant_298 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.126 = f32[768]{0} reduce(f32[1216,768]{1,0} %convert.34, f32[] %constant_298), dimensions={0}, to_apply=%add_float_.1128, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %reduce.122.clone.1 = f32[768]{0} reduce(f32[1216,768]{1,0} %convert.163.clone.1, f32[] %constant_298), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_sub_Sum-reduction.1904, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub/Sum"}
  %multiply.737.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.163.clone.1, f32[1216,768]{1,0} %convert.419.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1/Mul_1"}
  %negate.18.clone.1 = f32[1216,768]{1,0} negate(f32[1216,768]{1,0} %convert.163.clone.1), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub/Neg"}
  %multiply.735.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %negate.18.clone.1, f32[1216,768]{1,0} %broadcast.1283.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Mul_1"}
  %add.309.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.737.clone.1, f32[1216,768]{1,0} %multiply.735.clone.1), metadata={op_type="AddN" op_name="AddN"}
  %multiply.99.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %add.309.clone.1, f32[1216,768]{1,0} %broadcast.496.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Mul_1"}
  %reduce.124.clone.1 = f32[768]{0} reduce(f32[1216,768]{1,0} %multiply.99.clone.1, f32[] %constant_298), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum_1-reduction.1895, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Sum_1"}
  ROOT %tuple.28 = (f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %reduce.126, f16[1216,768]{1,0} %add.42.clone.1, f32[768]{0} %reduce.122.clone.1, f32[768]{0} %reduce.124.clone.1)
}

%All_42-reduction.2366 (x.2367: pred[], y.2368: pred[]) -> pred[] {
  %x.2367 = pred[] parameter(0)
  %y.2368 = pred[] parameter(1)
  ROOT %and.2369 = pred[] and(pred[] %x.2367, pred[] %y.2368)
}

%All_5-reduction.2426 (x.2427: pred[], y.2428: pred[]) -> pred[] {
  %x.2427 = pred[] parameter(0)
  %y.2428 = pred[] parameter(1)
  ROOT %and.2429 = pred[] and(pred[] %x.2427, pred[] %y.2428)
}

%All_7-reduction.2450 (x.2451: pred[], y.2452: pred[]) -> pred[] {
  %x.2451 = pred[] parameter(0)
  %y.2452 = pred[] parameter(1)
  ROOT %and.2453 = pred[] and(pred[] %x.2451, pred[] %y.2452)
}

%All_9-reduction.1934 (x.1935: pred[], y.1936: pred[]) -> pred[] {
  %x.1935 = pred[] parameter(0)
  %y.1936 = pred[] parameter(1)
  ROOT %and.1937 = pred[] and(pred[] %x.1935, pred[] %y.1936)
}

%All_27-reduction.2162 (x.2163: pred[], y.2164: pred[]) -> pred[] {
  %x.2163 = pred[] parameter(0)
  %y.2164 = pred[] parameter(1)
  ROOT %and.2165 = pred[] and(pred[] %x.2163, pred[] %y.2164)
}

%All_37-reduction.2294 (x.2295: pred[], y.2296: pred[]) -> pred[] {
  %x.2295 = pred[] parameter(0)
  %y.2296 = pred[] parameter(1)
  ROOT %and.2297 = pred[] and(pred[] %x.2295, pred[] %y.2296)
}

%All_21-reduction.2090 (x.2091: pred[], y.2092: pred[]) -> pred[] {
  %x.2091 = pred[] parameter(0)
  %y.2092 = pred[] parameter(1)
  ROOT %and.2093 = pred[] and(pred[] %x.2091, pred[] %y.2092)
}

%All_25-reduction.2138 (x.2139: pred[], y.2140: pred[]) -> pred[] {
  %x.2139 = pred[] parameter(0)
  %y.2140 = pred[] parameter(1)
  ROOT %and.2141 = pred[] and(pred[] %x.2139, pred[] %y.2140)
}

%All_11-reduction.1958 (x.1959: pred[], y.1960: pred[]) -> pred[] {
  %x.1959 = pred[] parameter(0)
  %y.1960 = pred[] parameter(1)
  ROOT %and.1961 = pred[] and(pred[] %x.1959, pred[] %y.1960)
}

%All_23-reduction.2114 (x.2115: pred[], y.2116: pred[]) -> pred[] {
  %x.2115 = pred[] parameter(0)
  %y.2116 = pred[] parameter(1)
  ROOT %and.2117 = pred[] and(pred[] %x.2115, pred[] %y.2116)
}

%fused_computation.19 (param_0.793: f32[768,768], param_1.1465: f32[768,12,64], param_2.1041: f32[768,12,64], param_3.851: f32[768,12,64], param_4.689: f32[12,64,768], param_5.621: f32[768,768], param_6.607: f32[768,12,64], param_7.513: f32[768,12,64], param_8.436: f32[12,64,768], param_9.378: f32[768,12,64]) -> (pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) {
  %param_0.793 = f32[768,768]{1,0} parameter(0)
  %is-finite.4 = pred[768,768]{1,0} is-finite(f32[768,768]{1,0} %param_0.793), metadata={op_type="IsFinite" op_name="IsFinite_42"}
  %bitcast.170 = pred[589824]{0} bitcast(pred[768,768]{1,0} %is-finite.4)
  %constant_293 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.127 = pred[] reduce(pred[589824]{0} %bitcast.170, pred[] %constant_293), dimensions={0}, to_apply=%All_42-reduction.2366, metadata={op_type="All" op_name="All_42"}
  %param_1.1465 = f32[768,12,64]{2,1,0} parameter(1)
  %is-finite.40.clone.1 = pred[768,12,64]{2,1,0} is-finite(f32[768,12,64]{2,1,0} %param_1.1465), metadata={op_type="IsFinite" op_name="IsFinite_5"}
  %bitcast.228.clone.1 = pred[589824]{0} bitcast(pred[768,12,64]{2,1,0} %is-finite.40.clone.1)
  %reduce.184.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.228.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_5-reduction.2426, metadata={op_type="All" op_name="All_5"}
  %param_2.1041 = f32[768,12,64]{2,1,0} parameter(2)
  %is-finite.38.clone.1 = pred[768,12,64]{2,1,0} is-finite(f32[768,12,64]{2,1,0} %param_2.1041), metadata={op_type="IsFinite" op_name="IsFinite_7"}
  %bitcast.223.clone.1 = pred[589824]{0} bitcast(pred[768,12,64]{2,1,0} %is-finite.38.clone.1)
  %reduce.181.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.223.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_7-reduction.2450, metadata={op_type="All" op_name="All_7"}
  %param_3.851 = f32[768,12,64]{2,1,0} parameter(3)
  %is-finite.36.clone.1 = pred[768,12,64]{2,1,0} is-finite(f32[768,12,64]{2,1,0} %param_3.851), metadata={op_type="IsFinite" op_name="IsFinite_9"}
  %bitcast.218.clone.1 = pred[589824]{0} bitcast(pred[768,12,64]{2,1,0} %is-finite.36.clone.1)
  %reduce.178.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.218.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_9-reduction.1934, metadata={op_type="All" op_name="All_9"}
  %param_4.689 = f32[12,64,768]{2,1,0} parameter(4)
  %is-finite.18.clone.1 = pred[12,64,768]{2,1,0} is-finite(f32[12,64,768]{2,1,0} %param_4.689), metadata={op_type="IsFinite" op_name="IsFinite_27"}
  %bitcast.184.clone.1 = pred[589824]{0} bitcast(pred[12,64,768]{2,1,0} %is-finite.18.clone.1)
  %reduce.151.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.184.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_27-reduction.2162, metadata={op_type="All" op_name="All_27"}
  %param_5.621 = f32[768,768]{1,0} parameter(5)
  %is-finite.8.clone.1 = pred[768,768]{1,0} is-finite(f32[768,768]{1,0} %param_5.621), metadata={op_type="IsFinite" op_name="IsFinite_37"}
  %bitcast.172.clone.1 = pred[589824]{0} bitcast(pred[768,768]{1,0} %is-finite.8.clone.1)
  %reduce.134.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.172.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_37-reduction.2294, metadata={op_type="All" op_name="All_37"}
  %param_6.607 = f32[768,12,64]{2,1,0} parameter(6)
  %is-finite.24.clone.1 = pred[768,12,64]{2,1,0} is-finite(f32[768,12,64]{2,1,0} %param_6.607), metadata={op_type="IsFinite" op_name="IsFinite_21"}
  %bitcast.199.clone.1 = pred[589824]{0} bitcast(pred[768,12,64]{2,1,0} %is-finite.24.clone.1)
  %reduce.159.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.199.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_21-reduction.2090, metadata={op_type="All" op_name="All_21"}
  %param_7.513 = f32[768,12,64]{2,1,0} parameter(7)
  %is-finite.20.clone.1 = pred[768,12,64]{2,1,0} is-finite(f32[768,12,64]{2,1,0} %param_7.513), metadata={op_type="IsFinite" op_name="IsFinite_25"}
  %bitcast.189.clone.1 = pred[589824]{0} bitcast(pred[768,12,64]{2,1,0} %is-finite.20.clone.1)
  %reduce.153.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.189.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_25-reduction.2138, metadata={op_type="All" op_name="All_25"}
  %param_8.436 = f32[12,64,768]{2,1,0} parameter(8)
  %is-finite.34.clone.1 = pred[12,64,768]{2,1,0} is-finite(f32[12,64,768]{2,1,0} %param_8.436), metadata={op_type="IsFinite" op_name="IsFinite_11"}
  %bitcast.213.clone.1 = pred[589824]{0} bitcast(pred[12,64,768]{2,1,0} %is-finite.34.clone.1)
  %reduce.176.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.213.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_11-reduction.1958, metadata={op_type="All" op_name="All_11"}
  %param_9.378 = f32[768,12,64]{2,1,0} parameter(9)
  %is-finite.22.clone.1 = pred[768,12,64]{2,1,0} is-finite(f32[768,12,64]{2,1,0} %param_9.378), metadata={op_type="IsFinite" op_name="IsFinite_23"}
  %bitcast.194.clone.1 = pred[589824]{0} bitcast(pred[768,12,64]{2,1,0} %is-finite.22.clone.1)
  %reduce.156.clone.1 = pred[] reduce(pred[589824]{0} %bitcast.194.clone.1, pred[] %constant_293), dimensions={0}, to_apply=%All_23-reduction.2114, metadata={op_type="All" op_name="All_23"}
  ROOT %tuple.109 = (pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) tuple(pred[] %reduce.127, pred[] %reduce.184.clone.1, pred[] %reduce.181.clone.1, pred[] %reduce.178.clone.1, pred[] %reduce.151.clone.1, pred[] %reduce.134.clone.1, pred[] %reduce.159.clone.1, pred[] %reduce.153.clone.1, pred[] %reduce.176.clone.1, pred[] %reduce.156.clone.1)
}

%All_41-reduction.2354 (x.2355: pred[], y.2356: pred[]) -> pred[] {
  %x.2355 = pred[] parameter(0)
  %y.2356 = pred[] parameter(1)
  ROOT %and.2357 = pred[] and(pred[] %x.2355, pred[] %y.2356)
}

%fused_computation.21 (param_0.1206: f32[], param_1.1386: f32[30522]) -> (pred[], f32[30522]) {
  %param_1.1386 = f32[30522]{0} parameter(1)
  %convert.37.clone.1 = f16[30522]{0} convert(f32[30522]{0} %param_1.1386), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
  %convert.36.clone.1 = f32[30522]{0} convert(f16[30522]{0} %convert.37.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/Cast/Cast"}
  %constant_291_clone_1 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.1206 = f32[] parameter(0)
  %divide.58.clone.1 = f32[] divide(f32[] %constant_291_clone_1, f32[] %param_0.1206), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.230.clone.1 = f32[30522]{0} broadcast(f32[] %divide.58.clone.1), dimensions={}, metadata={op_type="Mul" op_name="mul_42"}
  %multiply.102.clone.1 = f32[30522]{0} multiply(f32[30522]{0} %convert.36.clone.1, f32[30522]{0} %broadcast.230.clone.1), metadata={op_type="Mul" op_name="mul_42"}
  %is-finite.5 = pred[30522]{0} is-finite(f32[30522]{0} %multiply.102.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_41"}
  %constant_290 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.128 = pred[] reduce(pred[30522]{0} %is-finite.5, pred[] %constant_290), dimensions={0}, to_apply=%All_41-reduction.2354, metadata={op_type="All" op_name="All_41"}
  ROOT %tuple.24 = (pred[], f32[30522]{0}) tuple(pred[] %reduce.128, f32[30522]{0} %multiply.102.clone.1)
}

%add_float_.1039 (x.1040: f32[], y.1041: f32[]) -> f32[] {
  %x.1040 = f32[] parameter(0)
  %y.1041 = f32[] parameter(1)
  ROOT %add.1042 = f32[] add(f32[] %x.1040, f32[] %y.1041)
}

%fused_computation.23 (param_0.1075: s32[16,76], param_1.1174: f32[1216,30522], param_2.719: f32[1216], param_3.499: f32[16,76], param_4.356: f32[], param_5.303: f32[]) -> f32[30522] {
  %param_5.303 = f32[] parameter(5)
  %constant_292 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %compare.77 = pred[] compare(f32[] %param_5.303, f32[] %constant_292), direction=EQ, metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %param_4.356 = f32[] parameter(4)
  %divide.80 = f32[] divide(f32[] %param_4.356, f32[] %param_5.303), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %select.80 = f32[] select(pred[] %compare.77, f32[] %constant_292, f32[] %divide.80), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %broadcast.1344 = f32[16,76]{1,0} broadcast(f32[] %select.80), dimensions={}, metadata={op_type="Tile" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Tile_1"}
  %param_3.499 = f32[16,76]{1,0} parameter(3)
  %convert.466 = s32[16,76]{1,0} convert(f32[16,76]{1,0} %param_3.499), metadata={op_type="Cast" op_name="model/Cast"}
  %convert.465 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %convert.466), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast"}
  %multiply.721 = f32[16,76]{1,0} multiply(f32[16,76]{1,0} %broadcast.1344, f32[16,76]{1,0} %convert.465), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/mul/Mul"}
  %bitcast.360 = f32[1216]{0} bitcast(f32[16,76]{1,0} %multiply.721), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %broadcast.1343 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %bitcast.360), dimensions={0}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %param_1.1174 = f32[1216,30522]{1,0} parameter(1)
  %param_2.719 = f32[1216]{0} parameter(2)
  %broadcast.1342 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_2.719), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %divide.79 = f32[1216,30522]{1,0} divide(f32[1216,30522]{1,0} %param_1.1174, f32[1216,30522]{1,0} %broadcast.1342), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.1075 = s32[16,76]{1,0} parameter(0)
  %convert.464 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %param_0.1075), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_2"}
  %convert.463 = s64[16,76]{1,0} convert(f32[16,76]{1,0} %convert.464), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %bitcast.359 = s64[1216]{0} bitcast(s64[16,76]{1,0} %convert.463), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %broadcast.1341 = s64[1216,30522]{1,0} broadcast(s64[1216]{0} %bitcast.359), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.10 = s64[1216,30522]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.76 = pred[1216,30522]{1,0} compare(s64[1216,30522]{1,0} %broadcast.1341, s64[1216,30522]{1,0} %iota.10), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_930 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1340 = f32[1216,30522]{1,0} broadcast(f32[] %constant_930), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1339 = f32[1216,30522]{1,0} broadcast(f32[] %constant_292), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.79 = f32[1216,30522]{1,0} select(pred[1216,30522]{1,0} %compare.76, f32[1216,30522]{1,0} %broadcast.1340, f32[1216,30522]{1,0} %broadcast.1339), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_928 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1338 = s64[1216]{0} broadcast(s64[] %constant_928), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.75 = pred[1216]{0} compare(s64[1216]{0} %broadcast.1338, s64[1216]{0} %bitcast.359), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_927 = s64[] constant(30522), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1337 = s64[1216]{0} broadcast(s64[] %constant_927), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.74 = pred[1216]{0} compare(s64[1216]{0} %bitcast.359, s64[1216]{0} %broadcast.1337), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %and.97 = pred[1216]{0} and(pred[1216]{0} %compare.75, pred[1216]{0} %compare.74), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1336 = f32[1216]{0} broadcast(f32[] %constant_292), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_926 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1335 = f32[1216]{0} broadcast(f32[] %constant_926), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.78 = f32[1216]{0} select(pred[1216]{0} %and.97, f32[1216]{0} %broadcast.1336, f32[1216]{0} %broadcast.1335), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1333 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %select.78), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %add.303 = f32[1216,30522]{1,0} add(f32[1216,30522]{1,0} %select.79, f32[1216,30522]{1,0} %broadcast.1333), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.85 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %divide.79, f32[1216,30522]{1,0} %add.303), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.720 = f32[1216,30522]{1,0} multiply(f32[1216,30522]{1,0} %broadcast.1343, f32[1216,30522]{1,0} %subtract.85), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %convert.462 = f16[1216,30522]{1,0} convert(f32[1216,30522]{1,0} %multiply.720), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %convert.38 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %convert.462), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
  ROOT %reduce.129 = f32[30522]{0} reduce(f32[1216,30522]{1,0} %convert.38, f32[] %constant_292), dimensions={0}, to_apply=%add_float_.1039, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
}

%add_float_.982 (x.983: f32[], y.984: f32[]) -> f32[] {
  %x.983 = f32[] parameter(0)
  %y.984 = f32[] parameter(1)
  ROOT %add.985 = f32[] add(f32[] %x.983, f32[] %y.984)
}

%fused_computation.24 (param_0.783: f16[16,2], param_1.867: f32[]) -> f32[2] {
  %param_0.783 = f16[16,2]{1,0} parameter(0)
  %convert.41 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %param_0.783), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/BiasAddGrad"}
  %constant_281 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.130 = f32[2]{0} reduce(f32[16,2]{1,0} %convert.41, f32[] %constant_281), dimensions={0}, to_apply=%add_float_.982, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/BiasAddGrad"}
  %convert.40 = f16[2]{0} convert(f32[2]{0} %reduce.130), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/BiasAddGrad"}
  %convert.39 = f32[2]{0} convert(f16[2]{0} %convert.40), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast/Cast"}
  %constant_282 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.867 = f32[] parameter(1)
  %divide.57 = f32[] divide(f32[] %constant_282, f32[] %param_1.867), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.231 = f32[2]{0} broadcast(f32[] %divide.57), dimensions={}, metadata={op_type="Mul" op_name="mul_41"}
  ROOT %multiply.103 = f32[2]{0} multiply(f32[2]{0} %convert.39, f32[2]{0} %broadcast.231), metadata={op_type="Mul" op_name="mul_41"}
}

%fused_computation.26 (param_0.781: f16[768,8], param_1.864: f32[]) -> f32[768,2] {
  %param_0.781 = f16[768,8]{1,0} parameter(0)
  %slice.8 = f16[768,2]{1,0} slice(f16[768,8]{1,0} %param_0.781), slice={[0:768], [0:2]}, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul_1"}
  %convert.42 = f32[768,2]{1,0} convert(f16[768,2]{1,0} %slice.8), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul/Cast/Cast"}
  %constant_280 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.864 = f32[] parameter(1)
  %divide.56 = f32[] divide(f32[] %constant_280, f32[] %param_1.864), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.232 = f32[768,2]{1,0} broadcast(f32[] %divide.56), dimensions={}, metadata={op_type="Mul" op_name="mul_40"}
  ROOT %multiply.104 = f32[768,2]{1,0} multiply(f32[768,2]{1,0} %convert.42, f32[768,2]{1,0} %broadcast.232), metadata={op_type="Mul" op_name="mul_40"}
}

%fused_computation.30 (param_0.776: f16[768,768], param_1.859: f32[], param_2.969: f16[768,768]) -> (f32[768,768], f32[768,768]) {
  %param_0.776 = f16[768,768]{1,0} parameter(0)
  %convert.46 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.776), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul/Cast/Cast"}
  %constant_274 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.859 = f32[] parameter(1)
  %divide.54 = f32[] divide(f32[] %constant_274, f32[] %param_1.859), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.489 = f32[768,768]{1,0} broadcast(f32[] %divide.54), dimensions={}, metadata={op_type="Mul" op_name="mul_6"}
  %multiply.106 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.46, f32[768,768]{1,0} %broadcast.489), metadata={op_type="Mul" op_name="mul_38"}
  %param_2.969 = f16[768,768]{1,0} parameter(2)
  %convert.35.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_2.969), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/MatMul/Cast/Cast"}
  %multiply.101.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.35.clone.1, f32[768,768]{1,0} %broadcast.489), metadata={op_type="Mul" op_name="mul_43"}
  ROOT %tuple.73 = (f32[768,768]{1,0}, f32[768,768]{1,0}) tuple(f32[768,768]{1,0} %multiply.106, f32[768,768]{1,0} %multiply.101.clone.1)
}

%fused_computation.40 (param_0.77: f16[16,512,768]) -> f16[768,8192] {
  %param_0.77 = f16[16,512,768]{2,1,0} parameter(0)
  %transpose.120 = f16[768,16,512]{0,2,1} transpose(f16[16,512,768]{2,1,0} %param_0.77), dimensions={2,0,1}
  ROOT %bitcast.177 = f16[768,8192]{0,1} bitcast(f16[768,16,512]{0,2,1} %transpose.120)
}

%All_31-reduction.2222 (x.2223: pred[], y.2224: pred[]) -> pred[] {
  %x.2223 = pred[] parameter(0)
  %y.2224 = pred[] parameter(1)
  ROOT %and.2225 = pred[] and(pred[] %x.2223, pred[] %y.2224)
}

%All_15-reduction.2006 (x.2007: pred[], y.2008: pred[]) -> pred[] {
  %x.2007 = pred[] parameter(0)
  %y.2008 = pred[] parameter(1)
  ROOT %and.2009 = pred[] and(pred[] %x.2007, pred[] %y.2008)
}

%All_33-reduction.2246 (x.2247: pred[], y.2248: pred[]) -> pred[] {
  %x.2247 = pred[] parameter(0)
  %y.2248 = pred[] parameter(1)
  ROOT %and.2249 = pred[] and(pred[] %x.2247, pred[] %y.2248)
}

%All_17-reduction.2030 (x.2031: pred[], y.2032: pred[]) -> pred[] {
  %x.2031 = pred[] parameter(0)
  %y.2032 = pred[] parameter(1)
  ROOT %and.2033 = pred[] and(pred[] %x.2031, pred[] %y.2032)
}

%fused_computation.44 (param_0.747: f32[768,3072], param_1.1468: f32[768,3072], param_2.1045: f32[3072,768], param_3.855: f32[3072,768]) -> (pred[], pred[], pred[], pred[]) {
  %param_0.747 = f32[768,3072]{1,0} parameter(0)
  %is-finite.14 = pred[768,3072]{1,0} is-finite(f32[768,3072]{1,0} %param_0.747), metadata={op_type="IsFinite" op_name="IsFinite_31"}
  %bitcast.179 = pred[2359296]{0} bitcast(pred[768,3072]{1,0} %is-finite.14)
  %constant_235 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.144 = pred[] reduce(pred[2359296]{0} %bitcast.179, pred[] %constant_235), dimensions={0}, to_apply=%All_31-reduction.2222, metadata={op_type="All" op_name="All_31"}
  %param_1.1468 = f32[768,3072]{1,0} parameter(1)
  %is-finite.30.clone.1 = pred[768,3072]{1,0} is-finite(f32[768,3072]{1,0} %param_1.1468), metadata={op_type="IsFinite" op_name="IsFinite_15"}
  %bitcast.208.clone.1 = pred[2359296]{0} bitcast(pred[768,3072]{1,0} %is-finite.30.clone.1)
  %reduce.169.clone.1 = pred[] reduce(pred[2359296]{0} %bitcast.208.clone.1, pred[] %constant_235), dimensions={0}, to_apply=%All_15-reduction.2006, metadata={op_type="All" op_name="All_15"}
  %param_2.1045 = f32[3072,768]{1,0} parameter(2)
  %is-finite.12.clone.1 = pred[3072,768]{1,0} is-finite(f32[3072,768]{1,0} %param_2.1045), metadata={op_type="IsFinite" op_name="IsFinite_33"}
  %bitcast.176.clone.1 = pred[2359296]{0} bitcast(pred[3072,768]{1,0} %is-finite.12.clone.1)
  %reduce.141.clone.1 = pred[] reduce(pred[2359296]{0} %bitcast.176.clone.1, pred[] %constant_235), dimensions={0}, to_apply=%All_33-reduction.2246, metadata={op_type="All" op_name="All_33"}
  %param_3.855 = f32[3072,768]{1,0} parameter(3)
  %is-finite.28.clone.1 = pred[3072,768]{1,0} is-finite(f32[3072,768]{1,0} %param_3.855), metadata={op_type="IsFinite" op_name="IsFinite_17"}
  %bitcast.205.clone.1 = pred[2359296]{0} bitcast(pred[3072,768]{1,0} %is-finite.28.clone.1)
  %reduce.166.clone.1 = pred[] reduce(pred[2359296]{0} %bitcast.205.clone.1, pred[] %constant_235), dimensions={0}, to_apply=%All_17-reduction.2030, metadata={op_type="All" op_name="All_17"}
  ROOT %tuple.112 = (pred[], pred[], pred[], pred[]) tuple(pred[] %reduce.144, pred[] %reduce.169.clone.1, pred[] %reduce.141.clone.1, pred[] %reduce.166.clone.1)
}

%fused_computation.46 (param_0.89: f16[16,512,3072]) -> f16[3072,8192] {
  %param_0.89 = f16[16,512,3072]{2,1,0} parameter(0)
  %transpose.121 = f16[3072,16,512]{0,2,1} transpose(f16[16,512,3072]{2,1,0} %param_0.89), dimensions={2,0,1}
  ROOT %bitcast.180 = f16[3072,8192]{0,1} bitcast(f16[3072,16,512]{0,2,1} %transpose.121)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_mul_Sum_1-reduction.1844 (x.1845: f32[], y.1846: f32[]) -> f32[] {
  %x.1845 = f32[] parameter(0)
  %y.1846 = f32[] parameter(1)
  ROOT %add.1847 = f32[] add(f32[] %x.1845, f32[] %y.1846)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_sub_Sum-reduction.1853 (x.1854: f32[], y.1855: f32[]) -> f32[] {
  %x.1854 = f32[] parameter(0)
  %y.1855 = f32[] parameter(1)
  ROOT %add.1856 = f32[] add(f32[] %x.1854, f32[] %y.1855)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_mul_Sum_1-reduction.1871 (x.1872: f32[], y.1873: f32[]) -> f32[] {
  %x.1872 = f32[] parameter(0)
  %y.1873 = f32[] parameter(1)
  ROOT %add.1874 = f32[] add(f32[] %x.1872, f32[] %y.1873)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_sub_Sum-reduction.1880 (x.1881: f32[], y.1882: f32[]) -> f32[] {
  %x.1881 = f32[] parameter(0)
  %y.1882 = f32[] parameter(1)
  ROOT %add.1883 = f32[] add(f32[] %x.1881, f32[] %y.1882)
}

%fused_computation.50 (param_0.1122: f32[16,512], param_1.1237: f32[16,512], param_2.802: f32[16,512,768], param_3.592: f16[16,512,768], param_4.659: f32[16,512], param_5.591: f32[16,512], param_6.576: f32[768], param_7.485: f32[768], param_8.411: f16[8192,768], param_9.356: f32[768], param_10.280: f16[16,512,768], param_11.224: f16[16,768], param_12.165: f16[8192,768]) -> (f32[768], f32[768], f32[768], f32[768]) {
  %param_2.802 = f32[16,512,768]{2,1,0} parameter(2)
  %param_3.592 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.574 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.592), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %multiply.832 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %param_2.802, f32[16,512,768]{2,1,0} %convert.574), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1/Mul_1"}
  %negate.26 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %param_2.802), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub/Neg"}
  %param_1.1237 = f32[16,512]{1,0} parameter(1)
  %constant_710 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1543 = f32[16,512]{1,0} broadcast(f32[] %constant_710), dimensions={}
  %multiply.831 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1237, f32[16,512]{1,0} %broadcast.1543), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1542 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.831), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.830 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.26, f32[16,512,768]{2,1,0} %broadcast.1542), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.383 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.832, f32[16,512,768]{2,1,0} %multiply.830), metadata={op_type="AddN" op_name="AddN_8"}
  %param_0.1122 = f32[16,512]{1,0} parameter(0)
  %multiply.552 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1122, f32[16,512]{1,0} %broadcast.1543), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_709 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1000 = f32[16,512]{1,0} broadcast(f32[] %constant_709), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.196 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.552, f32[16,512]{1,0} %broadcast.1000), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.55 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.196), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.453 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.55), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.113 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.383, f32[16,512,768]{2,1,0} %broadcast.453), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul/Mul_1"}
  %bitcast.182 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %multiply.113)
  %constant_230 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.148 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.182, f32[] %constant_230), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_mul_Sum_1-reduction.1844, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul/Sum_1"}
  %bitcast.181.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %param_2.802)
  %reduce.146.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.181.clone.1, f32[] %constant_230), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_sub_Sum-reduction.1853, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub/Sum"}
  %param_12.165 = f16[8192,768]{1,0} parameter(12)
  %convert.540.clone.1 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_12.165), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %bitcast.384.clone.1 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.540.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %param_11.224 = f16[16,768]{1,0} parameter(11)
  %convert.539.clone.1 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_11.224), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_3/Cast"}
  %bitcast.383.clone.1 = f32[16,1,768]{2,1,0} bitcast(f32[16,768]{1,0} %convert.539.clone.1), metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %pad.29.clone.1 = f32[16,512,768]{2,1,0} pad(f32[16,1,768]{2,1,0} %bitcast.383.clone.1, f32[] %constant_230), padding=0_0x0_511x0_0, metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %add.335.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %bitcast.384.clone.1, f32[16,512,768]{2,1,0} %pad.29.clone.1), metadata={op_type="AddN" op_name="AddN_3"}
  %bitcast.173.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %add.335.clone.1)
  %reduce.136.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.173.clone.1, f32[] %constant_230), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_sub_Sum-reduction.1880, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub/Sum"}
  %param_10.280 = f16[16,512,768]{2,1,0} parameter(10)
  %constant_1037_clone_1_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1487.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1037_clone_1_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.101.clone.1.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_10.280, f16[16,512,768]{2,1,0} %broadcast.1487.clone.1.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %constant_1036_clone_1_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1486.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1036_clone_1_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_1035_clone_1_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1485.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1035_clone_1_clone_1), dimensions={}
  %select.98.clone.1.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.101.clone.1.clone.1, f16[16,512,768]{2,1,0} %broadcast.1486.clone.1.clone.1, f16[16,512,768]{2,1,0} %broadcast.1485.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %param_9.356 = f32[768]{0} parameter(9)
  %convert.560.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_9.356), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast"}
  %broadcast.1484.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.560.clone.1.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %param_8.411 = f16[8192,768]{1,0} parameter(8)
  %bitcast.394.clone.1.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_8.411), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %add.360.clone.1.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1484.clone.1.clone.1, f16[16,512,768]{2,1,0} %bitcast.394.clone.1.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %multiply.775.clone.1.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.98.clone.1.clone.1, f16[16,512,768]{2,1,0} %add.360.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %convert.559.clone.1.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.775.clone.1.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2"}
  %param_7.485 = f32[768]{0} parameter(7)
  %broadcast.1479.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_7.485), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.772.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.453, f32[16,512,768]{2,1,0} %broadcast.1479.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.771.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.574, f32[16,512,768]{2,1,0} %multiply.772.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_6.576 = f32[768]{0} parameter(6)
  %broadcast.1478.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_6.576), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %multiply.769.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.772.clone.1.clone.1, f32[16,512,768]{2,1,0} %broadcast.1542), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.95.clone.1.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1478.clone.1.clone.1, f32[16,512,768]{2,1,0} %multiply.769.clone.1.clone.1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.357.clone.1.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.771.clone.1.clone.1, f32[16,512,768]{2,1,0} %subtract.95.clone.1.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %add.356.clone.1.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.559.clone.1.clone.1, f32[16,512,768]{2,1,0} %add.357.clone.1.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add_1"}
  %multiply.768.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.335.clone.1, f32[16,512,768]{2,1,0} %add.356.clone.1.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_1/Mul_1"}
  %negate.22.clone.1.clone.1 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %add.335.clone.1), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub/Neg"}
  %param_5.591 = f32[16,512]{1,0} parameter(5)
  %multiply.767.clone.1.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.591, f32[16,512]{1,0} %broadcast.1543), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
  %broadcast.1474.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.767.clone.1.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %multiply.766.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.22.clone.1.clone.1, f32[16,512,768]{2,1,0} %broadcast.1474.clone.1.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.355.clone.1.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.768.clone.1.clone.1, f32[16,512,768]{2,1,0} %multiply.766.clone.1.clone.1), metadata={op_type="AddN" op_name="AddN_4"}
  %param_4.659 = f32[16,512]{1,0} parameter(4)
  %multiply.647.clone.1.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.659, f32[16,512]{1,0} %broadcast.1543), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %add.253.clone.1.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.647.clone.1.clone.1, f32[16,512]{1,0} %broadcast.1000), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add"}
  %rsqrt.75.clone.1.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.253.clone.1.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.486.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.75.clone.1.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.107.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.355.clone.1.clone.1, f32[16,512,768]{2,1,0} %broadcast.486.clone.1.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul/Mul_1"}
  %bitcast.174.clone.1.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %multiply.107.clone.1.clone.1)
  %reduce.138.clone.1.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.174.clone.1.clone.1, f32[] %constant_230), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_mul_Sum_1-reduction.1871, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul/Sum_1"}
  ROOT %tuple.80 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %reduce.148, f32[768]{0} %reduce.146.clone.1, f32[768]{0} %reduce.136.clone.1, f32[768]{0} %reduce.138.clone.1.clone.1)
}

%fused_computation.56 (param_0.112: f16[16,512,768]) -> f16[768,8192] {
  %param_0.112 = f16[16,512,768]{2,1,0} parameter(0)
  %transpose.123 = f16[768,16,512]{0,2,1} transpose(f16[16,512,768]{2,1,0} %param_0.112), dimensions={2,0,1}
  ROOT %bitcast.186 = f16[768,8192]{0,1} bitcast(f16[768,16,512]{0,2,1} %transpose.123)
}

%fused_computation.62 (param_0.128: f16[16,12,64,512]) -> f16[768,8192] {
  %param_0.128 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.125 = f16[12,64,16,512]{3,1,0,2} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.128), dimensions={1,2,0,3}
  %copy.84 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{3,1,0,2} %transpose.125)
  ROOT %bitcast.191 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.84)
}

%fused_computation.68 (param_0.144: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.144 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.127 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.144), dimensions={1,3,0,2}
  %copy.86 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.127)
  ROOT %bitcast.196 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.86)
}

%All_22-reduction.2102 (x.2103: pred[], y.2104: pred[]) -> pred[] {
  %x.2103 = pred[] parameter(0)
  %y.2104 = pred[] parameter(1)
  ROOT %and.2105 = pred[] and(pred[] %x.2103, pred[] %y.2104)
}

%All_26-reduction.2150 (x.2151: pred[], y.2152: pred[]) -> pred[] {
  %x.2151 = pred[] parameter(0)
  %y.2152 = pred[] parameter(1)
  ROOT %and.2153 = pred[] and(pred[] %x.2151, pred[] %y.2152)
}

%All_6-reduction.2438 (x.2439: pred[], y.2440: pred[]) -> pred[] {
  %x.2439 = pred[] parameter(0)
  %y.2440 = pred[] parameter(1)
  ROOT %and.2441 = pred[] and(pred[] %x.2439, pred[] %y.2440)
}

%All_10-reduction.1946 (x.1947: pred[], y.1948: pred[]) -> pred[] {
  %x.1947 = pred[] parameter(0)
  %y.1948 = pred[] parameter(1)
  ROOT %and.1949 = pred[] and(pred[] %x.1947, pred[] %y.1948)
}

%All_24-reduction.2126 (x.2127: pred[], y.2128: pred[]) -> pred[] {
  %x.2127 = pred[] parameter(0)
  %y.2128 = pred[] parameter(1)
  ROOT %and.2129 = pred[] and(pred[] %x.2127, pred[] %y.2128)
}

%All_8-reduction.2462 (x.2463: pred[], y.2464: pred[]) -> pred[] {
  %x.2463 = pred[] parameter(0)
  %y.2464 = pred[] parameter(1)
  ROOT %and.2465 = pred[] and(pred[] %x.2463, pred[] %y.2464)
}

%All_45-reduction.2402 (x.2403: pred[], y.2404: pred[]) -> pred[] {
  %x.2403 = pred[] parameter(0)
  %y.2404 = pred[] parameter(1)
  ROOT %and.2405 = pred[] and(pred[] %x.2403, pred[] %y.2404)
}

%All_44-reduction.2390 (x.2391: pred[], y.2392: pred[]) -> pred[] {
  %x.2391 = pred[] parameter(0)
  %y.2392 = pred[] parameter(1)
  ROOT %and.2393 = pred[] and(pred[] %x.2391, pred[] %y.2392)
}

%All_36-reduction.2282 (x.2283: pred[], y.2284: pred[]) -> pred[] {
  %x.2283 = pred[] parameter(0)
  %y.2284 = pred[] parameter(1)
  ROOT %and.2285 = pred[] and(pred[] %x.2283, pred[] %y.2284)
}

%All_35-reduction.2270 (x.2271: pred[], y.2272: pred[]) -> pred[] {
  %x.2271 = pred[] parameter(0)
  %y.2272 = pred[] parameter(1)
  ROOT %and.2273 = pred[] and(pred[] %x.2271, pred[] %y.2272)
}

%All_30-reduction.2210 (x.2211: pred[], y.2212: pred[]) -> pred[] {
  %x.2211 = pred[] parameter(0)
  %y.2212 = pred[] parameter(1)
  ROOT %and.2213 = pred[] and(pred[] %x.2211, pred[] %y.2212)
}

%All_29-reduction.2198 (x.2199: pred[], y.2200: pred[]) -> pred[] {
  %x.2199 = pred[] parameter(0)
  %y.2200 = pred[] parameter(1)
  ROOT %and.2201 = pred[] and(pred[] %x.2199, pred[] %y.2200)
}

%All_20-reduction.2078 (x.2079: pred[], y.2080: pred[]) -> pred[] {
  %x.2079 = pred[] parameter(0)
  %y.2080 = pred[] parameter(1)
  ROOT %and.2081 = pred[] and(pred[] %x.2079, pred[] %y.2080)
}

%All_19-reduction.2066 (x.2067: pred[], y.2068: pred[]) -> pred[] {
  %x.2067 = pred[] parameter(0)
  %y.2068 = pred[] parameter(1)
  ROOT %and.2069 = pred[] and(pred[] %x.2067, pred[] %y.2068)
}

%All_14-reduction.1994 (x.1995: pred[], y.1996: pred[]) -> pred[] {
  %x.1995 = pred[] parameter(0)
  %y.1996 = pred[] parameter(1)
  ROOT %and.1997 = pred[] and(pred[] %x.1995, pred[] %y.1996)
}

%All_13-reduction.1982 (x.1983: pred[], y.1984: pred[]) -> pred[] {
  %x.1983 = pred[] parameter(0)
  %y.1984 = pred[] parameter(1)
  ROOT %and.1985 = pred[] and(pred[] %x.1983, pred[] %y.1984)
}

%All_4-reduction.2414 (x.2415: pred[], y.2416: pred[]) -> pred[] {
  %x.2415 = pred[] parameter(0)
  %y.2416 = pred[] parameter(1)
  ROOT %and.2417 = pred[] and(pred[] %x.2415, pred[] %y.2416)
}

%All_3-reduction.2318 (x.2319: pred[], y.2320: pred[]) -> pred[] {
  %x.2319 = pred[] parameter(0)
  %y.2320 = pred[] parameter(1)
  ROOT %and.2321 = pred[] and(pred[] %x.2319, pred[] %y.2320)
}

%All_43-reduction.2378 (x.2379: pred[], y.2380: pred[]) -> pred[] {
  %x.2379 = pred[] parameter(0)
  %y.2380 = pred[] parameter(1)
  ROOT %and.2381 = pred[] and(pred[] %x.2379, pred[] %y.2380)
}

%All_38-reduction.2306 (x.2307: pred[], y.2308: pred[]) -> pred[] {
  %x.2307 = pred[] parameter(0)
  %y.2308 = pred[] parameter(1)
  ROOT %and.2309 = pred[] and(pred[] %x.2307, pred[] %y.2308)
}

%add_float_.1000 (x.1001: f32[], y.1002: f32[]) -> f32[] {
  %x.1001 = f32[] parameter(0)
  %y.1002 = f32[] parameter(1)
  ROOT %add.1003 = f32[] add(f32[] %x.1001, f32[] %y.1002)
}

%All_34-reduction.2258 (x.2259: pred[], y.2260: pred[]) -> pred[] {
  %x.2259 = pred[] parameter(0)
  %y.2260 = pred[] parameter(1)
  ROOT %and.2261 = pred[] and(pred[] %x.2259, pred[] %y.2260)
}

%All_28-reduction.2174 (x.2175: pred[], y.2176: pred[]) -> pred[] {
  %x.2175 = pred[] parameter(0)
  %y.2176 = pred[] parameter(1)
  ROOT %and.2177 = pred[] and(pred[] %x.2175, pred[] %y.2176)
}

%All_18-reduction.2042 (x.2043: pred[], y.2044: pred[]) -> pred[] {
  %x.2043 = pred[] parameter(0)
  %y.2044 = pred[] parameter(1)
  ROOT %and.2045 = pred[] and(pred[] %x.2043, pred[] %y.2044)
}

%All_12-reduction.1970 (x.1971: pred[], y.1972: pred[]) -> pred[] {
  %x.1971 = pred[] parameter(0)
  %y.1972 = pred[] parameter(1)
  ROOT %and.1973 = pred[] and(pred[] %x.1971, pred[] %y.1972)
}

%fused_computation.69 (param_0.713: f32[12,64], param_1.1459: f32[12,64], param_2.1036: f32[12,64], param_3.847: f32[12,64], param_4.685: f32[12,64], param_5.617: f32[12,64], param_6.603: f32[768], param_7.509: f32[768], param_8.432: f32[768], param_9.374: f32[768], param_10.298: f32[768], param_11.244: f32[768], param_12.189: f32[768], param_13.135: f32[768], param_14.83: f32[768], param_15.62: f32[768], param_16.47: f32[768], param_17.38: f32[768], param_18.35: f32[768], param_19.36: f16[16,768], param_20.35: f32[768], param_21.33: f32[768], param_22.30: f32[768], param_23.29: f32[], param_24.30: f32[768]) -> (pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768], pred[], f32[768], pred[], f32[768], pred[], f32[768], pred[], f32[768], pred[], f32[768], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) {
  %param_0.713 = f32[12,64]{1,0} parameter(0)
  %is-finite.23 = pred[12,64]{1,0} is-finite(f32[12,64]{1,0} %param_0.713), metadata={op_type="IsFinite" op_name="IsFinite_22"}
  %bitcast.197 = pred[768]{0} bitcast(pred[12,64]{1,0} %is-finite.23)
  %constant_198 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.157 = pred[] reduce(pred[768]{0} %bitcast.197, pred[] %constant_198), dimensions={0}, to_apply=%All_22-reduction.2102, metadata={op_type="All" op_name="All_22"}
  %param_1.1459 = f32[12,64]{1,0} parameter(1)
  %is-finite.19.clone.1 = pred[12,64]{1,0} is-finite(f32[12,64]{1,0} %param_1.1459), metadata={op_type="IsFinite" op_name="IsFinite_26"}
  %bitcast.187.clone.1 = pred[768]{0} bitcast(pred[12,64]{1,0} %is-finite.19.clone.1)
  %reduce.152.clone.1 = pred[] reduce(pred[768]{0} %bitcast.187.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_26-reduction.2150, metadata={op_type="All" op_name="All_26"}
  %param_2.1036 = f32[12,64]{1,0} parameter(2)
  %is-finite.39.clone.1 = pred[12,64]{1,0} is-finite(f32[12,64]{1,0} %param_2.1036), metadata={op_type="IsFinite" op_name="IsFinite_6"}
  %bitcast.226.clone.1 = pred[768]{0} bitcast(pred[12,64]{1,0} %is-finite.39.clone.1)
  %reduce.182.clone.1 = pred[] reduce(pred[768]{0} %bitcast.226.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_6-reduction.2438, metadata={op_type="All" op_name="All_6"}
  %param_3.847 = f32[12,64]{1,0} parameter(3)
  %is-finite.35.clone.1 = pred[12,64]{1,0} is-finite(f32[12,64]{1,0} %param_3.847), metadata={op_type="IsFinite" op_name="IsFinite_10"}
  %bitcast.216.clone.1 = pred[768]{0} bitcast(pred[12,64]{1,0} %is-finite.35.clone.1)
  %reduce.177.clone.1 = pred[] reduce(pred[768]{0} %bitcast.216.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_10-reduction.1946, metadata={op_type="All" op_name="All_10"}
  %param_4.685 = f32[12,64]{1,0} parameter(4)
  %is-finite.21.clone.1 = pred[12,64]{1,0} is-finite(f32[12,64]{1,0} %param_4.685), metadata={op_type="IsFinite" op_name="IsFinite_24"}
  %bitcast.192.clone.1 = pred[768]{0} bitcast(pred[12,64]{1,0} %is-finite.21.clone.1)
  %reduce.154.clone.1 = pred[] reduce(pred[768]{0} %bitcast.192.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_24-reduction.2126, metadata={op_type="All" op_name="All_24"}
  %param_5.617 = f32[12,64]{1,0} parameter(5)
  %is-finite.37.clone.1 = pred[12,64]{1,0} is-finite(f32[12,64]{1,0} %param_5.617), metadata={op_type="IsFinite" op_name="IsFinite_8"}
  %bitcast.221.clone.1 = pred[768]{0} bitcast(pred[12,64]{1,0} %is-finite.37.clone.1)
  %reduce.179.clone.1 = pred[] reduce(pred[768]{0} %bitcast.221.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_8-reduction.2462, metadata={op_type="All" op_name="All_8"}
  %param_24.30 = f32[768]{0} parameter(24)
  %convert.81.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_24.30), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add/Sum"}
  %convert.80.clone.1.clone.1 = f32[768]{0} convert(f16[768]{0} %convert.81.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add/Cast/Cast"}
  %constant_134_clone_1_clone_1 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_23.29 = f32[] parameter(23)
  %divide.21.clone.1.clone.1 = f32[] divide(f32[] %constant_134_clone_1_clone_1, f32[] %param_23.29), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.367.clone.1.clone.1 = f32[768]{0} broadcast(f32[] %divide.21.clone.1.clone.1), dimensions={}, metadata={op_type="Mul" op_name="mul_4"}
  %multiply.129.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %convert.80.clone.1.clone.1, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_13"}
  %is-finite.33.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.129.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_12"}
  %reduce.174.clone.1 = pred[] reduce(pred[768]{0} %is-finite.33.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_12-reduction.1970, metadata={op_type="All" op_name="All_12"}
  %param_22.30 = f32[768]{0} parameter(22)
  %convert.73.clone.1.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_22.30), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Sum"}
  %convert.72.clone.1.clone.1.clone.1 = f32[768]{0} convert(f16[768]{0} %convert.73.clone.1.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Cast/Cast"}
  %multiply.123.clone.1.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %convert.72.clone.1.clone.1.clone.1, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_19"}
  %is-finite.27.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.123.clone.1.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_18"}
  %reduce.164.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.27.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_18-reduction.2042, metadata={op_type="All" op_name="All_18"}
  %param_21.33 = f32[768]{0} parameter(21)
  %convert.56.clone.1.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_21.33), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add/Sum"}
  %convert.55.clone.1.clone.1.clone.1 = f32[768]{0} convert(f16[768]{0} %convert.56.clone.1.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add/Cast/Cast"}
  %multiply.114.clone.1.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %convert.55.clone.1.clone.1.clone.1, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_29"}
  %is-finite.17.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.114.clone.1.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_28"}
  %reduce.149.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.17.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_28-reduction.2174, metadata={op_type="All" op_name="All_28"}
  %param_20.35 = f32[768]{0} parameter(20)
  %convert.48.clone.1.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_20.35), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Sum"}
  %convert.47.clone.1.clone.1.clone.1 = f32[768]{0} convert(f16[768]{0} %convert.48.clone.1.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast/Cast"}
  %multiply.108.clone.1.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %convert.47.clone.1.clone.1.clone.1, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_35"}
  %is-finite.11.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.108.clone.1.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_34"}
  %reduce.139.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.11.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_34-reduction.2258, metadata={op_type="All" op_name="All_34"}
  %param_19.36 = f16[16,768]{1,0} parameter(19)
  %convert.45.clone.1.clone.1.clone.1 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_19.36), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/BiasAddGrad"}
  %constant_276_clone_1_clone_1_clone_1 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.133.clone.1.clone.1.clone.1 = f32[768]{0} reduce(f32[16,768]{1,0} %convert.45.clone.1.clone.1.clone.1, f32[] %constant_276_clone_1_clone_1_clone_1), dimensions={0}, to_apply=%add_float_.1000, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/BiasAddGrad"}
  %convert.44.clone.1.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %reduce.133.clone.1.clone.1.clone.1), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/BiasAddGrad"}
  %convert.43.clone.1.clone.1.clone.1 = f32[768]{0} convert(f16[768]{0} %convert.44.clone.1.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/Cast/Cast"}
  %multiply.105.clone.1.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %convert.43.clone.1.clone.1.clone.1, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_39"}
  %is-finite.7.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.105.clone.1.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_38"}
  %reduce.132.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.7.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_38-reduction.2306, metadata={op_type="All" op_name="All_38"}
  %param_18.35 = f32[768]{0} parameter(18)
  %convert.33.clone.1.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_18.35), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %convert.32.clone.1.clone.1.clone.1 = f32[768]{0} convert(f16[768]{0} %convert.33.clone.1.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/Cast/Cast"}
  %multiply.100.clone.1.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %convert.32.clone.1.clone.1.clone.1, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_44"}
  %is-finite.3.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.100.clone.1.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_43"}
  %reduce.125.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.3.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_43-reduction.2378, metadata={op_type="All" op_name="All_43"}
  %param_17.38 = f32[768]{0} parameter(17)
  %multiply.330.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_17.38, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_4"}
  %is-finite.42.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.330.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_3"}
  %reduce.187.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.42.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_3-reduction.2318, metadata={op_type="All" op_name="All_3"}
  %param_16.47 = f32[768]{0} parameter(16)
  %multiply.333.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_16.47, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_5"}
  %is-finite.41.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.333.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_4"}
  %reduce.185.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.41.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_4-reduction.2414, metadata={op_type="All" op_name="All_4"}
  %param_15.62 = f32[768]{0} parameter(15)
  %multiply.337.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_15.62, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_14"}
  %is-finite.32.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.337.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_13"}
  %reduce.172.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.32.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_13-reduction.1982, metadata={op_type="All" op_name="All_13"}
  %param_14.83 = f32[768]{0} parameter(14)
  %multiply.339.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_14.83, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_15"}
  %is-finite.31.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.339.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_14"}
  %reduce.170.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.31.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_14-reduction.1994, metadata={op_type="All" op_name="All_14"}
  %param_13.135 = f32[768]{0} parameter(13)
  %multiply.345.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_13.135, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_20"}
  %is-finite.26.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.345.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_19"}
  %reduce.162.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.26.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_19-reduction.2066, metadata={op_type="All" op_name="All_19"}
  %param_12.189 = f32[768]{0} parameter(12)
  %multiply.347.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_12.189, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_21"}
  %is-finite.25.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.347.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_20"}
  %reduce.160.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.25.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_20-reduction.2078, metadata={op_type="All" op_name="All_20"}
  %param_11.244 = f32[768]{0} parameter(11)
  %multiply.351.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_11.244, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_30"}
  %is-finite.16.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.351.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_29"}
  %reduce.147.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.16.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_29-reduction.2198, metadata={op_type="All" op_name="All_29"}
  %param_10.298 = f32[768]{0} parameter(10)
  %multiply.354.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_10.298, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_31"}
  %is-finite.15.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.354.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_30"}
  %reduce.145.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.15.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_30-reduction.2210, metadata={op_type="All" op_name="All_30"}
  %param_9.374 = f32[768]{0} parameter(9)
  %multiply.360.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_9.374, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_36"}
  %is-finite.10.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.360.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_35"}
  %reduce.137.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.10.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_35-reduction.2270, metadata={op_type="All" op_name="All_35"}
  %param_8.432 = f32[768]{0} parameter(8)
  %multiply.362.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_8.432, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_37"}
  %is-finite.9.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.362.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_36"}
  %reduce.135.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.9.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_36-reduction.2282, metadata={op_type="All" op_name="All_36"}
  %param_7.509 = f32[768]{0} parameter(7)
  %multiply.368.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_7.509, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_45"}
  %is-finite.2.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.368.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_44"}
  %reduce.123.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.2.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_44-reduction.2390, metadata={op_type="All" op_name="All_44"}
  %param_6.603 = f32[768]{0} parameter(6)
  %multiply.370.clone.1.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_6.603, f32[768]{0} %broadcast.367.clone.1.clone.1), metadata={op_type="Mul" op_name="mul_46"}
  %is-finite.1.clone.1.clone.1 = pred[768]{0} is-finite(f32[768]{0} %multiply.370.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_45"}
  %reduce.121.clone.1.clone.1 = pred[] reduce(pred[768]{0} %is-finite.1.clone.1.clone.1, pred[] %constant_198), dimensions={0}, to_apply=%All_45-reduction.2402, metadata={op_type="All" op_name="All_45"}
  ROOT %tuple.99 = (pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) tuple(pred[] %reduce.157, pred[] %reduce.152.clone.1, pred[] %reduce.182.clone.1, pred[] %reduce.177.clone.1, pred[] %reduce.154.clone.1, pred[] %reduce.179.clone.1, pred[] %reduce.174.clone.1, f32[768]{0} %multiply.129.clone.1.clone.1, pred[] %reduce.164.clone.1.clone.1, f32[768]{0} %multiply.123.clone.1.clone.1.clone.1, pred[] %reduce.149.clone.1.clone.1, f32[768]{0} %multiply.114.clone.1.clone.1.clone.1, pred[] %reduce.139.clone.1.clone.1, f32[768]{0} %multiply.108.clone.1.clone.1.clone.1, pred[] %reduce.132.clone.1.clone.1, f32[768]{0} %multiply.105.clone.1.clone.1.clone.1, pred[] %reduce.125.clone.1.clone.1, f32[768]{0} %multiply.100.clone.1.clone.1.clone.1, pred[] %reduce.187.clone.1.clone.1, pred[] %reduce.185.clone.1.clone.1, pred[] %reduce.172.clone.1.clone.1, pred[] %reduce.170.clone.1.clone.1, pred[] %reduce.162.clone.1.clone.1, pred[] %reduce.160.clone.1.clone.1, pred[] %reduce.147.clone.1.clone.1, pred[] %reduce.145.clone.1.clone.1, pred[] %reduce.137.clone.1.clone.1, pred[] %reduce.135.clone.1.clone.1, pred[] %reduce.123.clone.1.clone.1, pred[] %reduce.121.clone.1.clone.1)
}

%fused_computation.74 (param_0.160: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.160 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.129 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.160), dimensions={1,3,0,2}
  %copy.88 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.129)
  ROOT %bitcast.201 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.88)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_add_Sum-reduction.1472 (x.1473: f32[], y.1474: f32[]) -> f32[] {
  %x.1473 = f32[] parameter(0)
  %y.1474 = f32[] parameter(1)
  ROOT %add.1475 = f32[] add(f32[] %x.1473, f32[] %y.1474)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_add_Sum-reduction.1214 (x.1215: f32[], y.1216: f32[]) -> f32[] {
  %x.1215 = f32[] parameter(0)
  %y.1216 = f32[] parameter(1)
  ROOT %add.1217 = f32[] add(f32[] %x.1215, f32[] %y.1216)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_attention_output_add_Sum-reduction.1322 (x.1323: f32[], y.1324: f32[]) -> f32[] {
  %x.1323 = f32[] parameter(0)
  %y.1324 = f32[] parameter(1)
  ROOT %add.1325 = f32[] add(f32[] %x.1323, f32[] %y.1324)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_attention_output_add_Sum-reduction.1580 (x.1581: f32[], y.1582: f32[]) -> f32[] {
  %x.1581 = f32[] parameter(0)
  %y.1582 = f32[] parameter(1)
  ROOT %add.1583 = f32[] add(f32[] %x.1581, f32[] %y.1582)
}

%fused_computation.81 (param_0.692: f16[16,512,768], param_1.1445: f16[16,512,768], param_2.1025: f16[16,512,768], param_3.838: f16[16,512,768]) -> (f32[768], f32[768], f32[768], f32[768]) {
  %param_0.692 = f16[16,512,768]{2,1,0} parameter(0)
  %convert.74 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_0.692), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Sum"}
  %bitcast.204 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %convert.74)
  %constant_174 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.165 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.204, f32[] %constant_174), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_add_Sum-reduction.1472, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Sum"}
  %param_1.1445 = f16[16,512,768]{2,1,0} parameter(1)
  %convert.49.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_1.1445), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Sum"}
  %bitcast.175.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %convert.49.clone.1)
  %reduce.140.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.175.clone.1, f32[] %constant_174), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_add_Sum-reduction.1214, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Sum"}
  %param_2.1025 = f16[16,512,768]{2,1,0} parameter(2)
  %convert.57.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_2.1025), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add/Sum"}
  %bitcast.183.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %convert.57.clone.1)
  %reduce.150.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.183.clone.1, f32[] %constant_174), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_attention_output_add_Sum-reduction.1322, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add/Sum"}
  %param_3.838 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.82.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.838), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add/Sum"}
  %bitcast.212.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %convert.82.clone.1)
  %reduce.175.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.212.clone.1, f32[] %constant_174), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_attention_output_add_Sum-reduction.1580, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add/Sum"}
  ROOT %tuple.88 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %reduce.165, f32[768]{0} %reduce.140.clone.1, f32[768]{0} %reduce.150.clone.1, f32[768]{0} %reduce.175.clone.1)
}

%fused_computation.83 (param_0.689: f16[3072,768], param_1.739: f32[], param_2.967: f16[3072,768]) -> (f32[3072,768], f32[3072,768]) {
  %param_0.689 = f16[3072,768]{1,0} parameter(0)
  %convert.75 = f32[3072,768]{1,0} convert(f16[3072,768]{1,0} %param_0.689), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum/Cast/Cast"}
  %constant_169 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.739 = f32[] parameter(1)
  %divide.28 = f32[] divide(f32[] %constant_169, f32[] %param_1.739), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.402 = f32[3072,768]{1,0} broadcast(f32[] %divide.28), dimensions={}, metadata={op_type="Mul" op_name="mul_18"}
  %multiply.125 = f32[3072,768]{1,0} multiply(f32[3072,768]{1,0} %convert.75, f32[3072,768]{1,0} %broadcast.402), metadata={op_type="Mul" op_name="mul_18"}
  %param_2.967 = f16[3072,768]{1,0} parameter(2)
  %convert.50.clone.1 = f32[3072,768]{1,0} convert(f16[3072,768]{1,0} %param_2.967), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum/Cast/Cast"}
  %multiply.109.clone.1 = f32[3072,768]{1,0} multiply(f32[3072,768]{1,0} %convert.50.clone.1, f32[3072,768]{1,0} %broadcast.402), metadata={op_type="Mul" op_name="mul_34"}
  ROOT %tuple.72 = (f32[3072,768]{1,0}, f32[3072,768]{1,0}) tuple(f32[3072,768]{1,0} %multiply.125, f32[3072,768]{1,0} %multiply.109.clone.1)
}

%fused_computation.84 (param_0.180: f16[16,512,768]) -> f16[768,8192] {
  %param_0.180 = f16[16,512,768]{2,1,0} parameter(0)
  %transpose.130 = f16[768,16,512]{0,2,1} transpose(f16[16,512,768]{2,1,0} %param_0.180), dimensions={2,0,1}
  ROOT %bitcast.206 = f16[768,8192]{0,1} bitcast(f16[768,16,512]{0,2,1} %transpose.130)
}

%All_16-reduction.2018 (x.2019: pred[], y.2020: pred[]) -> pred[] {
  %x.2019 = pred[] parameter(0)
  %y.2020 = pred[] parameter(1)
  ROOT %and.2021 = pred[] and(pred[] %x.2019, pred[] %y.2020)
}

%All_32-reduction.2234 (x.2235: pred[], y.2236: pred[]) -> pred[] {
  %x.2235 = pred[] parameter(0)
  %y.2236 = pred[] parameter(1)
  ROOT %and.2237 = pred[] and(pred[] %x.2235, pred[] %y.2236)
}

%fused_computation.85 (param_0.1191: f32[], param_1.1342: f32[3072], param_2.948: f32[3072]) -> (pred[], f32[3072], pred[], f32[3072]) {
  %param_1.1342 = f32[3072]{0} parameter(1)
  %convert.77.clone.1 = f16[3072]{0} convert(f32[3072]{0} %param_1.1342), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  %convert.76.clone.1 = f32[3072]{0} convert(f16[3072]{0} %convert.77.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast/Cast"}
  %constant_159_clone_1 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.1191 = f32[] parameter(0)
  %divide.27.clone.1 = f32[] divide(f32[] %constant_159_clone_1, f32[] %param_0.1191), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.394.clone.1 = f32[3072]{0} broadcast(f32[] %divide.27.clone.1), dimensions={}, metadata={op_type="Mul" op_name="mul_17"}
  %multiply.126.clone.1 = f32[3072]{0} multiply(f32[3072]{0} %convert.76.clone.1, f32[3072]{0} %broadcast.394.clone.1), metadata={op_type="Mul" op_name="mul_17"}
  %is-finite.29 = pred[3072]{0} is-finite(f32[3072]{0} %multiply.126.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_16"}
  %constant_158 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.167 = pred[] reduce(pred[3072]{0} %is-finite.29, pred[] %constant_158), dimensions={0}, to_apply=%All_16-reduction.2018, metadata={op_type="All" op_name="All_16"}
  %param_2.948 = f32[3072]{0} parameter(2)
  %convert.52.clone.1.clone.1 = f16[3072]{0} convert(f32[3072]{0} %param_2.948), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
  %convert.51.clone.1.clone.1 = f32[3072]{0} convert(f16[3072]{0} %convert.52.clone.1.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast/Cast"}
  %multiply.110.clone.1.clone.1 = f32[3072]{0} multiply(f32[3072]{0} %convert.51.clone.1.clone.1, f32[3072]{0} %broadcast.394.clone.1), metadata={op_type="Mul" op_name="mul_33"}
  %is-finite.13.clone.1 = pred[3072]{0} is-finite(f32[3072]{0} %multiply.110.clone.1.clone.1), metadata={op_type="IsFinite" op_name="IsFinite_32"}
  %reduce.142.clone.1 = pred[] reduce(pred[3072]{0} %is-finite.13.clone.1, pred[] %constant_158), dimensions={0}, to_apply=%All_32-reduction.2234, metadata={op_type="All" op_name="All_32"}
  ROOT %tuple.48 = (pred[], f32[3072]{0}, pred[], f32[3072]{0}) tuple(pred[] %reduce.167, f32[3072]{0} %multiply.126.clone.1, pred[] %reduce.142.clone.1, f32[3072]{0} %multiply.110.clone.1.clone.1)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_intermediate_add_Sum-reduction.1504 (x.1505: f32[], y.1506: f32[]) -> f32[] {
  %x.1505 = f32[] parameter(0)
  %y.1506 = f32[] parameter(1)
  ROOT %add.1507 = f32[] add(f32[] %x.1505, f32[] %y.1506)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_intermediate_add_Sum-reduction.1246 (x.1247: f32[], y.1248: f32[]) -> f32[] {
  %x.1247 = f32[] parameter(0)
  %y.1248 = f32[] parameter(1)
  ROOT %add.1249 = f32[] add(f32[] %x.1247, f32[] %y.1248)
}

%fused_computation.87 (param_0.683: f16[16,512,3072], param_1.1448: f16[16,512,3072]) -> (f32[3072], f32[3072]) {
  %param_0.683 = f16[16,512,3072]{2,1,0} parameter(0)
  %convert.78 = f32[16,512,3072]{2,1,0} convert(f16[16,512,3072]{2,1,0} %param_0.683), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  %bitcast.207 = f32[8192,3072]{1,0} bitcast(f32[16,512,3072]{2,1,0} %convert.78)
  %constant_160 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.168 = f32[3072]{0} reduce(f32[8192,3072]{1,0} %bitcast.207, f32[] %constant_160), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_intermediate_add_Sum-reduction.1504, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  %param_1.1448 = f16[16,512,3072]{2,1,0} parameter(1)
  %convert.53.clone.1 = f32[16,512,3072]{2,1,0} convert(f16[16,512,3072]{2,1,0} %param_1.1448), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
  %bitcast.178.clone.1 = f32[8192,3072]{1,0} bitcast(f32[16,512,3072]{2,1,0} %convert.53.clone.1)
  %reduce.143.clone.1 = f32[3072]{0} reduce(f32[8192,3072]{1,0} %bitcast.178.clone.1, f32[] %constant_160), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_intermediate_add_Sum-reduction.1246, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
  ROOT %tuple.89 = (f32[3072]{0}, f32[3072]{0}) tuple(f32[3072]{0} %reduce.168, f32[3072]{0} %reduce.143.clone.1)
}

%fused_computation.89 (param_0.680: f16[768,3072], param_1.726: f32[], param_2.965: f16[768,3072]) -> (f32[768,3072], f32[768,3072]) {
  %param_0.680 = f16[768,3072]{1,0} parameter(0)
  %convert.79 = f32[768,3072]{1,0} convert(f16[768,3072]{1,0} %param_0.680), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum/Cast/Cast"}
  %constant_157 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.726 = f32[] parameter(1)
  %divide.26 = f32[] divide(f32[] %constant_157, f32[] %param_1.726), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.393 = f32[768,3072]{1,0} broadcast(f32[] %divide.26), dimensions={}, metadata={op_type="Mul" op_name="mul_16"}
  %multiply.127 = f32[768,3072]{1,0} multiply(f32[768,3072]{1,0} %convert.79, f32[768,3072]{1,0} %broadcast.393), metadata={op_type="Mul" op_name="mul_16"}
  %param_2.965 = f16[768,3072]{1,0} parameter(2)
  %convert.54.clone.1 = f32[768,3072]{1,0} convert(f16[768,3072]{1,0} %param_2.965), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum/Cast/Cast"}
  %multiply.111.clone.1 = f32[768,3072]{1,0} multiply(f32[768,3072]{1,0} %convert.54.clone.1, f32[768,3072]{1,0} %broadcast.393), metadata={op_type="Mul" op_name="mul_32"}
  ROOT %tuple.71 = (f32[768,3072]{1,0}, f32[768,3072]{1,0}) tuple(f32[768,3072]{1,0} %multiply.127, f32[768,3072]{1,0} %multiply.111.clone.1)
}

%fused_computation.90 (param_0.192: f16[16,512,3072]) -> f16[3072,8192] {
  %param_0.192 = f16[16,512,3072]{2,1,0} parameter(0)
  %transpose.131 = f16[3072,16,512]{0,2,1} transpose(f16[16,512,3072]{2,1,0} %param_0.192), dimensions={2,0,1}
  ROOT %bitcast.209 = f16[3072,8192]{0,1} bitcast(f16[3072,16,512]{0,2,1} %transpose.131)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_mul_Sum_1-reduction.1790 (x.1791: f32[], y.1792: f32[]) -> f32[] {
  %x.1791 = f32[] parameter(0)
  %y.1792 = f32[] parameter(1)
  ROOT %add.1793 = f32[] add(f32[] %x.1791, f32[] %y.1792)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_sub_Sum-reduction.1799 (x.1800: f32[], y.1801: f32[]) -> f32[] {
  %x.1800 = f32[] parameter(0)
  %y.1801 = f32[] parameter(1)
  ROOT %add.1802 = f32[] add(f32[] %x.1800, f32[] %y.1801)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_mul_Sum_1-reduction.1817 (x.1818: f32[], y.1819: f32[]) -> f32[] {
  %x.1818 = f32[] parameter(0)
  %y.1819 = f32[] parameter(1)
  ROOT %add.1820 = f32[] add(f32[] %x.1818, f32[] %y.1819)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_sub_Sum-reduction.1826 (x.1827: f32[], y.1828: f32[]) -> f32[] {
  %x.1827 = f32[] parameter(0)
  %y.1828 = f32[] parameter(1)
  ROOT %add.1829 = f32[] add(f32[] %x.1827, f32[] %y.1828)
}

%fused_computation.94 (param_0.1154: f32[16,512], param_1.1278: f32[16,512], param_2.854: f32[16,512,768], param_3.650: f16[16,512,768], param_4.672: f32[16,512], param_5.608: f32[16,512], param_6.597: f16[16,512,768], param_7.502: f32[768], param_8.425: f32[768], param_9.367: f16[8192,768], param_10.289: f32[768], param_11.235: f16[16,512,768]) -> (f32[768], f32[768], f32[768], f32[768]) {
  %param_2.854 = f32[16,512,768]{2,1,0} parameter(2)
  %param_3.650 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.629 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.650), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %multiply.980 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %param_2.854, f32[16,512,768]{2,1,0} %convert.629), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1/Mul_1"}
  %negate.38 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %param_2.854), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub/Neg"}
  %param_1.1278 = f32[16,512]{1,0} parameter(1)
  %constant_539 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1725 = f32[16,512]{1,0} broadcast(f32[] %constant_539), dimensions={}
  %multiply.979 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1278, f32[16,512]{1,0} %broadcast.1725), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.1724 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.979), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.977 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.38, f32[16,512,768]{2,1,0} %broadcast.1724), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.437 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.980, f32[16,512,768]{2,1,0} %multiply.977), metadata={op_type="AddN" op_name="AddN_15"}
  %param_0.1154 = f32[16,512]{1,0} parameter(0)
  %multiply.439 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1154, f32[16,512]{1,0} %broadcast.1725), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_538 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.764 = f32[16,512]{1,0} broadcast(f32[] %constant_538), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.126 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.439, f32[16,512]{1,0} %broadcast.764), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.27 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.126), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.388 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.27), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.128 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.437, f32[16,512,768]{2,1,0} %broadcast.388), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul/Mul_1"}
  %bitcast.211 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %multiply.128)
  %constant_150 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.173 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.211, f32[] %constant_150), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_mul_Sum_1-reduction.1790, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul/Sum_1"}
  %bitcast.210.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %param_2.854)
  %reduce.171.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.210.clone.1, f32[] %constant_150), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_sub_Sum-reduction.1799, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub/Sum"}
  %param_6.597 = f16[16,512,768]{2,1,0} parameter(6)
  %convert.71.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_6.597), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_2/Cast"}
  %bitcast.202.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %convert.71.clone.1)
  %reduce.161.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.202.clone.1, f32[] %constant_150), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_sub_Sum-reduction.1826, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/sub/Sum"}
  %param_11.235 = f16[16,512,768]{2,1,0} parameter(11)
  %constant_1161_clone_1_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1666.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1161_clone_1_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.110.clone.1.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_11.235, f16[16,512,768]{2,1,0} %broadcast.1666.clone.1.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/GreaterEqual"}
  %constant_1160_clone_1_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1664.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1160_clone_1_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_1159_clone_1_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1663.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1159_clone_1_clone_1), dimensions={}
  %select.106.clone.1.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.110.clone.1.clone.1, f16[16,512,768]{2,1,0} %broadcast.1664.clone.1.clone.1, f16[16,512,768]{2,1,0} %broadcast.1663.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul"}
  %param_10.289 = f32[768]{0} parameter(10)
  %convert.619.clone.1.clone.1 = f16[768]{0} convert(f32[768]{0} %param_10.289), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Cast"}
  %broadcast.1662.clone.1.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.619.clone.1.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %param_9.367 = f16[8192,768]{1,0} parameter(9)
  %bitcast.438.clone.1.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_9.367), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %add.421.clone.1.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1662.clone.1.clone.1, f16[16,512,768]{2,1,0} %bitcast.438.clone.1.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %multiply.921.clone.1.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.106.clone.1.clone.1, f16[16,512,768]{2,1,0} %add.421.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
  %convert.618.clone.1.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.921.clone.1.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_2"}
  %param_8.425 = f32[768]{0} parameter(8)
  %broadcast.1658.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_8.425), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.919.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.388, f32[16,512,768]{2,1,0} %broadcast.1658.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.918.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.629, f32[16,512,768]{2,1,0} %multiply.919.clone.1.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1"}
  %param_7.502 = f32[768]{0} parameter(7)
  %broadcast.1657.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_7.502), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %multiply.916.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.919.clone.1.clone.1, f32[16,512,768]{2,1,0} %broadcast.1724), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.103.clone.1.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1657.clone.1.clone.1, f32[16,512,768]{2,1,0} %multiply.916.clone.1.clone.1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %add.418.clone.1.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.918.clone.1.clone.1, f32[16,512,768]{2,1,0} %subtract.103.clone.1.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add_1"}
  %add.417.clone.1.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.618.clone.1.clone.1, f32[16,512,768]{2,1,0} %add.418.clone.1.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/add_1"}
  %multiply.915.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.71.clone.1, f32[16,512,768]{2,1,0} %add.417.clone.1.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_1/Mul_1"}
  %negate.34.clone.1.clone.1 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %convert.71.clone.1), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/sub/Neg"}
  %param_5.608 = f32[16,512]{1,0} parameter(5)
  %multiply.914.clone.1.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.608, f32[16,512]{1,0} %broadcast.1725), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mean"}
  %broadcast.1653.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.914.clone.1.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/SquaredDifference"}
  %multiply.913.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.34.clone.1.clone.1, f32[16,512,768]{2,1,0} %broadcast.1653.clone.1.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.416.clone.1.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.915.clone.1.clone.1, f32[16,512,768]{2,1,0} %multiply.913.clone.1.clone.1), metadata={op_type="AddN" op_name="AddN_11"}
  %param_4.672 = f32[16,512]{1,0} parameter(4)
  %multiply.527.clone.1.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.672, f32[16,512]{1,0} %broadcast.1725), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  %add.180.clone.1.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.527.clone.1.clone.1, f32[16,512]{1,0} %broadcast.764), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/add"}
  %rsqrt.47.clone.1.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.180.clone.1.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.420.clone.1.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.47.clone.1.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.122.clone.1.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.416.clone.1.clone.1, f32[16,512,768]{2,1,0} %broadcast.420.clone.1.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul/Mul_1"}
  %bitcast.203.clone.1.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %multiply.122.clone.1.clone.1)
  %reduce.163.clone.1.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.203.clone.1.clone.1, f32[] %constant_150), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_mul_Sum_1-reduction.1817, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul/Sum_1"}
  ROOT %tuple.83 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %reduce.173, f32[768]{0} %reduce.171.clone.1, f32[768]{0} %reduce.161.clone.1, f32[768]{0} %reduce.163.clone.1.clone.1)
}

%fused_computation.99 (param_0.663: f16[768,768], param_1.696: f32[], param_2.963: f16[768,768]) -> (f32[12,64,768], f32[12,64,768]) {
  %param_0.663 = f16[768,768]{1,0} parameter(0)
  %convert.83 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.663), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast/Cast"}
  %constant_131 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.696 = f32[] parameter(1)
  %divide.20 = f32[] divide(f32[] %constant_131, f32[] %param_1.696), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.366 = f32[768,768]{1,0} broadcast(f32[] %divide.20), dimensions={}, metadata={op_type="Mul" op_name="mul_6"}
  %multiply.130 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.83, f32[768,768]{1,0} %broadcast.366), metadata={op_type="Mul" op_name="mul_12"}
  %bitcast.214 = f32[768,12,64]{2,1,0} bitcast(f32[768,768]{1,0} %multiply.130), metadata={op_type="Mul" op_name="mul_12"}
  %copy.89 = f32[768,12,64]{0,2,1} copy(f32[768,12,64]{2,1,0} %bitcast.214), metadata={op_type="Mul" op_name="mul_12"}
  %transpose.132 = f32[12,64,768]{2,1,0} transpose(f32[768,12,64]{0,2,1} %copy.89), dimensions={1,2,0}, metadata={op_type="Mul" op_name="mul_12"}
  %param_2.963 = f16[768,768]{1,0} parameter(2)
  %convert.58.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_2.963), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast/Cast"}
  %multiply.115.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.58.clone.1, f32[768,768]{1,0} %broadcast.366), metadata={op_type="Mul" op_name="mul_28"}
  %bitcast.185.clone.1 = f32[768,12,64]{2,1,0} bitcast(f32[768,768]{1,0} %multiply.115.clone.1), metadata={op_type="Mul" op_name="mul_28"}
  %copy.82.clone.1 = f32[768,12,64]{0,2,1} copy(f32[768,12,64]{2,1,0} %bitcast.185.clone.1), metadata={op_type="Mul" op_name="mul_28"}
  %transpose.122.clone.1 = f32[12,64,768]{2,1,0} transpose(f32[768,12,64]{0,2,1} %copy.82.clone.1), dimensions={1,2,0}, metadata={op_type="Mul" op_name="mul_28"}
  ROOT %tuple.70 = (f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) tuple(f32[12,64,768]{2,1,0} %transpose.132, f32[12,64,768]{2,1,0} %transpose.122.clone.1)
}

%fused_computation.100 (param_0.215: f16[16,512,768]) -> f16[768,8192] {
  %param_0.215 = f16[16,512,768]{2,1,0} parameter(0)
  %transpose.133 = f16[768,16,512]{0,2,1} transpose(f16[16,512,768]{2,1,0} %param_0.215), dimensions={2,0,1}
  ROOT %bitcast.215 = f16[768,8192]{0,1} bitcast(f16[768,16,512]{0,2,1} %transpose.133)
}

%fused_computation.106 (param_0.231: f16[16,12,64,512]) -> f16[768,8192] {
  %param_0.231 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.135 = f16[12,64,16,512]{3,1,0,2} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.231), dimensions={1,2,0,3}
  %copy.91 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{3,1,0,2} %transpose.135)
  ROOT %bitcast.220 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.91)
}

%fused_computation.112 (param_0.247: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.247 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.137 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.247), dimensions={1,3,0,2}
  %copy.93 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.137)
  ROOT %bitcast.225 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.93)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_query_add_Sum-reduction.1635 (x.1636: f32[], y.1637: f32[]) -> f32[] {
  %x.1636 = f32[] parameter(0)
  %y.1637 = f32[] parameter(1)
  ROOT %add.1638 = f32[] add(f32[] %x.1636, f32[] %y.1637)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_key_add_Sum-reduction.1615 (x.1616: f32[], y.1617: f32[]) -> f32[] {
  %x.1616 = f32[] parameter(0)
  %y.1617 = f32[] parameter(1)
  ROOT %add.1618 = f32[] add(f32[] %x.1616, f32[] %y.1617)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_query_add_Sum-reduction.1377 (x.1378: f32[], y.1379: f32[]) -> f32[] {
  %x.1378 = f32[] parameter(0)
  %y.1379 = f32[] parameter(1)
  ROOT %add.1380 = f32[] add(f32[] %x.1378, f32[] %y.1379)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_key_add_Sum-reduction.1357 (x.1358: f32[], y.1359: f32[]) -> f32[] {
  %x.1358 = f32[] parameter(0)
  %y.1359 = f32[] parameter(1)
  ROOT %add.1360 = f32[] add(f32[] %x.1358, f32[] %y.1359)
}

%fused_computation.114 (param_0.646: f32[16,12,64], param_1.679: f32[], param_2.958: f32[16,12,64], param_3.780: f32[12,64], param_4.649: f32[16,12,64], param_5.575: f32[16,12,64], param_6.555: f32[12,64]) -> (f32[12,64], f32[12,64], f32[12,64], f32[12,64], f32[12,64], f32[12,64]) {
  %param_0.646 = f32[16,12,64]{2,1,0} parameter(0)
  %constant_114 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.183 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_0.646, f32[] %constant_114), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_query_add_Sum-reduction.1635, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %convert.101 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.183), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %convert.100 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.101), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast/Cast"}
  %constant_115 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.679 = f32[] parameter(1)
  %divide.15 = f32[] divide(f32[] %constant_115, f32[] %param_1.679), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.358 = f32[12,64]{1,0} broadcast(f32[] %divide.15), dimensions={}, metadata={op_type="Mul" op_name="mul_7"}
  %multiply.136 = f32[12,64]{1,0} multiply(f32[12,64]{1,0} %convert.100, f32[12,64]{1,0} %broadcast.358), metadata={op_type="Mul" op_name="mul_7"}
  %param_2.958 = f32[16,12,64]{2,1,0} parameter(2)
  %reduce.180.clone.1 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_2.958, f32[] %constant_114), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_key_add_Sum-reduction.1615, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %convert.95.clone.1 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.180.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %convert.94.clone.1 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.95.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Cast/Cast"}
  %multiply.133.clone.1 = f32[12,64]{1,0} multiply(f32[12,64]{1,0} %convert.94.clone.1, f32[12,64]{1,0} %broadcast.358), metadata={op_type="Mul" op_name="mul_9"}
  %param_3.780 = f32[12,64]{1,0} parameter(3)
  %convert.90.clone.1 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_3.780), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %convert.84.clone.1 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.90.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast/Cast"}
  %multiply.131.clone.1 = f32[12,64]{1,0} multiply(f32[12,64]{1,0} %convert.84.clone.1, f32[12,64]{1,0} %broadcast.358), metadata={op_type="Mul" op_name="mul_11"}
  %param_4.649 = f32[16,12,64]{2,1,0} parameter(4)
  %reduce.158.clone.1 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_4.649, f32[] %constant_114), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_query_add_Sum-reduction.1377, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %convert.68.clone.1 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.158.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %convert.67.clone.1 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.68.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast/Cast"}
  %multiply.120.clone.1 = f32[12,64]{1,0} multiply(f32[12,64]{1,0} %convert.67.clone.1, f32[12,64]{1,0} %broadcast.358), metadata={op_type="Mul" op_name="mul_23"}
  %param_5.575 = f32[16,12,64]{2,1,0} parameter(5)
  %reduce.155.clone.1 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_5.575, f32[] %constant_114), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_key_add_Sum-reduction.1357, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %convert.64.clone.1 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.155.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %convert.63.clone.1 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.64.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Cast/Cast"}
  %multiply.118.clone.1 = f32[12,64]{1,0} multiply(f32[12,64]{1,0} %convert.63.clone.1, f32[12,64]{1,0} %broadcast.358), metadata={op_type="Mul" op_name="mul_25"}
  %param_6.555 = f32[12,64]{1,0} parameter(6)
  %convert.60.clone.1 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_6.555), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  %convert.59.clone.1 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.60.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast/Cast"}
  %multiply.116.clone.1 = f32[12,64]{1,0} multiply(f32[12,64]{1,0} %convert.59.clone.1, f32[12,64]{1,0} %broadcast.358), metadata={op_type="Mul" op_name="mul_27"}
  ROOT %tuple.69 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) tuple(f32[12,64]{1,0} %multiply.136, f32[12,64]{1,0} %multiply.133.clone.1, f32[12,64]{1,0} %multiply.131.clone.1, f32[12,64]{1,0} %multiply.120.clone.1, f32[12,64]{1,0} %multiply.118.clone.1, f32[12,64]{1,0} %multiply.116.clone.1)
}

%fused_computation.117 (param_0.644: f16[768,768], param_1.676: f32[], param_2.954: f16[768,768], param_3.773: f16[768,768], param_4.642: f16[768,768], param_5.567: f16[768,768], param_6.548: f16[768,768]) -> (f32[768,12,64], f32[768,12,64], f32[768,12,64], f32[768,12,64], f32[768,12,64], f32[768,12,64]) {
  %param_0.644 = f16[768,768]{1,0} parameter(0)
  %convert.103 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.644), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast/Cast"}
  %constant_112 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.676 = f32[] parameter(1)
  %divide.14 = f32[] divide(f32[] %constant_112, f32[] %param_1.676), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.357 = f32[768,768]{1,0} broadcast(f32[] %divide.14), dimensions={}, metadata={op_type="Mul" op_name="mul_6"}
  %multiply.137 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.103, f32[768,768]{1,0} %broadcast.357), metadata={op_type="Mul" op_name="mul_6"}
  %copy.94 = f32[768,768]{0,1} copy(f32[768,768]{1,0} %multiply.137), metadata={op_type="Mul" op_name="mul_6"}
  %bitcast.229 = f32[12,64,768]{1,0,2} bitcast(f32[768,768]{0,1} %copy.94), metadata={op_type="Mul" op_name="mul_6"}
  %transpose.138 = f32[768,12,64]{2,1,0} transpose(f32[12,64,768]{1,0,2} %bitcast.229), dimensions={2,0,1}, metadata={op_type="Mul" op_name="mul_6"}
  %param_2.954 = f16[768,768]{1,0} parameter(2)
  %convert.99.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_2.954), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast/Cast"}
  %multiply.134.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.99.clone.1, f32[768,768]{1,0} %broadcast.357), metadata={op_type="Mul" op_name="mul_8"}
  %copy.92.clone.1 = f32[768,768]{0,1} copy(f32[768,768]{1,0} %multiply.134.clone.1), metadata={op_type="Mul" op_name="mul_8"}
  %bitcast.224.clone.1 = f32[12,64,768]{1,0,2} bitcast(f32[768,768]{0,1} %copy.92.clone.1), metadata={op_type="Mul" op_name="mul_8"}
  %transpose.136.clone.1 = f32[768,12,64]{2,1,0} transpose(f32[12,64,768]{1,0,2} %bitcast.224.clone.1), dimensions={2,0,1}, metadata={op_type="Mul" op_name="mul_8"}
  %param_3.773 = f16[768,768]{1,0} parameter(3)
  %convert.93.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_3.773), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast/Cast"}
  %multiply.132.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.93.clone.1, f32[768,768]{1,0} %broadcast.357), metadata={op_type="Mul" op_name="mul_10"}
  %bitcast.219.clone.1 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %multiply.132.clone.1), metadata={op_type="Mul" op_name="mul_10"}
  %copy.90.clone.1 = f32[12,64,768]{1,0,2} copy(f32[12,64,768]{2,1,0} %bitcast.219.clone.1), metadata={op_type="Mul" op_name="mul_10"}
  %transpose.134.clone.1 = f32[768,12,64]{2,1,0} transpose(f32[12,64,768]{1,0,2} %copy.90.clone.1), dimensions={2,0,1}, metadata={op_type="Mul" op_name="mul_10"}
  %param_4.642 = f16[768,768]{1,0} parameter(4)
  %convert.70.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_4.642), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast/Cast"}
  %multiply.121.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.70.clone.1, f32[768,768]{1,0} %broadcast.357), metadata={op_type="Mul" op_name="mul_22"}
  %bitcast.200.clone.1 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %multiply.121.clone.1), metadata={op_type="Mul" op_name="mul_22"}
  %copy.87.clone.1 = f32[12,64,768]{1,0,2} copy(f32[12,64,768]{2,1,0} %bitcast.200.clone.1), metadata={op_type="Mul" op_name="mul_22"}
  %transpose.128.clone.1 = f32[768,12,64]{2,1,0} transpose(f32[12,64,768]{1,0,2} %copy.87.clone.1), dimensions={2,0,1}, metadata={op_type="Mul" op_name="mul_22"}
  %param_5.567 = f16[768,768]{1,0} parameter(5)
  %convert.66.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_5.567), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast/Cast"}
  %multiply.119.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.66.clone.1, f32[768,768]{1,0} %broadcast.357), metadata={op_type="Mul" op_name="mul_24"}
  %bitcast.195.clone.1 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %multiply.119.clone.1), metadata={op_type="Mul" op_name="mul_24"}
  %copy.85.clone.1 = f32[12,64,768]{1,0,2} copy(f32[12,64,768]{2,1,0} %bitcast.195.clone.1), metadata={op_type="Mul" op_name="mul_24"}
  %transpose.126.clone.1 = f32[768,12,64]{2,1,0} transpose(f32[12,64,768]{1,0,2} %copy.85.clone.1), dimensions={2,0,1}, metadata={op_type="Mul" op_name="mul_24"}
  %param_6.548 = f16[768,768]{1,0} parameter(6)
  %convert.62.clone.1 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_6.548), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast/Cast"}
  %multiply.117.clone.1 = f32[768,768]{1,0} multiply(f32[768,768]{1,0} %convert.62.clone.1, f32[768,768]{1,0} %broadcast.357), metadata={op_type="Mul" op_name="mul_26"}
  %bitcast.190.clone.1 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %multiply.117.clone.1), metadata={op_type="Mul" op_name="mul_26"}
  %copy.83.clone.1 = f32[12,64,768]{1,0,2} copy(f32[12,64,768]{2,1,0} %bitcast.190.clone.1), metadata={op_type="Mul" op_name="mul_26"}
  %transpose.124.clone.1 = f32[768,12,64]{2,1,0} transpose(f32[12,64,768]{1,0,2} %copy.83.clone.1), dimensions={2,0,1}, metadata={op_type="Mul" op_name="mul_26"}
  ROOT %tuple.64 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) tuple(f32[768,12,64]{2,1,0} %transpose.138, f32[768,12,64]{2,1,0} %transpose.136.clone.1, f32[768,12,64]{2,1,0} %transpose.134.clone.1, f32[768,12,64]{2,1,0} %transpose.128.clone.1, f32[768,12,64]{2,1,0} %transpose.126.clone.1, f32[768,12,64]{2,1,0} %transpose.124.clone.1)
}

%fused_computation.118 (param_0.263: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.263 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.139 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.263), dimensions={1,3,0,2}
  %copy.95 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.139)
  ROOT %bitcast.230 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.95)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_sub_Sum-reduction.1772 (x.1773: f32[], y.1774: f32[]) -> f32[] {
  %x.1773 = f32[] parameter(0)
  %y.1774 = f32[] parameter(1)
  ROOT %add.1775 = f32[] add(f32[] %x.1773, f32[] %y.1774)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_mul_Sum_1-reduction.1763 (x.1764: f32[], y.1765: f32[]) -> f32[] {
  %x.1764 = f32[] parameter(0)
  %y.1765 = f32[] parameter(1)
  ROOT %add.1766 = f32[] add(f32[] %x.1764, f32[] %y.1765)
}

%fused_computation.120 (param_0.635: f16[16,512,768], param_1.1329: f32[16,512], param_2.902: f32[16,512], param_3.699: f16[16,512,768]) -> (f32[768], f32[768]) {
  %param_0.635 = f16[16,512,768]{2,1,0} parameter(0)
  %convert.104 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_0.635), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/Cast"}
  %bitcast.231 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %convert.104)
  %constant_105 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.186 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.231, f32[] %constant_105), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_sub_Sum-reduction.1772, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/sub/Sum"}
  %param_3.699 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.659.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.699), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %multiply.1048.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.104, f32[16,512,768]{2,1,0} %convert.659.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_1/Mul_1"}
  %negate.46.clone.1 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %convert.104), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/sub/Neg"}
  %param_2.902 = f32[16,512]{1,0} parameter(2)
  %constant_471_clone_1 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1796.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_471_clone_1), dimensions={}
  %multiply.1047.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.902, f32[16,512]{1,0} %broadcast.1796.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  %broadcast.1795.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.1047.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %multiply.1046.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.46.clone.1, f32[16,512,768]{2,1,0} %broadcast.1795.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_2/Mul_1"}
  %add.455.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.1048.clone.1, f32[16,512,768]{2,1,0} %multiply.1046.clone.1), metadata={op_type="AddN" op_name="AddN_18"}
  %param_1.1329 = f32[16,512]{1,0} parameter(1)
  %multiply.412.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1329, f32[16,512]{1,0} %broadcast.1796.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/variance"}
  %constant_469_clone_1 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.676.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_469_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.110.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.412.clone.1, f32[16,512]{1,0} %broadcast.676.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %rsqrt.19.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.110.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/Rsqrt"}
  %broadcast.351.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.19.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.138.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.455.clone.1, f32[16,512,768]{2,1,0} %broadcast.351.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul/Mul_1"}
  %bitcast.232.clone.1 = f32[8192,768]{1,0} bitcast(f32[16,512,768]{2,1,0} %multiply.138.clone.1)
  %reduce.188.clone.1 = f32[768]{0} reduce(f32[8192,768]{1,0} %bitcast.232.clone.1, f32[] %constant_105), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_mul_Sum_1-reduction.1763, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul/Sum_1"}
  ROOT %tuple.3 = (f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %reduce.186, f32[768]{0} %reduce.188.clone.1)
}

%All_2-reduction.2186 (x.2187: pred[], y.2188: pred[]) -> pred[] {
  %x.2187 = pred[] parameter(0)
  %y.2188 = pred[] parameter(1)
  ROOT %and.2189 = pred[] and(pred[] %x.2187, pred[] %y.2188)
}

%All_39-reduction.2330 (x.2331: pred[], y.2332: pred[]) -> pred[] {
  %x.2331 = pred[] parameter(0)
  %y.2332 = pred[] parameter(1)
  ROOT %and.2333 = pred[] and(pred[] %x.2331, pred[] %y.2332)
}

%fused_computation.123 (param_0.625: f32[2,768], param_1.1462: f32[768,2]) -> (pred[], pred[]) {
  %param_0.625 = f32[2,768]{1,0} parameter(0)
  %is-finite.43 = pred[2,768]{1,0} is-finite(f32[2,768]{1,0} %param_0.625), metadata={op_type="IsFinite" op_name="IsFinite_2"}
  %bitcast.233 = pred[1536]{0} bitcast(pred[2,768]{1,0} %is-finite.43)
  %constant_88 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  %reduce.189 = pred[] reduce(pred[1536]{0} %bitcast.233, pred[] %constant_88), dimensions={0}, to_apply=%All_2-reduction.2186, metadata={op_type="All" op_name="All_2"}
  %param_1.1462 = f32[768,2]{1,0} parameter(1)
  %is-finite.6.clone.1 = pred[768,2]{1,0} is-finite(f32[768,2]{1,0} %param_1.1462), metadata={op_type="IsFinite" op_name="IsFinite_39"}
  %bitcast.171.clone.1 = pred[1536]{0} bitcast(pred[768,2]{1,0} %is-finite.6.clone.1)
  %reduce.131.clone.1 = pred[] reduce(pred[1536]{0} %bitcast.171.clone.1, pred[] %constant_88), dimensions={0}, to_apply=%All_39-reduction.2330, metadata={op_type="All" op_name="All_39"}
  ROOT %tuple.100 = (pred[], pred[]) tuple(pred[] %reduce.189, pred[] %reduce.131.clone.1)
}

%fused_computation.124 (param_0.626: f16[8,768], param_1.652: f32[]) -> f32[2,768] {
  %param_0.626 = f16[8,768]{1,0} parameter(0)
  %slice.9 = f16[2,768]{1,0} slice(f16[8,768]{1,0} %param_0.626), slice={[0:2], [0:768]}, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul/MatMul"}
  %convert.105 = f32[2,768]{1,0} convert(f16[2,768]{1,0} %slice.9), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul/Cast/Cast"}
  %constant_89 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.652 = f32[] parameter(1)
  %divide.9 = f32[] divide(f32[] %constant_89, f32[] %param_1.652), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.233 = f32[2,768]{1,0} broadcast(f32[] %divide.9), dimensions={}, metadata={op_type="Mul" op_name="mul_3"}
  ROOT %multiply.139 = f32[2,768]{1,0} multiply(f32[2,768]{1,0} %convert.105, f32[2,768]{1,0} %broadcast.233), metadata={op_type="Mul" op_name="mul_3"}
}

%All_1-reduction.2054 (x.2055: pred[], y.2056: pred[]) -> pred[] {
  %x.2055 = pred[] parameter(0)
  %y.2056 = pred[] parameter(1)
  ROOT %and.2057 = pred[] and(pred[] %x.2055, pred[] %y.2056)
}

%fused_computation.125 (param_0.620: f32[512,768]) -> pred[] {
  %param_0.620 = f32[512,768]{1,0} parameter(0)
  %is-finite.44 = pred[512,768]{1,0} is-finite(f32[512,768]{1,0} %param_0.620), metadata={op_type="IsFinite" op_name="IsFinite_1"}
  %bitcast.234 = pred[393216]{0} bitcast(pred[512,768]{1,0} %is-finite.44)
  %constant_83 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  ROOT %reduce.190 = pred[] reduce(pred[393216]{0} %bitcast.234, pred[] %constant_83), dimensions={0}, to_apply=%All_1-reduction.2054, metadata={op_type="All" op_name="All_1"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_position_embedding_Sum-reduction.1730 (x.1731: f32[], y.1732: f32[]) -> f32[] {
  %x.1731 = f32[] parameter(0)
  %y.1732 = f32[] parameter(1)
  ROOT %add.1733 = f32[] add(f32[] %x.1731, f32[] %y.1732)
}

%fused_computation.126 (param_0.622: f16[16,512,768], param_1.646: f32[]) -> f32[512,768] {
  %param_0.622 = f16[16,512,768]{2,1,0} parameter(0)
  %convert.108 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_0.622), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Sum"}
  %constant_84 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.191 = f32[512,768]{1,0} reduce(f32[16,512,768]{2,1,0} %convert.108, f32[] %constant_84), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_position_embedding_Sum-reduction.1730, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Sum"}
  %convert.107 = f16[512,768]{1,0} convert(f32[512,768]{1,0} %reduce.191), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Sum"}
  %convert.106 = f32[512,768]{1,0} convert(f16[512,768]{1,0} %convert.107), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Cast/Cast"}
  %constant_85 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.646 = f32[] parameter(1)
  %divide.7 = f32[] divide(f32[] %constant_85, f32[] %param_1.646), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.234 = f32[512,768]{1,0} broadcast(f32[] %divide.7), dimensions={}, metadata={op_type="Mul" op_name="mul_2"}
  ROOT %multiply.140 = f32[512,768]{1,0} multiply(f32[512,768]{1,0} %convert.106, f32[512,768]{1,0} %broadcast.234), metadata={op_type="Mul" op_name="mul_2"}
}

%All-reduction.1922 (x.1923: pred[], y.1924: pred[]) -> pred[] {
  %x.1923 = pred[] parameter(0)
  %y.1924 = pred[] parameter(1)
  ROOT %and.1925 = pred[] and(pred[] %x.1923, pred[] %y.1924)
}

%fused_computation.127 (param_0.623: f32[30522,768]) -> pred[] {
  %param_0.623 = f32[30522,768]{1,0} parameter(0)
  %is-finite.45 = pred[30522,768]{1,0} is-finite(f32[30522,768]{1,0} %param_0.623), metadata={op_type="IsFinite" op_name="IsFinite"}
  %bitcast.235 = pred[23440896]{0} bitcast(pred[30522,768]{1,0} %is-finite.45)
  %constant_86 = pred[] constant(true), metadata={op_type="All" op_name="All"}
  ROOT %reduce.192 = pred[] reduce(pred[23440896]{0} %bitcast.235, pred[] %constant_86), dimensions={0}, to_apply=%All-reduction.1922, metadata={op_type="All" op_name="All"}
}

%fused_computation.128 (param_0.282: f32[30522,768], param_1.648: f32[]) -> f32[30522,768] {
  %param_0.282 = f32[30522,768]{1,0} parameter(0)
  %constant_87 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.648 = f32[] parameter(1)
  %divide.8 = f32[] divide(f32[] %constant_87, f32[] %param_1.648), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.235 = f32[30522,768]{1,0} broadcast(f32[] %divide.8), dimensions={}, metadata={op_type="Mul" op_name="mul_1"}
  ROOT %multiply.141 = f32[30522,768]{1,0} multiply(f32[30522,768]{1,0} %param_0.282, f32[30522,768]{1,0} %broadcast.235), metadata={op_type="Mul" op_name="mul_1"}
}

%scatter-combiner.1752 (p0.1753: f32[], p1.1754: f32[]) -> f32[] {
  %p0.1753 = f32[] parameter(0)
  %p1.1754 = f32[] parameter(1)
  ROOT %add.1755 = f32[] add(f32[] %p0.1753, f32[] %p1.1754)
}

%fused_computation.129 (param_0.624: f16[30528,768], param_1.650: f16[16,512,768], param_2.261: s32[16,512]) -> f32[30522,768] {
  %param_0.624 = f16[30528,768]{1,0} parameter(0)
  %slice.10 = f16[30522,768]{1,0} slice(f16[30528,768]{1,0} %param_0.624), slice={[0:30522], [0:768]}, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul_1"}
  %convert.111 = f32[30522,768]{1,0} convert(f16[30522,768]{1,0} %slice.10), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/Cast/Cast"}
  %param_2.261 = s32[16,512]{1,0} parameter(2)
  %bitcast.307 = s32[8192]{0} bitcast(s32[16,512]{1,0} %param_2.261), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %param_1.650 = f16[16,512,768]{2,1,0} parameter(1)
  %bitcast.306 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %param_1.650), metadata={op_type="Reshape" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %convert.110 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %bitcast.306), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2/Cast/Cast"}
  ROOT %scatter.0 = f32[30522,768]{1,0} scatter(f32[30522,768]{1,0} %convert.111, s32[8192]{0} %bitcast.307, f32[8192,768]{1,0} %convert.110), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1752, metadata={op_type="UnsortedSegmentSum" op_name="AddN_20/inputs_1"}
}

%fused_computation.130 (param_0.627: f32[16,512], param_1.961: f32[16,512], param_2.512: f16[16,512,768], param_3.315: f32[768], param_4.190: f32[16,512], param_5.142: f16[16,512,768], param_6.133: f32[16,512]) -> f16[16,512,768] {
  %param_2.512 = f16[16,512,768]{2,1,0} parameter(2)
  %convert.116 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_2.512), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/Cast"}
  %param_6.133 = f32[16,512]{1,0} parameter(6)
  %constant_93 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.664 = f32[16,512]{1,0} broadcast(f32[] %constant_93), dimensions={}
  %multiply.402 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_6.133, f32[16,512]{1,0} %broadcast.664), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/variance"}
  %constant_454 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.663 = f32[16,512]{1,0} broadcast(f32[] %constant_454), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.103 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.402, f32[16,512]{1,0} %broadcast.663), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %rsqrt.13 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.103), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/Rsqrt"}
  %broadcast.342 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.13), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %param_3.315 = f32[768]{0} parameter(3)
  %broadcast.341 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_3.315), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.328 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.342, f32[16,512,768]{2,1,0} %broadcast.341), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.151 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.116, f32[16,512,768]{2,1,0} %multiply.328), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_1/Mul"}
  %constant_91 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.340 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_91), dimensions={}
  %broadcast.339 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_93), dimensions={}
  %bitcast.238 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.13), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/Rsqrt"}
  %multiply.150 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.238, f32[16,512,1]{1,0,2} %bitcast.238), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/RsqrtGrad"}
  %multiply.149 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.150, f32[16,512,1]{1,0,2} %bitcast.238), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/RsqrtGrad"}
  %param_1.961 = f32[16,512]{1,0} parameter(1)
  %constant_92 = f32[] constant(-0.5)
  %broadcast.338 = f32[16,512]{1,0} broadcast(f32[] %constant_92), dimensions={}
  %multiply.148 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.961, f32[16,512]{1,0} %broadcast.338), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.237 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.148), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/RsqrtGrad"}
  %multiply.147 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.149, f32[16,512,1]{1,0,2} %bitcast.237), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/RsqrtGrad"}
  %multiply.146 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.339, f32[16,512,1]{1,0,2} %multiply.147), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/truediv"}
  %multiply.145 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.340, f32[16,512,1]{1,0,2} %multiply.146), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/Mul"}
  %bitcast.236 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.145), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/Mul"}
  %broadcast.237 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.236), dimensions={0,1}
  %param_5.142 = f16[16,512,768]{2,1,0} parameter(5)
  %convert.279 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_5.142), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %param_4.190 = f32[16,512]{1,0} parameter(4)
  %multiply.400 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.190, f32[16,512]{1,0} %broadcast.664), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  %broadcast.658 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.400), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %subtract.29 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.279, f32[16,512,768]{2,1,0} %broadcast.658), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %multiply.144 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.237, f32[16,512,768]{2,1,0} %subtract.29), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mul_1"}
  %add.13 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.151, f32[16,512,768]{2,1,0} %multiply.144), metadata={op_type="AddN" op_name="AddN_19"}
  %param_0.627 = f32[16,512]{1,0} parameter(0)
  %multiply.143 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.664, f32[16,512]{1,0} %param_0.627), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/truediv_1"}
  %broadcast.236 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.143), dimensions={0,1}
  %add.12 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.13, f32[16,512,768]{2,1,0} %broadcast.236), metadata={op_type="AddN" op_name="AddN_19"}
  ROOT %convert.112 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.12), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast/Cast"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_mul_2_Sum-reduction.1676 (x.1677: f32[], y.1678: f32[]) -> f32[] {
  %x.1677 = f32[] parameter(0)
  %y.1678 = f32[] parameter(1)
  ROOT %add.1679 = f32[] add(f32[] %x.1677, f32[] %y.1678)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_mul_Sum-reduction.1698 (x.1699: f32[], y.1700: f32[]) -> f32[] {
  %x.1699 = f32[] parameter(0)
  %y.1700 = f32[] parameter(1)
  ROOT %add.1701 = f32[] add(f32[] %x.1699, f32[] %y.1700)
}

%fused_computation.131 (param_0.1172: f32[768], param_1.1300: f32[16,512], param_2.903: f32[16,512], param_3.700: f16[16,512,768], param_4.551: f16[8192,768], param_5.470: f16[8192,768], param_6.459: f16[8192,768], param_7.394: f32[16,512], param_8.321: f32[16,512], param_9.253: f16[16,512,768], param_10.160: f32[16,512], param_11.120: f32[16,512,768], param_12.82: f32[768], param_13.46: f32[16,512], param_14.33: f16[16,512,768]) -> (f32[16,512], f32[16,512], f16[16,512,768]) {
  %param_14.33 = f16[16,512,768]{2,1,0} parameter(14)
  %constant_109_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.356.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_109_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.15.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_14.33, f16[16,512,768]{2,1,0} %broadcast.356.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %param_11.120 = f32[16,512,768]{2,1,0} parameter(11)
  %param_13.46 = f32[16,512]{1,0} parameter(13)
  %constant_465 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1775.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_465), dimensions={}
  %multiply.1031.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_13.46, f32[16,512]{1,0} %broadcast.1775.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_464 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1774.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_464), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.451.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.1031.clone.1, f32[16,512]{1,0} %broadcast.1774.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.115.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.451.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1773.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.115.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_12.82 = f32[768]{0} parameter(12)
  %broadcast.1772.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_12.82), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.1029.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1773.clone.1, f32[16,512,768]{2,1,0} %broadcast.1772.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.1028.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %param_11.120, f32[16,512,768]{2,1,0} %multiply.1029.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1220_clone_1 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1771.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1220_clone_1), dimensions={}
  %broadcast.1770.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_465), dimensions={}
  %bitcast.462.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.115.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %multiply.1027.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.462.clone.1, f32[16,512,1]{1,0,2} %bitcast.462.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.1026.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.1027.clone.1, f32[16,512,1]{1,0,2} %bitcast.462.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %param_10.160 = f32[16,512]{1,0} parameter(10)
  %constant_1218_clone_1 = f32[] constant(-0.5)
  %broadcast.1769.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_1218_clone_1), dimensions={}
  %multiply.1025.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_10.160, f32[16,512]{1,0} %broadcast.1769.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.461.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.1025.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.1024.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.1026.clone.1, f32[16,512,1]{1,0,2} %bitcast.461.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.1023.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1770.clone.1, f32[16,512,1]{1,0,2} %multiply.1024.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/truediv"}
  %multiply.1022.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1771.clone.1, f32[16,512,1]{1,0,2} %multiply.1023.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/Mul"}
  %bitcast.460.clone.1 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.1022.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/Mul"}
  %broadcast.1768.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.460.clone.1), dimensions={0,1}
  %param_9.253 = f16[16,512,768]{2,1,0} parameter(9)
  %convert.638.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_9.253), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_8.321 = f32[16,512]{1,0} parameter(8)
  %multiply.1021.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_8.321, f32[16,512]{1,0} %broadcast.1775.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.1766.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.1021.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %subtract.107.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.638.clone.1, f32[16,512,768]{2,1,0} %broadcast.1766.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.1020.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1768.clone.1, f32[16,512,768]{2,1,0} %subtract.107.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mul_1"}
  %add.450.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.1028.clone.1, f32[16,512,768]{2,1,0} %multiply.1020.clone.1), metadata={op_type="AddN" op_name="AddN_16"}
  %param_7.394 = f32[16,512]{1,0} parameter(7)
  %multiply.1019.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1775.clone.1, f32[16,512]{1,0} %param_7.394), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/truediv_1"}
  %broadcast.1764.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.1019.clone.1), dimensions={0,1}
  %add.449.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.450.clone.1, f32[16,512,768]{2,1,0} %broadcast.1764.clone.1), metadata={op_type="AddN" op_name="AddN_16"}
  %convert.636.clone.1 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.449.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast/Cast"}
  %param_6.459 = f16[8192,768]{1,0} parameter(6)
  %bitcast.241.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_6.459), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum"}
  %add.17.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %convert.636.clone.1, f16[16,512,768]{2,1,0} %bitcast.241.clone.1), metadata={op_type="AddN" op_name="AddN_17"}
  %param_5.470 = f16[8192,768]{1,0} parameter(5)
  %bitcast.240.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_5.470), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum"}
  %add.16.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.17.clone.1, f16[16,512,768]{2,1,0} %bitcast.240.clone.1), metadata={op_type="AddN" op_name="AddN_17"}
  %param_4.551 = f16[8192,768]{1,0} parameter(4)
  %bitcast.239.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_4.551), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum"}
  %add.15.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.16.clone.1, f16[16,512,768]{2,1,0} %bitcast.239.clone.1), metadata={op_type="AddN" op_name="AddN_17"}
  %constant_108_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.355.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_108_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %multiply.157.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %add.15.clone.1, f16[16,512,768]{2,1,0} %broadcast.355.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %constant_107_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.354.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_107_clone_1), dimensions={}
  %select.23.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.15.clone.1, f16[16,512,768]{2,1,0} %multiply.157.clone.1, f16[16,512,768]{2,1,0} %broadcast.354.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul_1"}
  %convert.646 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %select.23.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/Cast"}
  %negate.40 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %convert.646), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/sub/Neg"}
  %param_1.1300 = f32[16,512]{1,0} parameter(1)
  %multiply.409 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1300, f32[16,512]{1,0} %broadcast.1775.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/variance"}
  %add.108 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.409, f32[16,512]{1,0} %broadcast.1774.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %rsqrt.17 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.108), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/Rsqrt"}
  %broadcast.347 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.17), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %param_0.1172 = f32[768]{0} parameter(0)
  %broadcast.346 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.1172), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.329 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.347, f32[16,512,768]{2,1,0} %broadcast.346), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.152 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.40, f32[16,512,768]{2,1,0} %multiply.329), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_2/Mul"}
  %constant_95 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.193 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.152, f32[] %constant_95), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_mul_2_Sum-reduction.1676, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_2/Sum"}
  %param_3.700 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.653.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.700), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %multiply.1042.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.646, f32[16,512,768]{2,1,0} %convert.653.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_1/Mul_1"}
  %param_2.903 = f32[16,512]{1,0} parameter(2)
  %multiply.1041.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.903, f32[16,512]{1,0} %broadcast.1775.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  %broadcast.1791.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.1041.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %multiply.1040.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.40, f32[16,512,768]{2,1,0} %broadcast.1791.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_2/Mul_1"}
  %add.453.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.1042.clone.1, f32[16,512,768]{2,1,0} %multiply.1040.clone.1), metadata={op_type="AddN" op_name="AddN_18"}
  %multiply.153.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.453.clone.1, f32[16,512,768]{2,1,0} %broadcast.346), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul/Mul"}
  %reduce.194.clone.1 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.153.clone.1, f32[] %constant_95), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_batchnorm_mul_Sum-reduction.1698, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul/Sum"}
  ROOT %tuple.4 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.193, f32[16,512]{1,0} %reduce.194.clone.1, f16[16,512,768]{2,1,0} %select.23.clone.1)
}

%fused_computation.136 (param_0.641: f32[768,12,64]) -> f16[768,768] {
  %param_0.641 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.247 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.641), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast"}
  %transpose.140 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.247), dimensions={1,2,0}
  ROOT %bitcast.242 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.140)
}

%fused_computation.137 (param_0.642: f16[16,12,64,512]) -> f16[8192,768] {
  %param_0.642 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.162 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.642), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}
  %copy.96 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{1,3,2,0} %transpose.162), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}
  ROOT %bitcast.243 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.96)
}

%fused_computation.138 (param_0.660: f16[8192,768]) -> f16[16,12,64,512] {
  %param_0.660 = f16[8192,768]{1,0} parameter(0)
  %reshape.439 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.660), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum"}
  %transpose.141 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.439), dimensions={0,2,3,1}
  ROOT %copy.97 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.141)
}

%fused_computation.139 (param_0.639: f32[768,12,64]) -> f16[768,768] {
  %param_0.639 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.245 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.639), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast"}
  %transpose.142 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.245), dimensions={1,2,0}
  ROOT %bitcast.244 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.142)
}

%fused_computation.140 (param_0.640: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.640 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.161 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.640), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %copy.98 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.161), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  ROOT %bitcast.245 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.98)
}

%fused_computation.141 (param_0.637: f32[768,12,64]) -> f16[768,768] {
  %param_0.637 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.244 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.637), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast"}
  %transpose.143 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.244), dimensions={1,2,0}
  ROOT %bitcast.246 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.143)
}

%fused_computation.142 (param_0.638: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.638 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.160 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.638), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  %copy.99 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.160), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  ROOT %bitcast.247 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.99)
}

%fused_computation.143 (param_0.905: f16[8192,768], param_1.984: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.984 = f32[12,64]{1,0} parameter(1)
  %convert.288 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.984), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast"}
  %broadcast.701 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.288), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %param_0.905 = f16[8192,768]{1,0} parameter(0)
  %reshape.446 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.905), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum"}
  %add.114 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.701, f16[16,512,12,64]{1,3,2,0} %reshape.446), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %constant_489 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.699 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_489), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %multiply.420 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %add.114, f16[16,512,12,64]{1,3,2,0} %broadcast.699), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %transpose.144 = f16[16,12,512,64]{2,3,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.420), dimensions={0,2,1,3}
  ROOT %copy.100 = f16[16,12,512,64]{3,2,1,0} copy(f16[16,12,512,64]{2,3,1,0} %transpose.144)
}

%fused_computation.144 (param_0.1164: f32[16,12,512], param_1.1291: f16[16,12,512,512], param_2.867: f32[16,12,512], param_3.665: f16[16,12,512,512], param_4.523: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_4.523 = f16[16,12,512,512]{3,2,1,0} parameter(4)
  %copy.141 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_4.523), metadata={op_name="XLA_Args"}
  %constant_1241 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1788 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1241), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.114 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.141, f16[16,12,512,512]{2,3,1,0} %broadcast.1788), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_3.665 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.140 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.665), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  %constant_1240 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1787 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1240), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.1036 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.140, f16[16,12,512,512]{2,3,1,0} %broadcast.1787), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_1239 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1786 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1239), dimensions={}
  %select.110 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.114, f16[16,12,512,512]{2,3,1,0} %multiply.1036, f16[16,12,512,512]{2,3,1,0} %broadcast.1786), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.1164 = f32[16,12,512]{2,1,0} parameter(0)
  %convert.120 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_0.1164), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  %broadcast.238 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.120), dimensions={0,1,2}, metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/sub"}
  %subtract.3 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %select.110, f16[16,12,512,512]{2,3,1,0} %broadcast.238), metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/sub"}
  %param_1.1291 = f16[16,12,512,512]{2,3,1,0} parameter(1)
  %param_2.867 = f32[16,12,512]{2,1,0} parameter(2)
  %convert.294 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_2.867), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %broadcast.710 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.294), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %divide.70 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_1.1291, f16[16,12,512,512]{2,3,1,0} %broadcast.710), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %multiply.158 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %subtract.3, f16[16,12,512,512]{2,3,1,0} %divide.70), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul_1"}
  ROOT %copy.101 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.158), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul_1"}
}

%fused_computation.145 (param_0.1162: f16[16,12,512,512], param_1.1289: f32[16,12,512], param_2.866: f16[16,12,512,512], param_3.663: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_3.663 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.137 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.663), metadata={op_name="XLA_Args"}
  %constant_1232 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1781 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1232), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.112 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.137, f16[16,12,512,512]{2,3,1,0} %broadcast.1781), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_2.866 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.136 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.866), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  %constant_1231 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1780 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1231), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.1033 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.136, f16[16,12,512,512]{2,3,1,0} %broadcast.1780), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_1229 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1779 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1229), dimensions={}
  %select.108 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.112, f16[16,12,512,512]{2,3,1,0} %multiply.1033, f16[16,12,512,512]{2,3,1,0} %broadcast.1779), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.1162 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.1289 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.292 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.1289), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %broadcast.707 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.292), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %divide.68 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.1162, f16[16,12,512,512]{2,3,1,0} %broadcast.707), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %multiply.159 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.108, f16[16,12,512,512]{2,3,1,0} %divide.68), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul"}
  %convert.121 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %multiply.159), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  ROOT %bitcast.248 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.121), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
}

%fused_computation.147 (param_0.921: f16[8192,768], param_1.1003: f32[12,64]) -> f16[16,12,64,512] {
  %param_1.1003 = f32[12,64]{1,0} parameter(1)
  %convert.298 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.1003), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast"}
  %broadcast.721 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.298), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  %param_0.921 = f16[8192,768]{1,0} parameter(0)
  %reshape.450 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.921), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum"}
  %add.118 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.721, f16[16,512,12,64]{3,1,2,0} %reshape.450), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  %transpose.145 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.118), dimensions={0,2,3,1}
  ROOT %copy.103 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.145)
}

%fused_computation.148 (param_0.661: f32[12,64,768]) -> f16[768,768] {
  %param_0.661 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.248 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.661), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast"}
  %transpose.146 = f16[768,12,64]{0,2,1} transpose(f16[12,64,768]{2,1,0} %convert.248), dimensions={2,0,1}
  ROOT %bitcast.249 = f16[768,768]{0,1} bitcast(f16[768,12,64]{0,2,1} %transpose.146)
}

%fused_computation.149 (param_0.1156: f16[16,512,768], param_1.1282: f32[16,512], param_2.862: f32[16,512], param_3.656: f16[16,512,768], param_4.508: f32[16,512], param_5.438: f32[16,512,768], param_6.439: f32[768], param_7.383: f32[16,512]) -> f16[16,512,768] {
  %param_0.1156 = f16[16,512,768]{2,1,0} parameter(0)
  %constant_139 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.372 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_139), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.16 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_0.1156, f16[16,512,768]{2,1,0} %broadcast.372), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/GreaterEqual"}
  %param_5.438 = f32[16,512,768]{2,1,0} parameter(5)
  %param_7.383 = f32[16,512]{1,0} parameter(7)
  %constant_1202 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1750 = f32[16,512]{1,0} broadcast(f32[] %constant_1202), dimensions={}
  %multiply.1006 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_7.383, f32[16,512]{1,0} %broadcast.1750), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_1206 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1748 = f32[16,512]{1,0} broadcast(f32[] %constant_1206), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.444 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.1006, f32[16,512]{1,0} %broadcast.1748), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.113 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.444), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1747 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.113), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_6.439 = f32[768]{0} parameter(6)
  %broadcast.1746 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_6.439), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.1005 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1747, f32[16,512,768]{2,1,0} %broadcast.1746), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.1004 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %param_5.438, f32[16,512,768]{2,1,0} %multiply.1005), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1205 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1745 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1205), dimensions={}
  %broadcast.1744 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1202), dimensions={}
  %bitcast.456 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.113), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %multiply.1003 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.456, f32[16,512,1]{1,0,2} %bitcast.456), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.1002 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.1003, f32[16,512,1]{1,0,2} %bitcast.456), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %param_4.508 = f32[16,512]{1,0} parameter(4)
  %constant_1203 = f32[] constant(-0.5)
  %broadcast.1743 = f32[16,512]{1,0} broadcast(f32[] %constant_1203), dimensions={}
  %multiply.1001 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.508, f32[16,512]{1,0} %broadcast.1743), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.455 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.1001), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.1000 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.1002, f32[16,512,1]{1,0,2} %bitcast.455), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.999 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1744, f32[16,512,1]{1,0,2} %multiply.1000), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/truediv"}
  %multiply.998 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1745, f32[16,512,1]{1,0,2} %multiply.999), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/Mul"}
  %bitcast.454 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.998), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/Mul"}
  %broadcast.1742 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.454), dimensions={0,1}
  %param_3.656 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.633 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.656), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_2.862 = f32[16,512]{1,0} parameter(2)
  %multiply.996 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.862, f32[16,512]{1,0} %broadcast.1750), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.1740 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.996), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %subtract.105 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.633, f32[16,512,768]{2,1,0} %broadcast.1740), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.995 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1742, f32[16,512,768]{2,1,0} %subtract.105), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mul_1"}
  %add.443 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.1004, f32[16,512,768]{2,1,0} %multiply.995), metadata={op_type="AddN" op_name="AddN_16"}
  %param_1.1282 = f32[16,512]{1,0} parameter(1)
  %multiply.994 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1750, f32[16,512]{1,0} %param_1.1282), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/truediv_1"}
  %broadcast.1738 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.994), dimensions={0,1}
  %add.442 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.443, f32[16,512,768]{2,1,0} %broadcast.1738), metadata={op_type="AddN" op_name="AddN_16"}
  %convert.632 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.442), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast/Cast"}
  %constant_138 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.370 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_138), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %multiply.161 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %convert.632, f16[16,512,768]{2,1,0} %broadcast.370), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/Mul"}
  %constant_137 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.369 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_137), dimensions={}
  ROOT %select.25 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.16, f16[16,512,768]{2,1,0} %multiply.161, f16[16,512,768]{2,1,0} %broadcast.369), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/Mul_1"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_mul_2_Sum-reduction.1522 (x.1523: f32[], y.1524: f32[]) -> f32[] {
  %x.1523 = f32[] parameter(0)
  %y.1524 = f32[] parameter(1)
  ROOT %add.1525 = f32[] add(f32[] %x.1523, f32[] %y.1524)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_mul_Sum-reduction.1544 (x.1545: f32[], y.1546: f32[]) -> f32[] {
  %x.1545 = f32[] parameter(0)
  %y.1546 = f32[] parameter(1)
  ROOT %add.1547 = f32[] add(f32[] %x.1545, f32[] %y.1546)
}

%fused_computation.151 (param_0.1189: f32[768], param_1.1336: f32[16,512], param_2.905: f32[16,512], param_3.705: f16[16,512,768], param_4.561: f16[8192,768], param_5.479: f32[16,512], param_6.468: f32[16,512,768], param_7.399: f32[16,512], param_8.329: f32[768], param_9.262: f32[16,512], param_10.172: f16[16,512,768]) -> (f32[16,512], f32[16,512], f32[16,512,768]) {
  %param_10.172 = f16[16,512,768]{2,1,0} parameter(10)
  %convert.625.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_10.172), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_2/Cast"}
  %param_9.262 = f32[16,512]{1,0} parameter(9)
  %constant_533 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1714.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_533), dimensions={}
  %multiply.966.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_9.262, f32[16,512]{1,0} %broadcast.1714.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  %constant_532 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1712.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_532), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.433.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.966.clone.1, f32[16,512]{1,0} %broadcast.1712.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/add"}
  %rsqrt.111.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.433.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1710.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.111.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %param_8.329 = f32[768]{0} parameter(8)
  %broadcast.1709.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_8.329), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.965.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1710.clone.1, f32[16,512,768]{2,1,0} %broadcast.1709.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.964.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.625.clone.1, f32[16,512,768]{2,1,0} %multiply.965.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1183_clone_1 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1708.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1183_clone_1), dimensions={}
  %broadcast.1707.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_533), dimensions={}
  %bitcast.450.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.111.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %multiply.963.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.450.clone.1, f32[16,512,1]{1,0,2} %bitcast.450.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.962.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.963.clone.1, f32[16,512,1]{1,0,2} %bitcast.450.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %param_7.399 = f32[16,512]{1,0} parameter(7)
  %constant_1181_clone_1 = f32[] constant(-0.5)
  %broadcast.1705.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_1181_clone_1), dimensions={}
  %multiply.961.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_7.399, f32[16,512]{1,0} %broadcast.1705.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.449.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.961.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.960.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.962.clone.1, f32[16,512,1]{1,0,2} %bitcast.449.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.959.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1707.clone.1, f32[16,512,1]{1,0,2} %multiply.960.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/truediv"}
  %multiply.957.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1708.clone.1, f32[16,512,1]{1,0,2} %multiply.959.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/Mul"}
  %bitcast.448.clone.1 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.957.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/Mul"}
  %broadcast.1704.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.448.clone.1), dimensions={0,1}
  %param_6.468 = f32[16,512,768]{2,1,0} parameter(6)
  %multiply.956.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1704.clone.1, f32[16,512,768]{2,1,0} %param_6.468), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mul_1"}
  %add.432.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.964.clone.1, f32[16,512,768]{2,1,0} %multiply.956.clone.1), metadata={op_type="AddN" op_name="AddN_12"}
  %param_5.479 = f32[16,512]{1,0} parameter(5)
  %multiply.955.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1714.clone.1, f32[16,512]{1,0} %param_5.479), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/truediv_1"}
  %broadcast.1702.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.955.clone.1), dimensions={0,1}
  %add.431.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.432.clone.1, f32[16,512,768]{2,1,0} %broadcast.1702.clone.1), metadata={op_type="AddN" op_name="AddN_12"}
  %param_4.561 = f16[8192,768]{1,0} parameter(4)
  %convert.124.clone.1 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_4.561), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_1/Cast"}
  %bitcast.253.clone.1 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.124.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_1/Cast"}
  %add.21.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.431.clone.1, f32[16,512,768]{2,1,0} %bitcast.253.clone.1), metadata={op_type="AddN" op_name="AddN_14"}
  %negate.5 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %add.21.clone.1), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub/Neg"}
  %param_1.1336 = f32[16,512]{1,0} parameter(1)
  %multiply.437 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1336, f32[16,512]{1,0} %broadcast.1714.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %add.124 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.437, f32[16,512]{1,0} %broadcast.1712.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.25 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.124), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.384 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.25), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_0.1189 = f32[768]{0} parameter(0)
  %broadcast.383 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.1189), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.336 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.384, f32[16,512,768]{2,1,0} %broadcast.383), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.171 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.5, f32[16,512,768]{2,1,0} %multiply.336), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2/Mul"}
  %constant_145 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.195 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.171, f32[] %constant_145), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_mul_2_Sum-reduction.1522, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2/Sum"}
  %param_3.705 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.627.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.705), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %multiply.972.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.21.clone.1, f32[16,512,768]{2,1,0} %convert.627.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1/Mul_1"}
  %param_2.905 = f32[16,512]{1,0} parameter(2)
  %multiply.971.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.905, f32[16,512]{1,0} %broadcast.1714.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.1719.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.971.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.970.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.5, f32[16,512,768]{2,1,0} %broadcast.1719.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.435.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.972.clone.1, f32[16,512,768]{2,1,0} %multiply.970.clone.1), metadata={op_type="AddN" op_name="AddN_15"}
  %multiply.173.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.435.clone.1, f32[16,512,768]{2,1,0} %broadcast.383), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul/Mul"}
  %reduce.196.clone.1 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.173.clone.1, f32[] %constant_145), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_batchnorm_mul_Sum-reduction.1544, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul/Sum"}
  ROOT %tuple.8 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.195, f32[16,512]{1,0} %reduce.196.clone.1, f32[16,512,768]{2,1,0} %add.21.clone.1)
}

%fused_computation.155 (param_0.687: f16[16,512,3072], param_1.736: f16[8192,3072], param_2.564: f16[16,512,3072], param_3.356: f16[8192,3072], param_4.218: f32[3072]) -> f16[16,512,3072] {
  %param_4.218 = f32[3072]{0} parameter(4)
  %convert.308 = f16[3072]{0} convert(f32[3072]{0} %param_4.218), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast"}
  %broadcast.797 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.308), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %param_3.356 = f16[8192,3072]{1,0} parameter(3)
  %bitcast.326 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_3.356), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum"}
  %add.136 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.797, f16[16,512,3072]{2,1,0} %bitcast.326), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %multiply.342 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.136, f16[16,512,3072]{2,1,0} %add.136)
  %param_2.564 = f16[16,512,3072]{2,1,0} parameter(2)
  %constant_164 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.401 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_164), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.17 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_2.564, f16[16,512,3072]{2,1,0} %broadcast.401), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %param_1.736 = f16[8192,3072]{1,0} parameter(1)
  %bitcast.254 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_1.736), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %select.26 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.17, f16[16,512,3072]{2,1,0} %bitcast.254, f16[16,512,3072]{2,1,0} %broadcast.401), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/Mul"}
  %constant_163 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.397 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_163), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.341 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.136, f16[16,512,3072]{2,1,0} %broadcast.397), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.184 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.26, f16[16,512,3072]{2,1,0} %multiply.341), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_3/Mul_1"}
  %constant_167 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.399 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_167), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.687 = f16[16,512,3072]{2,1,0} parameter(0)
  %multiply.183 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %param_0.687, f16[16,512,3072]{2,1,0} %param_0.687), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/TanhGrad"}
  %subtract.4 = f16[16,512,3072]{2,1,0} subtract(f16[16,512,3072]{2,1,0} %broadcast.399, f16[16,512,3072]{2,1,0} %multiply.183), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/TanhGrad"}
  %multiply.182 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.184, f16[16,512,3072]{2,1,0} %subtract.4), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/TanhGrad"}
  %constant_165 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.398 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_165), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.181 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.182, f16[16,512,3072]{2,1,0} %broadcast.398), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2/Mul"}
  %multiply.180 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.342, f16[16,512,3072]{2,1,0} %multiply.181), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Pow/mul"}
  %constant_161 = f16[] constant(0.13416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %broadcast.395 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_161), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul_1"}
  %multiply.178 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.180, f16[16,512,3072]{2,1,0} %broadcast.395), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Pow/mul_1"}
  %multiply.177 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.26, f16[16,512,3072]{2,1,0} %broadcast.397), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_3/Mul"}
  %add.86 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.399, f16[16,512,3072]{2,1,0} %param_0.687), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/add_1"}
  %multiply.176 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.177, f16[16,512,3072]{2,1,0} %add.86), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul/Mul"}
  %add.23 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.178, f16[16,512,3072]{2,1,0} %multiply.176), metadata={op_type="AddN" op_name="AddN_13"}
  ROOT %add.22 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %add.23, f16[16,512,3072]{2,1,0} %multiply.181), metadata={op_type="AddN" op_name="AddN_13"}
}

%fused_computation.156 (param_0.1148: f16[16,512,768], param_1.1269: f32[16,512], param_2.836: f32[16,512,768], param_3.632: f32[16,512], param_4.489: f32[768], param_5.419: f32[16,512], param_6.415: f16[16,512,768]) -> f16[16,512,768] {
  %param_0.1148 = f16[16,512,768]{2,1,0} parameter(0)
  %constant_178 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.407 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_178), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.18 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_0.1148, f16[16,512,768]{2,1,0} %broadcast.407), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/GreaterEqual"}
  %param_6.415 = f16[16,512,768]{2,1,0} parameter(6)
  %convert.623 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_6.415), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_2/Cast"}
  %param_5.419 = f32[16,512]{1,0} parameter(5)
  %constant_1170 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1689 = f32[16,512]{1,0} broadcast(f32[] %constant_1170), dimensions={}
  %multiply.943 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.419, f32[16,512]{1,0} %broadcast.1689), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  %constant_1172 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1688 = f32[16,512]{1,0} broadcast(f32[] %constant_1172), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.427 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.943, f32[16,512]{1,0} %broadcast.1688), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/add"}
  %rsqrt.109 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.427), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1686 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.109), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %param_4.489 = f32[768]{0} parameter(4)
  %broadcast.1684 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_4.489), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.942 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1686, f32[16,512,768]{2,1,0} %broadcast.1684), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.941 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.623, f32[16,512,768]{2,1,0} %multiply.942), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1171 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1682 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1171), dimensions={}
  %broadcast.1681 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1170), dimensions={}
  %bitcast.444 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.109), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %multiply.940 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.444, f32[16,512,1]{1,0,2} %bitcast.444), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.939 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.940, f32[16,512,1]{1,0,2} %bitcast.444), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %param_3.632 = f32[16,512]{1,0} parameter(3)
  %constant_1169 = f32[] constant(-0.5)
  %broadcast.1680 = f32[16,512]{1,0} broadcast(f32[] %constant_1169), dimensions={}
  %multiply.938 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.632, f32[16,512]{1,0} %broadcast.1680), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.443 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.938), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.937 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.939, f32[16,512,1]{1,0,2} %bitcast.443), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.936 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1681, f32[16,512,1]{1,0,2} %multiply.937), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/truediv"}
  %multiply.935 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1682, f32[16,512,1]{1,0,2} %multiply.936), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/Mul"}
  %bitcast.442 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.935), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/Mul"}
  %broadcast.1679 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.442), dimensions={0,1}
  %param_2.836 = f32[16,512,768]{2,1,0} parameter(2)
  %multiply.934 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1679, f32[16,512,768]{2,1,0} %param_2.836), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mul_1"}
  %add.426 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.941, f32[16,512,768]{2,1,0} %multiply.934), metadata={op_type="AddN" op_name="AddN_12"}
  %param_1.1269 = f32[16,512]{1,0} parameter(1)
  %multiply.933 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1689, f32[16,512]{1,0} %param_1.1269), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/truediv_1"}
  %broadcast.1677 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.933), dimensions={0,1}
  %add.425 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.426, f32[16,512,768]{2,1,0} %broadcast.1677), metadata={op_type="AddN" op_name="AddN_12"}
  %convert.125 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.425), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_2/Cast"}
  %constant_176 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.406 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_176), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %multiply.185 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %convert.125, f16[16,512,768]{2,1,0} %broadcast.406), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul"}
  %constant_175 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.405 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_175), dimensions={}
  ROOT %select.27 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.18, f16[16,512,768]{2,1,0} %multiply.185, f16[16,512,768]{2,1,0} %broadcast.405), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_mul_2_Sum-reduction.1414 (x.1415: f32[], y.1416: f32[]) -> f32[] {
  %x.1415 = f32[] parameter(0)
  %y.1416 = f32[] parameter(1)
  ROOT %add.1417 = f32[] add(f32[] %x.1415, f32[] %y.1416)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_mul_Sum-reduction.1436 (x.1437: f32[], y.1438: f32[]) -> f32[] {
  %x.1437 = f32[] parameter(0)
  %y.1438 = f32[] parameter(1)
  ROOT %add.1439 = f32[] add(f32[] %x.1437, f32[] %y.1438)
}

%fused_computation.158 (param_0.1140: f32[768], param_1.1259: f32[16,512], param_2.916: f32[16,512], param_3.719: f32[16,512], param_4.577: f32[768], param_5.496: f32[768], param_6.484: f32[16,512], param_7.415: f16[16,512,768], param_8.348: f16[8192,768], param_9.286: f32[768], param_10.195: f16[16,512,768], param_11.147: f16[8192,768], param_12.102: f16[8192,768], param_13.59: f16[8192,768], param_14.40: f32[16,512], param_15.36: f32[16,512], param_16.25: f16[16,512,768], param_17.16: f32[16,512], param_18.12: f32[16,512,768], param_19.15: f32[768], param_20.17: f32[16,512]) -> (f32[16,512], f32[16,512], f16[16,512,768]) {
  %param_18.12 = f32[16,512,768]{2,1,0} parameter(18)
  %param_20.17 = f32[16,512]{1,0} parameter(20)
  %constant_643 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1599.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_643), dimensions={}
  %multiply.880.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_20.17, f32[16,512]{1,0} %broadcast.1599.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_642 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1598.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_642), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.397.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.880.clone.1, f32[16,512]{1,0} %broadcast.1598.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.103.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.397.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1597.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.103.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_19.15 = f32[768]{0} parameter(19)
  %broadcast.1596.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_19.15), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.879.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1597.clone.1, f32[16,512,768]{2,1,0} %broadcast.1596.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.878.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %param_18.12, f32[16,512,768]{2,1,0} %multiply.879.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1101_clone_1 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1595.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1101_clone_1), dimensions={}
  %broadcast.1594.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_643), dimensions={}
  %bitcast.428.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.103.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %multiply.877.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.428.clone.1, f32[16,512,1]{1,0,2} %bitcast.428.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.876.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.877.clone.1, f32[16,512,1]{1,0,2} %bitcast.428.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %param_17.16 = f32[16,512]{1,0} parameter(17)
  %constant_1099_clone_1 = f32[] constant(-0.5)
  %broadcast.1593.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_1099_clone_1), dimensions={}
  %multiply.875.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_17.16, f32[16,512]{1,0} %broadcast.1593.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.427.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.875.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.874.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.876.clone.1, f32[16,512,1]{1,0,2} %bitcast.427.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.873.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1594.clone.1, f32[16,512,1]{1,0,2} %multiply.874.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/truediv"}
  %multiply.872.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1595.clone.1, f32[16,512,1]{1,0,2} %multiply.873.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/Mul"}
  %bitcast.426.clone.1 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.872.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/Mul"}
  %broadcast.1591.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.426.clone.1), dimensions={0,1}
  %param_16.25 = f16[16,512,768]{2,1,0} parameter(16)
  %convert.582.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_16.25), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_15.36 = f32[16,512]{1,0} parameter(15)
  %multiply.871.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_15.36, f32[16,512]{1,0} %broadcast.1599.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1589.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.871.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %subtract.99.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.582.clone.1, f32[16,512,768]{2,1,0} %broadcast.1589.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.870.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1591.clone.1, f32[16,512,768]{2,1,0} %subtract.99.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mul_1"}
  %add.396.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.878.clone.1, f32[16,512,768]{2,1,0} %multiply.870.clone.1), metadata={op_type="AddN" op_name="AddN_9"}
  %param_14.40 = f32[16,512]{1,0} parameter(14)
  %multiply.869.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1599.clone.1, f32[16,512]{1,0} %param_14.40), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/truediv_1"}
  %broadcast.1587.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.869.clone.1), dimensions={0,1}
  %add.395.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.396.clone.1, f32[16,512,768]{2,1,0} %broadcast.1587.clone.1), metadata={op_type="AddN" op_name="AddN_9"}
  %convert.581.clone.1 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.395.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast/Cast"}
  %param_13.59 = f16[8192,768]{1,0} parameter(13)
  %bitcast.260.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_13.59), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum"}
  %add.29.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %convert.581.clone.1, f16[16,512,768]{2,1,0} %bitcast.260.clone.1), metadata={op_type="AddN" op_name="AddN_10"}
  %param_12.102 = f16[8192,768]{1,0} parameter(12)
  %bitcast.259.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_12.102), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum"}
  %add.28.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.29.clone.1, f16[16,512,768]{2,1,0} %bitcast.259.clone.1), metadata={op_type="AddN" op_name="AddN_10"}
  %param_11.147 = f16[8192,768]{1,0} parameter(11)
  %bitcast.258.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_11.147), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum"}
  %add.27.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.28.clone.1, f16[16,512,768]{2,1,0} %bitcast.258.clone.1), metadata={op_type="AddN" op_name="AddN_10"}
  %convert.593 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %add.27.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_2/Cast"}
  %negate.28 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %convert.593), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/sub/Neg"}
  %param_1.1259 = f32[16,512]{1,0} parameter(1)
  %multiply.525 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1259, f32[16,512]{1,0} %broadcast.1599.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  %add.178 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.525, f32[16,512]{1,0} %broadcast.1598.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/add"}
  %rsqrt.45 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.178), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.417 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.45), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %param_0.1140 = f32[768]{0} parameter(0)
  %broadcast.416 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.1140), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.344 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.417, f32[16,512,768]{2,1,0} %broadcast.416), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.196 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.28, f32[16,512,768]{2,1,0} %multiply.344), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_2/Mul"}
  %constant_183 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.197 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.196, f32[] %constant_183), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_mul_2_Sum-reduction.1414, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_2/Sum"}
  %param_10.195 = f16[16,512,768]{2,1,0} parameter(10)
  %constant_1145_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1639.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1145_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.107.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_10.195, f16[16,512,768]{2,1,0} %broadcast.1639.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/GreaterEqual"}
  %constant_1144_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1638.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1144_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_1143_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1637.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1143_clone_1), dimensions={}
  %select.104.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.107.clone.1, f16[16,512,768]{2,1,0} %broadcast.1638.clone.1, f16[16,512,768]{2,1,0} %broadcast.1637.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul"}
  %param_9.286 = f32[768]{0} parameter(9)
  %convert.606.clone.1 = f16[768]{0} convert(f32[768]{0} %param_9.286), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Cast"}
  %broadcast.1636.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.606.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %param_8.348 = f16[8192,768]{1,0} parameter(8)
  %bitcast.436.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_8.348), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %add.408.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1636.clone.1, f16[16,512,768]{2,1,0} %bitcast.436.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %multiply.902.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.104.clone.1, f16[16,512,768]{2,1,0} %add.408.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
  %convert.605.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.902.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_2"}
  %param_7.415 = f16[16,512,768]{2,1,0} parameter(7)
  %convert.604.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_7.415), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_6.484 = f32[16,512]{1,0} parameter(6)
  %multiply.901.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_6.484, f32[16,512]{1,0} %broadcast.1599.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %add.407.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.901.clone.1, f32[16,512]{1,0} %broadcast.1598.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.105.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.407.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1633.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.105.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_5.496 = f32[768]{0} parameter(5)
  %broadcast.1632.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_5.496), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.900.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1633.clone.1, f32[16,512,768]{2,1,0} %broadcast.1632.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.899.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.604.clone.1, f32[16,512,768]{2,1,0} %multiply.900.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1"}
  %param_4.577 = f32[768]{0} parameter(4)
  %broadcast.1631.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_4.577), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %param_3.719 = f32[16,512]{1,0} parameter(3)
  %multiply.898.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.719, f32[16,512]{1,0} %broadcast.1599.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.1628.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.898.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.897.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.900.clone.1, f32[16,512,768]{2,1,0} %broadcast.1628.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.101.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1631.clone.1, f32[16,512,768]{2,1,0} %multiply.897.clone.1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %add.406.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.899.clone.1, f32[16,512,768]{2,1,0} %subtract.101.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add_1"}
  %add.405.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.605.clone.1, f32[16,512,768]{2,1,0} %add.406.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/add_1"}
  %multiply.896.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.593, f32[16,512,768]{2,1,0} %add.405.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_1/Mul_1"}
  %param_2.916 = f32[16,512]{1,0} parameter(2)
  %multiply.895.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.916, f32[16,512]{1,0} %broadcast.1599.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mean"}
  %broadcast.1626.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.895.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/SquaredDifference"}
  %multiply.894.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.28, f32[16,512,768]{2,1,0} %broadcast.1626.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.404.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.896.clone.1, f32[16,512,768]{2,1,0} %multiply.894.clone.1), metadata={op_type="AddN" op_name="AddN_11"}
  %multiply.197.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.404.clone.1, f32[16,512,768]{2,1,0} %broadcast.416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul/Mul"}
  %reduce.198.clone.1 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.197.clone.1, f32[] %constant_183), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_batchnorm_mul_Sum-reduction.1436, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul/Sum"}
  ROOT %tuple.13 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.197, f32[16,512]{1,0} %reduce.198.clone.1, f16[16,512,768]{2,1,0} %add.27.clone.1)
}

%fused_computation.163 (param_0.709: f32[768,12,64]) -> f16[768,768] {
  %param_0.709 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.252 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.709), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast"}
  %transpose.147 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.252), dimensions={1,2,0}
  ROOT %bitcast.261 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.147)
}

%fused_computation.164 (param_0.710: f16[16,12,64,512]) -> f16[8192,768] {
  %param_0.710 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.169 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.710), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}
  %copy.104 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{1,3,2,0} %transpose.169), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}
  ROOT %bitcast.262 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.104)
}

%fused_computation.165 (param_0.728: f16[8192,768]) -> f16[16,12,64,512] {
  %param_0.728 = f16[8192,768]{1,0} parameter(0)
  %reshape.441 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.728), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum"}
  %transpose.148 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.441), dimensions={0,2,3,1}
  ROOT %copy.105 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.148)
}

%fused_computation.166 (param_0.707: f32[768,12,64]) -> f16[768,768] {
  %param_0.707 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.251 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.707), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast"}
  %transpose.149 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.251), dimensions={1,2,0}
  ROOT %bitcast.263 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.149)
}

%fused_computation.167 (param_0.708: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.708 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.168 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.708), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  %copy.106 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.168), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  ROOT %bitcast.264 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.106)
}

%fused_computation.168 (param_0.705: f32[768,12,64]) -> f16[768,768] {
  %param_0.705 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.250 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.705), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast"}
  %transpose.150 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.250), dimensions={1,2,0}
  ROOT %bitcast.265 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.150)
}

%fused_computation.169 (param_0.706: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.706 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.167 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.706), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  %copy.107 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.167), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  ROOT %bitcast.266 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.107)
}

%fused_computation.170 (param_0.971: f16[8192,768], param_1.1059: f32[12,64]) -> f16[16,12,512,64] {
  %constant_658 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.943 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_658), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %param_1.1059 = f32[12,64]{1,0} parameter(1)
  %convert.346 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.1059), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast"}
  %broadcast.942 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.346), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %param_0.971 = f16[8192,768]{1,0} parameter(0)
  %reshape.456 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.971), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum"}
  %add.184 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.942, f16[16,512,12,64]{1,3,2,0} %reshape.456), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %multiply.532 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %broadcast.943, f16[16,512,12,64]{1,3,2,0} %add.184), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  %transpose.151 = f16[16,12,512,64]{2,3,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.532), dimensions={0,2,1,3}
  ROOT %copy.108 = f16[16,12,512,64]{3,2,1,0} copy(f16[16,12,512,64]{2,3,1,0} %transpose.151)
}

%fused_computation.171 (param_0.1132: f32[16,12,512], param_1.1250: f16[16,12,512,512], param_2.815: f32[16,12,512], param_3.609: f16[16,12,512,512], param_4.465: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_4.465 = f16[16,12,512,512]{3,2,1,0} parameter(4)
  %copy.133 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_4.465), metadata={op_name="XLA_Args"}
  %constant_1122 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1612 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1122), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.105 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.133, f16[16,12,512,512]{2,3,1,0} %broadcast.1612), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_3.609 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.132 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.609), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  %constant_1120 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1611 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1120), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.884 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.132, f16[16,12,512,512]{2,3,1,0} %broadcast.1611), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul"}
  %constant_1119 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1610 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1119), dimensions={}
  %select.102 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.105, f16[16,12,512,512]{2,3,1,0} %multiply.884, f16[16,12,512,512]{2,3,1,0} %broadcast.1610), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.1132 = f32[16,12,512]{2,1,0} parameter(0)
  %convert.130 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_0.1132), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  %broadcast.243 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.130), dimensions={0,1,2}, metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %subtract.5 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %select.102, f16[16,12,512,512]{2,3,1,0} %broadcast.243), metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %param_1.1250 = f16[16,12,512,512]{2,3,1,0} parameter(1)
  %param_2.815 = f32[16,12,512]{2,1,0} parameter(2)
  %convert.354 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_2.815), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %broadcast.949 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.354), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %divide.76 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_1.1250, f16[16,12,512,512]{2,3,1,0} %broadcast.949), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %multiply.201 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %subtract.5, f16[16,12,512,512]{2,3,1,0} %divide.76), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul_1"}
  ROOT %copy.109 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.201), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul_1"}
}

%fused_computation.172 (param_0.1130: f16[16,12,512,512], param_1.1248: f32[16,12,512], param_2.814: f16[16,12,512,512], param_3.607: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_3.607 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.129 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.607), metadata={op_name="XLA_Args"}
  %constant_1112 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1605 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1112), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.103 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.129, f16[16,12,512,512]{2,3,1,0} %broadcast.1605), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_2.814 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.128 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.814), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  %constant_1111 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1604 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1111), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.882 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.128, f16[16,12,512,512]{2,3,1,0} %broadcast.1604), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul"}
  %constant_1110 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1603 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_1110), dimensions={}
  %select.100 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.103, f16[16,12,512,512]{2,3,1,0} %multiply.882, f16[16,12,512,512]{2,3,1,0} %broadcast.1603), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.1130 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.1248 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.351 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.1248), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %broadcast.947 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.351), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %divide.74 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.1130, f16[16,12,512,512]{2,3,1,0} %broadcast.947), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %multiply.202 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.100, f16[16,12,512,512]{2,3,1,0} %divide.74), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %convert.131 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %multiply.202), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  ROOT %bitcast.267 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.131), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
}

%fused_computation.174 (param_0.987: f16[8192,768], param_1.1078: f32[12,64]) -> f16[16,12,64,512] {
  %param_1.1078 = f32[12,64]{1,0} parameter(1)
  %convert.359 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.1078), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast"}
  %broadcast.960 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.359), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  %param_0.987 = f16[8192,768]{1,0} parameter(0)
  %reshape.460 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.987), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum"}
  %add.188 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.960, f16[16,512,12,64]{3,1,2,0} %reshape.460), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  %transpose.152 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.188), dimensions={0,2,3,1}
  ROOT %copy.111 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.152)
}

%fused_computation.175 (param_0.729: f32[12,64,768]) -> f16[768,768] {
  %param_0.729 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.253 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.729), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast"}
  %transpose.153 = f16[768,12,64]{0,2,1} transpose(f16[12,64,768]{2,1,0} %convert.253), dimensions={2,0,1}
  ROOT %bitcast.268 = f16[768,768]{0,1} bitcast(f16[768,12,64]{0,2,1} %transpose.153)
}

%fused_computation.176 (param_0.1124: f16[16,512,768], param_1.1241: f32[16,512], param_2.810: f32[16,512], param_3.598: f16[16,512,768], param_4.446: f32[16,512], param_5.382: f32[16,512,768], param_6.380: f32[768], param_7.323: f32[16,512]) -> f16[16,512,768] {
  %param_0.1124 = f16[16,512,768]{2,1,0} parameter(0)
  %constant_221 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.437 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_221), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.19 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_0.1124, f16[16,512,768]{2,1,0} %broadcast.437), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/GreaterEqual"}
  %param_5.382 = f32[16,512,768]{2,1,0} parameter(5)
  %param_7.323 = f32[16,512]{1,0} parameter(7)
  %constant_1082 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1573 = f32[16,512]{1,0} broadcast(f32[] %constant_1082), dimensions={}
  %multiply.856 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_7.323, f32[16,512]{1,0} %broadcast.1573), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_1087 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1572 = f32[16,512]{1,0} broadcast(f32[] %constant_1087), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.390 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.856, f32[16,512]{1,0} %broadcast.1572), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.101 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.390), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1571 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.101), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_6.380 = f32[768]{0} parameter(6)
  %broadcast.1570 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_6.380), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.855 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1571, f32[16,512,768]{2,1,0} %broadcast.1570), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.854 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %param_5.382, f32[16,512,768]{2,1,0} %multiply.855), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1086 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1569 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1086), dimensions={}
  %broadcast.1568 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1082), dimensions={}
  %bitcast.422 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.101), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %multiply.853 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.422, f32[16,512,1]{1,0,2} %bitcast.422), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.852 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.853, f32[16,512,1]{1,0,2} %bitcast.422), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %param_4.446 = f32[16,512]{1,0} parameter(4)
  %constant_1083 = f32[] constant(-0.5)
  %broadcast.1566 = f32[16,512]{1,0} broadcast(f32[] %constant_1083), dimensions={}
  %multiply.851 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.446, f32[16,512]{1,0} %broadcast.1566), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.421 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.851), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.850 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.852, f32[16,512,1]{1,0,2} %bitcast.421), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.849 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1568, f32[16,512,1]{1,0,2} %multiply.850), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/truediv"}
  %multiply.848 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1569, f32[16,512,1]{1,0,2} %multiply.849), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/Mul"}
  %bitcast.420 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.848), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/Mul"}
  %broadcast.1565 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.420), dimensions={0,1}
  %param_3.598 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.578 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.598), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_2.810 = f32[16,512]{1,0} parameter(2)
  %multiply.847 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.810, f32[16,512]{1,0} %broadcast.1573), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1562 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.847), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %subtract.97 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.578, f32[16,512,768]{2,1,0} %broadcast.1562), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.846 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1565, f32[16,512,768]{2,1,0} %subtract.97), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mul_1"}
  %add.389 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.854, f32[16,512,768]{2,1,0} %multiply.846), metadata={op_type="AddN" op_name="AddN_9"}
  %param_1.1241 = f32[16,512]{1,0} parameter(1)
  %multiply.845 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1573, f32[16,512]{1,0} %param_1.1241), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/truediv_1"}
  %broadcast.1558 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.845), dimensions={0,1}
  %add.388 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.389, f32[16,512,768]{2,1,0} %broadcast.1558), metadata={op_type="AddN" op_name="AddN_9"}
  %convert.577 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.388), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast/Cast"}
  %constant_220 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.436 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_220), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %multiply.204 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %convert.577, f16[16,512,768]{2,1,0} %broadcast.436), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/Mul"}
  %constant_219 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.435 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_219), dimensions={}
  ROOT %select.29 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.19, f16[16,512,768]{2,1,0} %multiply.204, f16[16,512,768]{2,1,0} %broadcast.435), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/Mul_1"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_mul_2_Sum-reduction.1264 (x.1265: f32[], y.1266: f32[]) -> f32[] {
  %x.1265 = f32[] parameter(0)
  %y.1266 = f32[] parameter(1)
  ROOT %add.1267 = f32[] add(f32[] %x.1265, f32[] %y.1266)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_mul_Sum-reduction.1286 (x.1287: f32[], y.1288: f32[]) -> f32[] {
  %x.1287 = f32[] parameter(0)
  %y.1288 = f32[] parameter(1)
  ROOT %add.1289 = f32[] add(f32[] %x.1287, f32[] %y.1288)
}

%fused_computation.178 (param_0.1196: f32[768], param_1.1359: f32[16,512], param_2.918: f32[16,512], param_3.724: f16[16,512,768], param_4.587: f16[8192,768], param_5.505: f32[16,512], param_6.493: f32[16,512,768], param_7.420: f32[16,512], param_8.356: f32[768], param_9.295: f32[16,512], param_10.208: f16[16,768], param_11.163: f16[8192,768]) -> (f32[16,512], f32[16,512], f32[16,512,768]) {
  %param_11.163 = f16[8192,768]{1,0} parameter(11)
  %convert.570.clone.1 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_11.163), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %bitcast.416.clone.1 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.570.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %param_10.208 = f16[16,768]{1,0} parameter(10)
  %convert.569.clone.1 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_10.208), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_3/Cast"}
  %bitcast.415.clone.1 = f32[16,1,768]{2,1,0} bitcast(f32[16,768]{1,0} %convert.569.clone.1), metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %constant_226 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %pad.37.clone.1 = f32[16,512,768]{2,1,0} pad(f32[16,1,768]{2,1,0} %bitcast.415.clone.1, f32[] %constant_226), padding=0_0x0_511x0_0, metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %add.378.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %bitcast.416.clone.1, f32[16,512,768]{2,1,0} %pad.37.clone.1), metadata={op_type="AddN" op_name="AddN_3"}
  %param_9.295 = f32[16,512]{1,0} parameter(9)
  %constant_705 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1532.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_705), dimensions={}
  %multiply.820.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_9.295, f32[16,512]{1,0} %broadcast.1532.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %constant_704 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1530.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_704), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.377.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.820.clone.1, f32[16,512]{1,0} %broadcast.1530.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add"}
  %rsqrt.99.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.377.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1528.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.99.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %param_8.356 = f32[768]{0} parameter(8)
  %broadcast.1527.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_8.356), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.819.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1528.clone.1, f32[16,512,768]{2,1,0} %broadcast.1527.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.818.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.378.clone.1, f32[16,512,768]{2,1,0} %multiply.819.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1063_clone_1 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1526.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1063_clone_1), dimensions={}
  %broadcast.1525.clone.1 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_705), dimensions={}
  %bitcast.414.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.99.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %multiply.817.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.414.clone.1, f32[16,512,1]{1,0,2} %bitcast.414.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.816.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.817.clone.1, f32[16,512,1]{1,0,2} %bitcast.414.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %param_7.420 = f32[16,512]{1,0} parameter(7)
  %constant_1061_clone_1 = f32[] constant(-0.5)
  %broadcast.1524.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_1061_clone_1), dimensions={}
  %multiply.815.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_7.420, f32[16,512]{1,0} %broadcast.1524.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.413.clone.1 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.815.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.814.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.816.clone.1, f32[16,512,1]{1,0,2} %bitcast.413.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.813.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1525.clone.1, f32[16,512,1]{1,0,2} %multiply.814.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/truediv"}
  %multiply.811.clone.1 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1526.clone.1, f32[16,512,1]{1,0,2} %multiply.813.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/Mul"}
  %bitcast.412.clone.1 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.811.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/Mul"}
  %broadcast.1523.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.412.clone.1), dimensions={0,1}
  %param_6.493 = f32[16,512,768]{2,1,0} parameter(6)
  %multiply.810.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1523.clone.1, f32[16,512,768]{2,1,0} %param_6.493), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mul_1"}
  %add.376.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.818.clone.1, f32[16,512,768]{2,1,0} %multiply.810.clone.1), metadata={op_type="AddN" op_name="AddN_5"}
  %param_5.505 = f32[16,512]{1,0} parameter(5)
  %multiply.809.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1532.clone.1, f32[16,512]{1,0} %param_5.505), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/truediv_1"}
  %broadcast.1521.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.809.clone.1), dimensions={0,1}
  %add.375.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.376.clone.1, f32[16,512,768]{2,1,0} %broadcast.1521.clone.1), metadata={op_type="AddN" op_name="AddN_5"}
  %param_4.587 = f16[8192,768]{1,0} parameter(4)
  %convert.135.clone.1 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_4.587), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_1/Cast"}
  %bitcast.272.clone.1 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.135.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_1/Cast"}
  %add.33.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.375.clone.1, f32[16,512,768]{2,1,0} %bitcast.272.clone.1), metadata={op_type="AddN" op_name="AddN_7"}
  %negate.7 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %add.33.clone.1), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub/Neg"}
  %param_1.1359 = f32[16,512]{1,0} parameter(1)
  %multiply.548 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1359, f32[16,512]{1,0} %broadcast.1532.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %add.194 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.548, f32[16,512]{1,0} %broadcast.1530.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.53 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.194), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.448 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.53), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_0.1196 = f32[768]{0} parameter(0)
  %broadcast.446 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.1196), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.350 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.448, f32[16,512,768]{2,1,0} %broadcast.446), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.214 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.7, f32[16,512,768]{2,1,0} %multiply.350), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2/Mul"}
  %reduce.199 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.214, f32[] %constant_226), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_mul_2_Sum-reduction.1264, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2/Sum"}
  %param_3.724 = f16[16,512,768]{2,1,0} parameter(3)
  %convert.572.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_3.724), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %multiply.826.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.33.clone.1, f32[16,512,768]{2,1,0} %convert.572.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1/Mul_1"}
  %param_2.918 = f32[16,512]{1,0} parameter(2)
  %multiply.825.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.918, f32[16,512]{1,0} %broadcast.1532.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1537.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.825.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.824.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.7, f32[16,512,768]{2,1,0} %broadcast.1537.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.380.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.826.clone.1, f32[16,512,768]{2,1,0} %multiply.824.clone.1), metadata={op_type="AddN" op_name="AddN_8"}
  %multiply.215.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.380.clone.1, f32[16,512,768]{2,1,0} %broadcast.446), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul/Mul"}
  %reduce.200.clone.1 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.215.clone.1, f32[] %constant_226), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_batchnorm_mul_Sum-reduction.1286, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul/Sum"}
  ROOT %tuple.17 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.199, f32[16,512]{1,0} %reduce.200.clone.1, f32[16,512,768]{2,1,0} %add.33.clone.1)
}

%fused_computation.182 (param_0.755: f16[16,512,3072], param_1.825: f16[8192,3072], param_2.640: f16[16,512,3072], param_3.418: f16[8192,3072], param_4.275: f32[3072]) -> f16[16,512,3072] {
  %constant_241 = f16[] constant(0.13416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %broadcast.460 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_241), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul_1"}
  %param_4.275 = f32[3072]{0} parameter(4)
  %convert.371 = f16[3072]{0} convert(f32[3072]{0} %param_4.275), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast"}
  %broadcast.1035 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.371), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %param_3.418 = f16[8192,3072]{1,0} parameter(3)
  %bitcast.340 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_3.418), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum"}
  %add.206 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.1035, f16[16,512,3072]{2,1,0} %bitcast.340), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %multiply.357 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.206, f16[16,512,3072]{2,1,0} %add.206)
  %param_2.640 = f16[16,512,3072]{2,1,0} parameter(2)
  %constant_243 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.465 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_243), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.20 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_2.640, f16[16,512,3072]{2,1,0} %broadcast.465), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %param_1.825 = f16[8192,3072]{1,0} parameter(1)
  %bitcast.273 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_1.825), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %select.30 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.20, f16[16,512,3072]{2,1,0} %bitcast.273, f16[16,512,3072]{2,1,0} %broadcast.465), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/Mul"}
  %constant_242 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.461 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_242), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.356 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.206, f16[16,512,3072]{2,1,0} %broadcast.461), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul"}
  %multiply.225 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.30, f16[16,512,3072]{2,1,0} %multiply.356), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_3/Mul_1"}
  %constant_246 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.463 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_246), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.755 = f16[16,512,3072]{2,1,0} parameter(0)
  %multiply.224 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %param_0.755, f16[16,512,3072]{2,1,0} %param_0.755), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/TanhGrad"}
  %subtract.6 = f16[16,512,3072]{2,1,0} subtract(f16[16,512,3072]{2,1,0} %broadcast.463, f16[16,512,3072]{2,1,0} %multiply.224), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/TanhGrad"}
  %multiply.223 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.225, f16[16,512,3072]{2,1,0} %subtract.6), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/TanhGrad"}
  %constant_244 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.462 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_244), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.222 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.223, f16[16,512,3072]{2,1,0} %broadcast.462), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_2/Mul"}
  %multiply.221 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.357, f16[16,512,3072]{2,1,0} %multiply.222), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul"}
  %multiply.220 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %broadcast.460, f16[16,512,3072]{2,1,0} %multiply.221), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul_1"}
  %multiply.219 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.30, f16[16,512,3072]{2,1,0} %broadcast.461), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_3/Mul"}
  %add.87 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.463, f16[16,512,3072]{2,1,0} %param_0.755), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %multiply.218 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.219, f16[16,512,3072]{2,1,0} %add.87), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul/Mul"}
  %add.35 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.220, f16[16,512,3072]{2,1,0} %multiply.218), metadata={op_type="AddN" op_name="AddN_6"}
  ROOT %add.34 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %add.35, f16[16,512,3072]{2,1,0} %multiply.222), metadata={op_type="AddN" op_name="AddN_6"}
}

%fused_computation.183 (param_0.1116: f16[16,512,768], param_1.1228: f32[16,512], param_2.784: f32[16,512,768], param_3.574: f32[16,512], param_4.427: f32[768], param_5.363: f32[16,512], param_6.354: f16[16,768], param_7.294: f16[8192,768]) -> f16[16,512,768] {
  %param_0.1116 = f16[16,512,768]{2,1,0} parameter(0)
  %constant_254 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.472 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_254), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.21 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_0.1116, f16[16,512,768]{2,1,0} %broadcast.472), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %param_7.294 = f16[8192,768]{1,0} parameter(7)
  %convert.566 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_7.294), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %bitcast.406 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.566), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %param_6.354 = f16[16,768]{1,0} parameter(6)
  %convert.565 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_6.354), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_3/Cast"}
  %bitcast.405 = f32[16,1,768]{2,1,0} bitcast(f32[16,768]{1,0} %convert.565), metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %constant_1052 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %pad.35 = f32[16,512,768]{2,1,0} pad(f32[16,1,768]{2,1,0} %bitcast.405, f32[] %constant_1052), padding=0_0x0_511x0_0, metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %add.370 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %bitcast.406, f32[16,512,768]{2,1,0} %pad.35), metadata={op_type="AddN" op_name="AddN_3"}
  %param_5.363 = f32[16,512]{1,0} parameter(5)
  %constant_1048 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1510 = f32[16,512]{1,0} broadcast(f32[] %constant_1048), dimensions={}
  %multiply.797 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.363, f32[16,512]{1,0} %broadcast.1510), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %constant_1050 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1509 = f32[16,512]{1,0} broadcast(f32[] %constant_1050), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.368 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.797, f32[16,512]{1,0} %broadcast.1509), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add"}
  %rsqrt.97 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.368), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1508 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.97), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %param_4.427 = f32[768]{0} parameter(4)
  %broadcast.1507 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_4.427), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.796 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1508, f32[16,512,768]{2,1,0} %broadcast.1507), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.795 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.370, f32[16,512,768]{2,1,0} %multiply.796), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_1/Mul"}
  %constant_1049 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.1506 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1049), dimensions={}
  %broadcast.1505 = f32[16,512,1]{1,0,2} broadcast(f32[] %constant_1048), dimensions={}
  %bitcast.404 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %rsqrt.97), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %multiply.794 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %bitcast.404, f32[16,512,1]{1,0,2} %bitcast.404), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.793 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.794, f32[16,512,1]{1,0,2} %bitcast.404), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %param_3.574 = f32[16,512]{1,0} parameter(3)
  %constant_1047 = f32[] constant(-0.5)
  %broadcast.1504 = f32[16,512]{1,0} broadcast(f32[] %constant_1047), dimensions={}
  %multiply.792 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.574, f32[16,512]{1,0} %broadcast.1504), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %bitcast.403 = f32[16,512,1]{1,0,2} bitcast(f32[16,512]{1,0} %multiply.792), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.791 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %multiply.793, f32[16,512,1]{1,0,2} %bitcast.403), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/RsqrtGrad"}
  %multiply.790 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1505, f32[16,512,1]{1,0,2} %multiply.791), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/truediv"}
  %multiply.789 = f32[16,512,1]{1,0,2} multiply(f32[16,512,1]{1,0,2} %broadcast.1506, f32[16,512,1]{1,0,2} %multiply.790), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/Mul"}
  %bitcast.402 = f32[16,512]{1,0} bitcast(f32[16,512,1]{1,0,2} %multiply.789), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/Mul"}
  %broadcast.1503 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %bitcast.402), dimensions={0,1}
  %param_2.784 = f32[16,512,768]{2,1,0} parameter(2)
  %multiply.788 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1503, f32[16,512,768]{2,1,0} %param_2.784), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mul_1"}
  %add.367 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.795, f32[16,512,768]{2,1,0} %multiply.788), metadata={op_type="AddN" op_name="AddN_5"}
  %param_1.1228 = f32[16,512]{1,0} parameter(1)
  %multiply.787 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %broadcast.1510, f32[16,512]{1,0} %param_1.1228), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/truediv_1"}
  %broadcast.1501 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.787), dimensions={0,1}
  %add.366 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %add.367, f32[16,512,768]{2,1,0} %broadcast.1501), metadata={op_type="AddN" op_name="AddN_5"}
  %convert.136 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.366), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2/Cast"}
  %constant_253 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.470 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_253), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %multiply.226 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %convert.136, f16[16,512,768]{2,1,0} %broadcast.470), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_252 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.469 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_252), dimensions={}
  ROOT %select.31 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.21, f16[16,512,768]{2,1,0} %multiply.226, f16[16,512,768]{2,1,0} %broadcast.469), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_mul_2_Sum-reduction.1156 (x.1157: f32[], y.1158: f32[]) -> f32[] {
  %x.1157 = f32[] parameter(0)
  %y.1158 = f32[] parameter(1)
  ROOT %add.1159 = f32[] add(f32[] %x.1157, f32[] %y.1158)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_mul_Sum-reduction.1178 (x.1179: f32[], y.1180: f32[]) -> f32[] {
  %x.1179 = f32[] parameter(0)
  %y.1180 = f32[] parameter(1)
  ROOT %add.1181 = f32[] add(f32[] %x.1179, f32[] %y.1180)
}

%fused_computation.185 (param_0.1107: f32[768], param_1.1212: f32[16,512], param_2.763: f16[16,768], param_3.550: f16[8192,768], param_4.600: f32[16,512], param_5.520: f32[16,512], param_6.509: f32[768], param_7.435: f32[768], param_8.371: f32[16,512], param_9.313: f16[16,512,768], param_10.231: f16[8192,768], param_11.185: f32[768], param_12.126: f16[16,512,768]) -> (f32[16,512], f32[16,512]) {
  %param_3.550 = f16[8192,768]{1,0} parameter(3)
  %convert.535 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_3.550), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %bitcast.380 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.535), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %param_2.763 = f16[16,768]{1,0} parameter(2)
  %convert.534 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_2.763), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_3/Cast"}
  %bitcast.379 = f32[16,1,768]{2,1,0} bitcast(f32[16,768]{1,0} %convert.534), metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %constant_260 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %pad.27 = f32[16,512,768]{2,1,0} pad(f32[16,1,768]{2,1,0} %bitcast.379, f32[] %constant_260), padding=0_0x0_511x0_0, metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %add.332 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %bitcast.380, f32[16,512,768]{2,1,0} %pad.27), metadata={op_type="AddN" op_name="AddN_3"}
  %negate.9 = f32[16,512,768]{2,1,0} negate(f32[16,512,768]{2,1,0} %add.332), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub/Neg"}
  %param_1.1212 = f32[16,512]{1,0} parameter(1)
  %constant_818 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1171 = f32[16,512]{1,0} broadcast(f32[] %constant_818), dimensions={}
  %multiply.645 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1212, f32[16,512]{1,0} %broadcast.1171), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %constant_817 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1169 = f32[16,512]{1,0} broadcast(f32[] %constant_817), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.251 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.645, f32[16,512]{1,0} %broadcast.1169), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add"}
  %rsqrt.73 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.251), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.482 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.73), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %param_0.1107 = f32[768]{0} parameter(0)
  %broadcast.481 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.1107), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.359 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.482, f32[16,512,768]{2,1,0} %broadcast.481), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.236 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.9, f32[16,512,768]{2,1,0} %multiply.359), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2/Mul"}
  %reduce.201 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.236, f32[] %constant_260), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_mul_2_Sum-reduction.1156, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2/Sum"}
  %param_12.126 = f16[16,512,768]{2,1,0} parameter(12)
  %constant_1018_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1458.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1018_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.99.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_12.126, f16[16,512,768]{2,1,0} %broadcast.1458.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %constant_1017_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1457.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1017_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_1016_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1455.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_1016_clone_1), dimensions={}
  %select.96.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.99.clone.1, f16[16,512,768]{2,1,0} %broadcast.1457.clone.1, f16[16,512,768]{2,1,0} %broadcast.1455.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %param_11.185 = f32[768]{0} parameter(11)
  %convert.549.clone.1 = f16[768]{0} convert(f32[768]{0} %param_11.185), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast"}
  %broadcast.1454.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.549.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %param_10.231 = f16[8192,768]{1,0} parameter(10)
  %bitcast.388.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_10.231), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %add.347.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1454.clone.1, f16[16,512,768]{2,1,0} %bitcast.388.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %multiply.755.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.96.clone.1, f16[16,512,768]{2,1,0} %add.347.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %convert.548.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.755.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2"}
  %param_9.313 = f16[16,512,768]{2,1,0} parameter(9)
  %convert.547.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_9.313), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_8.371 = f32[16,512]{1,0} parameter(8)
  %multiply.754.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_8.371, f32[16,512]{1,0} %broadcast.1171), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %add.346.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.754.clone.1, f32[16,512]{1,0} %broadcast.1169), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.93.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.346.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1448.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.93.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_7.435 = f32[768]{0} parameter(7)
  %broadcast.1447.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_7.435), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.753.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1448.clone.1, f32[16,512,768]{2,1,0} %broadcast.1447.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.752.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.547.clone.1, f32[16,512,768]{2,1,0} %multiply.753.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_6.509 = f32[768]{0} parameter(6)
  %broadcast.1446.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_6.509), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %param_5.520 = f32[16,512]{1,0} parameter(5)
  %multiply.751.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.520, f32[16,512]{1,0} %broadcast.1171), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1443.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.751.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.750.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.753.clone.1, f32[16,512,768]{2,1,0} %broadcast.1443.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.93.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1446.clone.1, f32[16,512,768]{2,1,0} %multiply.750.clone.1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.345.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.752.clone.1, f32[16,512,768]{2,1,0} %subtract.93.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %add.344.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.548.clone.1, f32[16,512,768]{2,1,0} %add.345.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add_1"}
  %multiply.749.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.332, f32[16,512,768]{2,1,0} %add.344.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_1/Mul_1"}
  %param_4.600 = f32[16,512]{1,0} parameter(4)
  %multiply.748.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.600, f32[16,512]{1,0} %broadcast.1171), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
  %broadcast.1441.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.748.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %multiply.747.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %negate.9, f32[16,512,768]{2,1,0} %broadcast.1441.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2/Mul_1"}
  %add.343.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.749.clone.1, f32[16,512,768]{2,1,0} %multiply.747.clone.1), metadata={op_type="AddN" op_name="AddN_4"}
  %multiply.237.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.343.clone.1, f32[16,512,768]{2,1,0} %broadcast.481), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul/Mul"}
  %reduce.202.clone.1 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.237.clone.1, f32[] %constant_260), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_batchnorm_mul_Sum-reduction.1178, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul/Sum"}
  ROOT %tuple.21 = (f32[16,512]{1,0}, f32[16,512]{1,0}) tuple(f32[16,512]{1,0} %reduce.201, f32[16,512]{1,0} %reduce.202.clone.1)
}

%fused_computation.189 (param_0.403: f16[16,768], param_1.406: f16[16,768]) -> f16[16,768] {
  %param_0.403 = f16[16,768]{1,0} parameter(0)
  %constant_278 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.249 = f16[16,768]{1,0} broadcast(f16[] %constant_278), dimensions={}, metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  %param_1.406 = f16[16,768]{1,0} parameter(1)
  %multiply.241 = f16[16,768]{1,0} multiply(f16[16,768]{1,0} %param_1.406, f16[16,768]{1,0} %param_1.406), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  %subtract.7 = f16[16,768]{1,0} subtract(f16[16,768]{1,0} %broadcast.249, f16[16,768]{1,0} %multiply.241), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  ROOT %multiply.240 = f16[16,768]{1,0} multiply(f16[16,768]{1,0} %param_0.403, f16[16,768]{1,0} %subtract.7), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
}

%fused_computation.190 (param_0.1089: f32[16,2], param_1.1188: f32[16], param_2.744: f32[], param_3.537: s32[16,1]) -> f16[16,2] {
  %constant_283 = f32[] constant(0.0625)
  %param_2.744 = f32[] parameter(2)
  %multiply.243 = f32[] multiply(f32[] %constant_283, f32[] %param_2.744), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/truediv"}
  %broadcast.251 = f32[16,2]{1,0} broadcast(f32[] %multiply.243), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/mul"}
  %param_0.1089 = f32[16,2]{1,0} parameter(0)
  %param_1.1188 = f32[16]{0} parameter(1)
  %broadcast.250 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_1.1188), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %divide.2 = f32[16,2]{1,0} divide(f32[16,2]{1,0} %param_0.1089, f32[16,2]{1,0} %broadcast.250), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %param_3.537 = s32[16,1]{1,0} parameter(3)
  %convert.502 = f32[16,1]{1,0} convert(s32[16,1]{1,0} %param_3.537), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_5"}
  %convert.501 = s64[16,1]{1,0} convert(f32[16,1]{1,0} %convert.502), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %bitcast.368 = s64[16]{0} bitcast(s64[16,1]{1,0} %convert.501), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %broadcast.1408 = s64[16,2]{1,0} broadcast(s64[16]{0} %bitcast.368), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.16 = s64[16,2]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.97 = pred[16,2]{1,0} compare(s64[16,2]{1,0} %broadcast.1408, s64[16,2]{1,0} %iota.16), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_980 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1407 = f32[16,2]{1,0} broadcast(f32[] %constant_980), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_979 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.1406 = f32[16,2]{1,0} broadcast(f32[] %constant_979), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.94 = f32[16,2]{1,0} select(pred[16,2]{1,0} %compare.97, f32[16,2]{1,0} %broadcast.1407, f32[16,2]{1,0} %broadcast.1406), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_978 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1405 = s64[16]{0} broadcast(s64[] %constant_978), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.96 = pred[16]{0} compare(s64[16]{0} %broadcast.1405, s64[16]{0} %bitcast.368), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_977 = s64[] constant(2), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1404 = s64[16]{0} broadcast(s64[] %constant_977), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.95 = pred[16]{0} compare(s64[16]{0} %bitcast.368, s64[16]{0} %broadcast.1404), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %and.103 = pred[16]{0} and(pred[16]{0} %compare.96, pred[16]{0} %compare.95), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1403 = f32[16]{0} broadcast(f32[] %constant_979), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_976 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1402 = f32[16]{0} broadcast(f32[] %constant_976), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.93 = f32[16]{0} select(pred[16]{0} %and.103, f32[16]{0} %broadcast.1403, f32[16]{0} %broadcast.1402), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1401 = f32[16,2]{1,0} broadcast(f32[16]{0} %select.93), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %add.313 = f32[16,2]{1,0} add(f32[16,2]{1,0} %select.94, f32[16,2]{1,0} %broadcast.1401), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.8 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %divide.2, f32[16,2]{1,0} %add.313), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.242 = f32[16,2]{1,0} multiply(f32[16,2]{1,0} %broadcast.251, f32[16,2]{1,0} %subtract.8), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/mul"}
  ROOT %convert.140 = f16[16,2]{1,0} convert(f32[16,2]{1,0} %multiply.242), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_4/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%max_float_.892 (x.893: f32[], y.894: f32[]) -> f32[] {
  %x.893 = f32[] parameter(0)
  %y.894 = f32[] parameter(1)
  ROOT %maximum.895 = f32[] maximum(f32[] %x.893, f32[] %y.894)
}

%fused_computation.193 (param_0.1092: f32[2], param_1.1193: f16[16,8]) -> f32[16] {
  %param_1.1193 = f16[16,8]{1,0} parameter(1)
  %slice.25 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_1.1193), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_0.1092 = f32[2]{0} parameter(0)
  %convert.504 = f16[2]{0} convert(f32[2]{0} %param_0.1092), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.1410 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.504), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.315 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.25, f16[16,2]{1,0} %broadcast.1410), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.150 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.315), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %constant_284 = f32[] constant(-inf), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %reduce.203 = f32[16]{0} reduce(f32[16,2]{1,0} %convert.150, f32[] %constant_284), dimensions={1}, to_apply=%max_float_.892, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
}

%fused_computation.195 (param_0.786: f32[768,2]) -> f16[768,8] {
  %param_0.786 = f32[768,2]{1,0} parameter(0)
  %convert.153 = f16[768,2]{1,0} convert(f32[768,2]{1,0} %param_0.786), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul/Cast"}
  %constant_285 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.17 = f16[768,8]{1,0} pad(f16[768,2]{1,0} %convert.153, f16[] %constant_285), padding=0_0x0_6, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
}

%fused_computation.196 (param_0.418: f32[768]) -> f16[16,768] {
  %param_0.418 = f32[768]{0} parameter(0)
  %convert.154 = f16[768]{0} convert(f32[768]{0} %param_0.418), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/Cast"}
  ROOT %broadcast.262 = f16[16,768]{1,0} broadcast(f16[768]{0} %convert.154), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd"}
}

%fused_computation.197 (param_0.1037: f32[16,512], param_1.1132: f32[768], param_2.681: f32[768], param_3.468: f32[16,512], param_4.324: f32[16,512], param_5.283: f32[768], param_6.285: f32[768], param_7.241: f32[16,512], param_8.190: f16[16,512,768], param_9.141: f16[8192,768], param_10.80: f32[768], param_11.65: f16[16,512,768]) -> f16[16,768] {
  %param_11.65 = f16[16,512,768]{2,1,0} parameter(11)
  %constant_864 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1259 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_864), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.69 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_11.65, f16[16,512,768]{2,1,0} %broadcast.1259), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %constant_863 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1258 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_863), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_862 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1257 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_862), dimensions={}
  %select.74 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.69, f16[16,512,768]{2,1,0} %broadcast.1258, f16[16,512,768]{2,1,0} %broadcast.1257), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %param_10.80 = f32[768]{0} parameter(10)
  %convert.413 = f16[768]{0} convert(f32[768]{0} %param_10.80), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast"}
  %broadcast.1256 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.413), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %param_9.141 = f16[8192,768]{1,0} parameter(9)
  %bitcast.356 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_9.141), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %add.279 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1256, f16[16,512,768]{2,1,0} %bitcast.356), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %multiply.696 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.74, f16[16,512,768]{2,1,0} %add.279), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %convert.411 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.696), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2"}
  %param_8.190 = f16[16,512,768]{2,1,0} parameter(8)
  %convert.410 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_8.190), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_7.241 = f32[16,512]{1,0} parameter(7)
  %constant_855 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1255 = f32[16,512]{1,0} broadcast(f32[] %constant_855), dimensions={}
  %multiply.695 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_7.241, f32[16,512]{1,0} %broadcast.1255), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_856 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1254 = f32[16,512]{1,0} broadcast(f32[] %constant_856), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.278 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.695, f32[16,512]{1,0} %broadcast.1254), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.83 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.278), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1253 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.83), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_6.285 = f32[768]{0} parameter(6)
  %broadcast.1252 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_6.285), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.694 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1253, f32[16,512,768]{2,1,0} %broadcast.1252), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.693 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.410, f32[16,512,768]{2,1,0} %multiply.694), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_5.283 = f32[768]{0} parameter(5)
  %broadcast.1251 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_5.283), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %param_4.324 = f32[16,512]{1,0} parameter(4)
  %multiply.692 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.324, f32[16,512]{1,0} %broadcast.1255), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1249 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.692), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.691 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.694, f32[16,512,768]{2,1,0} %broadcast.1249), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.75 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1251, f32[16,512,768]{2,1,0} %multiply.691), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.277 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.693, f32[16,512,768]{2,1,0} %subtract.75), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %add.276 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.411, f32[16,512,768]{2,1,0} %add.277), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add_1"}
  %param_3.468 = f32[16,512]{1,0} parameter(3)
  %multiply.690 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.468, f32[16,512]{1,0} %broadcast.1255), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %add.275 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.690, f32[16,512]{1,0} %broadcast.1254), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add"}
  %rsqrt.82 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.275), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1246 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.82), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %param_2.681 = f32[768]{0} parameter(2)
  %broadcast.1245 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.681), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.689 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1246, f32[16,512,768]{2,1,0} %broadcast.1245), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.688 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.276, f32[16,512,768]{2,1,0} %multiply.689), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_1"}
  %param_1.1132 = f32[768]{0} parameter(1)
  %broadcast.1244 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_1.1132), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub"}
  %param_0.1037 = f32[16,512]{1,0} parameter(0)
  %multiply.687 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1037, f32[16,512]{1,0} %broadcast.1255), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
  %broadcast.1242 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.687), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %multiply.686 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.689, f32[16,512,768]{2,1,0} %broadcast.1242), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2"}
  %subtract.74 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1244, f32[16,512,768]{2,1,0} %multiply.686), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub"}
  %add.274 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.688, f32[16,512,768]{2,1,0} %subtract.74), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add_1"}
  %slice.12 = f32[16,1,768]{2,1,0} slice(f32[16,512,768]{2,1,0} %add.274), slice={[0:16], [0:1], [0:768]}, metadata={op_type="StridedSlice" op_name="model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice"}
  %convert.155 = f16[16,1,768]{2,1,0} convert(f32[16,1,768]{2,1,0} %slice.12), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_3"}
  ROOT %bitcast.280 = f16[16,768]{1,0} bitcast(f16[16,1,768]{2,1,0} %convert.155), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_3"}
}

%scatter-combiner.1141 (p0.1142: f16[], p1.1143: f16[]) -> f16[] {
  %p0.1142 = f16[] parameter(0)
  %p1.1143 = f16[] parameter(1)
  ROOT %add.1144 = f16[] add(f16[] %p0.1142, f16[] %p1.1143)
}

%fused_computation.198 (param_0.879: f16[1216,768], param_1.950: s32[16,76], param_2.504: s32[16]) -> f16[8192,768] {
  %constant_272 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.263 = f16[8192,768]{1,0} broadcast(f16[] %constant_272), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/UnsortedSegmentSum"}
  %param_1.950 = s32[16,76]{1,0} parameter(1)
  %param_2.504 = s32[16]{0} parameter(2)
  %broadcast.633 = s32[16,76]{1,0} broadcast(s32[16]{0} %param_2.504), dimensions={0}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %add.100 = s32[16,76]{1,0} add(s32[16,76]{1,0} %param_1.950, s32[16,76]{1,0} %broadcast.633), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %bitcast.324 = s32[1216]{0} bitcast(s32[16,76]{1,0} %add.100), metadata={op_type="Reshape" op_name="model/bert_pretrainer/cls/predictions/Reshape_1"}
  %param_0.879 = f16[1216,768]{1,0} parameter(0)
  ROOT %scatter.1 = f16[8192,768]{1,0} scatter(f16[8192,768]{1,0} %broadcast.263, s32[1216]{0} %bitcast.324, f16[1216,768]{1,0} %param_0.879), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1141, metadata={op_type="UnsortedSegmentSum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/UnsortedSegmentSum"}
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_2_Sum-reduction.1056 (x.1057: f32[], y.1058: f32[]) -> f32[] {
  %x.1057 = f32[] parameter(0)
  %y.1058 = f32[] parameter(1)
  ROOT %add.1059 = f32[] add(f32[] %x.1057, f32[] %y.1058)
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum-reduction.1078 (x.1079: f32[], y.1080: f32[]) -> f32[] {
  %x.1079 = f32[] parameter(0)
  %y.1080 = f32[] parameter(1)
  ROOT %add.1081 = f32[] add(f32[] %x.1079, f32[] %y.1080)
}

%fused_computation.200 (param_0.1080: f32[768], param_1.1180: f32[1216], param_2.726: f16[1216,768], param_3.746: f32[1216], param_4.618: f16[1216,768]) -> (f32[1216], f32[1216]) {
  %param_2.726 = f16[1216,768]{1,0} parameter(2)
  %convert.478 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_2.726), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1/Cast"}
  %negate.12 = f32[1216,768]{1,0} negate(f32[1216,768]{1,0} %convert.478), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub/Neg"}
  %constant_902 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1297 = f32[1216]{0} broadcast(f32[] %constant_902), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %param_1.1180 = f32[1216]{0} parameter(1)
  %constant_900 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1296 = f32[1216]{0} broadcast(f32[] %constant_900), dimensions={}
  %multiply.715 = f32[1216]{0} multiply(f32[1216]{0} %param_1.1180, f32[1216]{0} %broadcast.1296), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %add.286 = f32[1216]{0} add(f32[1216]{0} %broadcast.1297, f32[1216]{0} %multiply.715), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %rsqrt.89 = f32[1216]{0} rsqrt(f32[1216]{0} %add.286), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %broadcast.504 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %rsqrt.89), dimensions={0}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %param_0.1080 = f32[768]{0} parameter(0)
  %broadcast.503 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_0.1080), dimensions={1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.367 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.504, f32[1216,768]{1,0} %broadcast.503), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.262 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %negate.12, f32[1216,768]{1,0} %multiply.367), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Mul"}
  %constant_308 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.204 = f32[1216]{0} reduce(f32[1216,768]{1,0} %multiply.262, f32[] %constant_308), dimensions={1}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_2_Sum-reduction.1056, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Sum"}
  %param_4.618 = f16[1216,768]{1,0} parameter(4)
  %convert.485.clone.1 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_4.618), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %multiply.731.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.478, f32[1216,768]{1,0} %convert.485.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1/Mul_1"}
  %param_3.746 = f32[1216]{0} parameter(3)
  %multiply.730.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_3.746, f32[1216]{0} %broadcast.1296), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.1370.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.730.clone.1), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.729.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %negate.12, f32[1216,768]{1,0} %broadcast.1370.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Mul_1"}
  %add.307.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.731.clone.1, f32[1216,768]{1,0} %multiply.729.clone.1), metadata={op_type="AddN" op_name="AddN"}
  %multiply.263.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %add.307.clone.1, f32[1216,768]{1,0} %broadcast.503), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Mul"}
  %reduce.205.clone.1 = f32[1216]{0} reduce(f32[1216,768]{1,0} %multiply.263.clone.1, f32[] %constant_308), dimensions={1}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum-reduction.1078, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Sum"}
  ROOT %tuple.29 = (f32[1216]{0}, f32[1216]{0}) tuple(f32[1216]{0} %reduce.204, f32[1216]{0} %reduce.205.clone.1)
}

%max_float_.740 (x.741: f32[], y.742: f32[]) -> f32[] {
  %x.741 = f32[] parameter(0)
  %y.742 = f32[] parameter(1)
  ROOT %maximum.743 = f32[] maximum(f32[] %x.741, f32[] %y.742)
}

%fused_computation.207 (param_0.1053: f32[30522], param_1.1150: f16[1216,30528]) -> f32[1216] {
  %param_1.1150 = f16[1216,30528]{1,0} parameter(1)
  %slice.15 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_1.1150), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_0.1053 = f32[30522]{0} parameter(0)
  %convert.421 = f16[30522]{0} convert(f32[30522]{0} %param_0.1053), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.1307 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.421), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.291 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.15, f16[1216,30522]{1,0} %broadcast.1307), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.173 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.291), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %constant_322 = f32[] constant(-inf), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %reduce.206 = f32[1216]{0} reduce(f32[1216,30522]{1,0} %convert.173, f32[] %constant_322), dimensions={1}, to_apply=%max_float_.740, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
}

%fused_computation.209 (param_0.1041: f32[768], param_1.1135: f16[1216,768], param_2.684: f32[768], param_3.474: f32[1216], param_4.333: f32[1216]) -> f16[1216,768] {
  %param_1.1135 = f16[1216,768]{1,0} parameter(1)
  %convert.177 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_1.1135), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %constant_896 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1292 = f32[1216]{0} broadcast(f32[] %constant_896), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %param_4.333 = f32[1216]{0} parameter(4)
  %constant_872 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1291 = f32[1216]{0} broadcast(f32[] %constant_872), dimensions={}
  %multiply.713 = f32[1216]{0} multiply(f32[1216]{0} %param_4.333, f32[1216]{0} %broadcast.1291), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %add.284 = f32[1216]{0} add(f32[1216]{0} %broadcast.1292, f32[1216]{0} %multiply.713), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %rsqrt.87 = f32[1216]{0} rsqrt(f32[1216]{0} %add.284), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %broadcast.512 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %rsqrt.87), dimensions={0}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %param_2.684 = f32[768]{0} parameter(2)
  %broadcast.511 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_2.684), dimensions={1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.372 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.512, f32[1216,768]{1,0} %broadcast.511), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.269 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.177, f32[1216,768]{1,0} %multiply.372), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1"}
  %param_0.1041 = f32[768]{0} parameter(0)
  %broadcast.290 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_0.1041), dimensions={1}, metadata={op_type="Sub" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub"}
  %param_3.474 = f32[1216]{0} parameter(3)
  %multiply.700 = f32[1216]{0} multiply(f32[1216]{0} %param_3.474, f32[1216]{0} %broadcast.1291), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.1266 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.700), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.268 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %multiply.372, f32[1216,768]{1,0} %broadcast.1266), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2"}
  %subtract.13 = f32[1216,768]{1,0} subtract(f32[1216,768]{1,0} %broadcast.290, f32[1216,768]{1,0} %multiply.268), metadata={op_type="Sub" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub"}
  %add.49 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.269, f32[1216,768]{1,0} %subtract.13), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add_1"}
  ROOT %convert.175 = f16[1216,768]{1,0} convert(f32[1216,768]{1,0} %add.49), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1"}
}

%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_variance-reduction.685 (x.686: f32[], y.687: f32[]) -> f32[] {
  %x.686 = f32[] parameter(0)
  %y.687 = f32[] parameter(1)
  ROOT %add.688 = f32[] add(f32[] %x.686, f32[] %y.687)
}

%fused_computation.211 (param_0.1045: f32[1216], param_1.1141: f16[1216,768]) -> f32[1216] {
  %param_1.1141 = f16[1216,768]{1,0} parameter(1)
  %convert.417 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_1.1141), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %param_0.1045 = f32[1216]{0} parameter(0)
  %constant_880 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1279 = f32[1216]{0} broadcast(f32[] %constant_880), dimensions={}
  %multiply.706 = f32[1216]{0} multiply(f32[1216]{0} %param_0.1045, f32[1216]{0} %broadcast.1279), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.1277 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.706), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %subtract.77 = f32[1216,768]{1,0} subtract(f32[1216,768]{1,0} %convert.417, f32[1216,768]{1,0} %broadcast.1277), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.271 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %subtract.77, f32[1216,768]{1,0} %subtract.77), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %constant_326 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.207 = f32[1216]{0} reduce(f32[1216,768]{1,0} %multiply.271, f32[] %constant_326), dimensions={1}, to_apply=%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_variance-reduction.685, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
}

%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_mean-reduction.667 (x.668: f32[], y.669: f32[]) -> f32[] {
  %x.668 = f32[] parameter(0)
  %y.669 = f32[] parameter(1)
  ROOT %add.670 = f32[] add(f32[] %x.668, f32[] %y.669)
}

%fused_computation.214 (param_0.1216: f16[1216,768]) -> (f32[1216], f16[1216,768], f16[1216,768]) {
  %constant_330_clone_1 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.516.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_330_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %param_0.1216 = f16[1216,768]{1,0} parameter(0)
  %constant_331_clone_1 = f16[] constant(0.044708), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %broadcast.294.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_331_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_1"}
  %multiply.377.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.1216, f16[1216,768]{1,0} %param_0.1216)
  %multiply.275.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.1216, f16[1216,768]{1,0} %multiply.377.clone.1), metadata={op_type="Pow" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow"}
  %multiply.274.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %broadcast.294.clone.1, f16[1216,768]{1,0} %multiply.275.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_1"}
  %add.51.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %param_0.1216, f16[1216,768]{1,0} %multiply.274.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add"}
  %constant_332_clone_1 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.517.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_332_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2"}
  %multiply.273.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %add.51.clone.1, f16[1216,768]{1,0} %broadcast.517.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2"}
  %tanh.0.clone.1 = f16[1216,768]{1,0} tanh(f16[1216,768]{1,0} %multiply.273.clone.1), metadata={op_type="Tanh" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Tanh"}
  %add.89.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %broadcast.516.clone.1, f16[1216,768]{1,0} %tanh.0.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %constant_329_clone_1 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.515.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_329_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.376.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.1216, f16[1216,768]{1,0} %broadcast.515.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.373.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %add.89.clone.1, f16[1216,768]{1,0} %multiply.376.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_3"}
  %convert.179 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %multiply.373.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %constant_328 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.208 = f32[1216]{0} reduce(f32[1216,768]{1,0} %convert.179, f32[] %constant_328), dimensions={1}, to_apply=%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_mean-reduction.667, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  ROOT %tuple.78 = (f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) tuple(f32[1216]{0} %reduce.208, f16[1216,768]{1,0} %multiply.373.clone.1, f16[1216,768]{1,0} %tanh.0.clone.1)
}

%fused_computation.216 (param_0.464: f32[768]) -> f16[1216,768] {
  %param_0.464 = f32[768]{0} parameter(0)
  %convert.186 = f16[768]{0} convert(f32[768]{0} %param_0.464), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/Cast"}
  ROOT %broadcast.296 = f16[1216,768]{1,0} broadcast(f16[768]{0} %convert.186), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd"}
}

%fused_computation.217 (param_0.1033: s32[16,76], param_1.1126: s32[16], param_2.674: f32[16,512], param_3.459: f32[768], param_4.313: f32[768], param_5.273: f32[16,512], param_6.278: f32[16,512], param_7.233: f32[768], param_8.180: f32[768], param_9.128: f32[16,512], param_10.71: f16[16,512,768], param_11.58: f16[8192,768], param_12.46: f32[768], param_13.25: f16[16,512,768]) -> f16[1216,768] {
  %param_13.25 = f16[16,512,768]{2,1,0} parameter(13)
  %constant_843 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1219 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_843), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.67 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_13.25, f16[16,512,768]{2,1,0} %broadcast.1219), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %constant_842 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1218 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_842), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_841 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1217 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_841), dimensions={}
  %select.72 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.67, f16[16,512,768]{2,1,0} %broadcast.1218, f16[16,512,768]{2,1,0} %broadcast.1217), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %param_12.46 = f32[768]{0} parameter(12)
  %convert.406 = f16[768]{0} convert(f32[768]{0} %param_12.46), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast"}
  %broadcast.1216 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.406), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %param_11.58 = f16[8192,768]{1,0} parameter(11)
  %bitcast.354 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_11.58), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %add.266 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1216, f16[16,512,768]{2,1,0} %bitcast.354), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %multiply.673 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.72, f16[16,512,768]{2,1,0} %add.266), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %convert.405 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.673), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2"}
  %param_10.71 = f16[16,512,768]{2,1,0} parameter(10)
  %convert.404 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_10.71), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_9.128 = f32[16,512]{1,0} parameter(9)
  %constant_834 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1215 = f32[16,512]{1,0} broadcast(f32[] %constant_834), dimensions={}
  %multiply.672 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_9.128, f32[16,512]{1,0} %broadcast.1215), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_835 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1214 = f32[16,512]{1,0} broadcast(f32[] %constant_835), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.265 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.672, f32[16,512]{1,0} %broadcast.1214), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.79 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.265), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1213 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.79), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_8.180 = f32[768]{0} parameter(8)
  %broadcast.1212 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_8.180), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.671 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1213, f32[16,512,768]{2,1,0} %broadcast.1212), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.670 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.404, f32[16,512,768]{2,1,0} %multiply.671), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_7.233 = f32[768]{0} parameter(7)
  %broadcast.1211 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_7.233), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %param_6.278 = f32[16,512]{1,0} parameter(6)
  %multiply.669 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_6.278, f32[16,512]{1,0} %broadcast.1215), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1209 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.669), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.668 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.671, f32[16,512,768]{2,1,0} %broadcast.1209), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.71 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1211, f32[16,512,768]{2,1,0} %multiply.668), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.264 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.670, f32[16,512,768]{2,1,0} %subtract.71), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %add.263 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.405, f32[16,512,768]{2,1,0} %add.264), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add_1"}
  %param_5.273 = f32[16,512]{1,0} parameter(5)
  %multiply.667 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.273, f32[16,512]{1,0} %broadcast.1215), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %add.262 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.667, f32[16,512]{1,0} %broadcast.1214), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add"}
  %rsqrt.78 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.262), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1205 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.78), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %param_4.313 = f32[768]{0} parameter(4)
  %broadcast.1204 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_4.313), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.666 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1205, f32[16,512,768]{2,1,0} %broadcast.1204), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul"}
  %multiply.665 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.263, f32[16,512,768]{2,1,0} %multiply.666), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_1"}
  %param_3.459 = f32[768]{0} parameter(3)
  %broadcast.1203 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_3.459), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub"}
  %param_2.674 = f32[16,512]{1,0} parameter(2)
  %multiply.664 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.674, f32[16,512]{1,0} %broadcast.1215), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
  %broadcast.1200 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.664), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %multiply.663 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.666, f32[16,512,768]{2,1,0} %broadcast.1200), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2"}
  %subtract.70 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1203, f32[16,512,768]{2,1,0} %multiply.663), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/sub"}
  %add.260 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.665, f32[16,512,768]{2,1,0} %subtract.70), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/add_1"}
  %convert.187 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.260), metadata={op_type="Cast" op_name="model/bert_pretrainer/Cast"}
  %bitcast.286 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %convert.187), metadata={op_type="Reshape" op_name="model/bert_pretrainer/cls/predictions/Reshape_2"}
  %param_0.1033 = s32[16,76]{1,0} parameter(0)
  %param_1.1126 = s32[16]{0} parameter(1)
  %broadcast.630 = s32[16,76]{1,0} broadcast(s32[16]{0} %param_1.1126), dimensions={0}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %add.98 = s32[16,76]{1,0} add(s32[16,76]{1,0} %param_0.1033, s32[16,76]{1,0} %broadcast.630), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %bitcast.322 = s32[1216]{0} bitcast(s32[16,76]{1,0} %add.98), metadata={op_type="Reshape" op_name="model/bert_pretrainer/cls/predictions/Reshape_1"}
  ROOT %gather.0 = f16[1216,768]{1,0} gather(f16[8192,768]{1,0} %bitcast.286, s32[1216]{0} %bitcast.322), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="GatherV2" op_name="model/bert_pretrainer/cls/predictions/GatherV2"}
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_moments_variance-reduction.611 (x.612: f32[], y.613: f32[]) -> f32[] {
  %x.612 = f32[] parameter(0)
  %y.613 = f32[] parameter(1)
  ROOT %add.614 = f32[] add(f32[] %x.612, f32[] %y.613)
}

%fused_computation.221 (param_0.1218: f32[16,512], param_1.1421: f32[16,512], param_2.997: f32[768], param_3.809: f32[768], param_4.657: f32[16,512], param_5.585: f16[16,512,768], param_6.567: f16[8192,768], param_7.475: f32[768], param_8.402: f16[16,512,768]) -> (f32[16,512], f32[16,512,768]) {
  %param_8.402 = f16[16,512,768]{2,1,0} parameter(8)
  %constant_762_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1089.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_762_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.61.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_8.402, f16[16,512,768]{2,1,0} %broadcast.1089.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %constant_760_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1088.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_760_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_759_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1087.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_759_clone_1), dimensions={}
  %select.66.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.61.clone.1, f16[16,512,768]{2,1,0} %broadcast.1088.clone.1, f16[16,512,768]{2,1,0} %broadcast.1087.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %param_7.475 = f32[768]{0} parameter(7)
  %convert.388.clone.1 = f16[768]{0} convert(f32[768]{0} %param_7.475), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast"}
  %broadcast.1085.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.388.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %param_6.567 = f16[8192,768]{1,0} parameter(6)
  %bitcast.348.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_6.567), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %add.226.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1085.clone.1, f16[16,512,768]{2,1,0} %bitcast.348.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %multiply.604.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.66.clone.1, f16[16,512,768]{2,1,0} %add.226.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %convert.387.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.604.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2"}
  %param_5.585 = f16[16,512,768]{2,1,0} parameter(5)
  %convert.386.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_5.585), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_4.657 = f32[16,512]{1,0} parameter(4)
  %constant_795_clone_1 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1084.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_795_clone_1), dimensions={}
  %multiply.603.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.657, f32[16,512]{1,0} %broadcast.1084.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_757_clone_1 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1083.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_757_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.225.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.603.clone.1, f32[16,512]{1,0} %broadcast.1083.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.63.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.225.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1082.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.63.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_3.809 = f32[768]{0} parameter(3)
  %broadcast.1081.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_3.809), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.602.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1082.clone.1, f32[16,512,768]{2,1,0} %broadcast.1081.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.601.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.386.clone.1, f32[16,512,768]{2,1,0} %multiply.602.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_2.997 = f32[768]{0} parameter(2)
  %broadcast.1080.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.997), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %param_1.1421 = f32[16,512]{1,0} parameter(1)
  %multiply.600.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1421, f32[16,512]{1,0} %broadcast.1084.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1078.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.600.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.599.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.602.clone.1, f32[16,512,768]{2,1,0} %broadcast.1078.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.63.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1080.clone.1, f32[16,512,768]{2,1,0} %multiply.599.clone.1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.224.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.601.clone.1, f32[16,512,768]{2,1,0} %subtract.63.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %add.223.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.387.clone.1, f32[16,512,768]{2,1,0} %add.224.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add_1"}
  %param_0.1218 = f32[16,512]{1,0} parameter(0)
  %multiply.634.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1218, f32[16,512]{1,0} %broadcast.1084.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
  %broadcast.1146.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.634.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %subtract.61.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %add.223.clone.1, f32[16,512,768]{2,1,0} %broadcast.1146.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %multiply.282 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %subtract.61.clone.1, f32[16,512,768]{2,1,0} %subtract.61.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/SquaredDifference"}
  %constant_335 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.209 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.282, f32[] %constant_335), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_moments_variance-reduction.611, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  ROOT %tuple.79 = (f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.209, f32[16,512,768]{2,1,0} %subtract.61.clone.1)
}

%fused_computation.224 (param_0.1010: f16[16,512,3072], param_1.1103: f16[16,512,3072], param_2.642: f16[8192,3072], param_3.422: f32[3072]) -> f16[8192,3072] {
  %param_1.1103 = f16[16,512,3072]{2,1,0} parameter(1)
  %constant_342 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.530 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_342), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.24 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_1.1103, f16[16,512,3072]{2,1,0} %broadcast.530), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %constant_344 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.529 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_344), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.1010 = f16[16,512,3072]{2,1,0} parameter(0)
  %add.90 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.529, f16[16,512,3072]{2,1,0} %param_0.1010), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_3.422 = f32[3072]{0} parameter(3)
  %convert.373 = f16[3072]{0} convert(f32[3072]{0} %param_3.422), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast"}
  %broadcast.1037 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.373), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %param_2.642 = f16[8192,3072]{1,0} parameter(2)
  %bitcast.342 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_2.642), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum"}
  %add.208 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.1037, f16[16,512,3072]{2,1,0} %bitcast.342), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %constant_341 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.528 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_341), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.380 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.208, f16[16,512,3072]{2,1,0} %broadcast.528), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul"}
  %multiply.286 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.90, f16[16,512,3072]{2,1,0} %multiply.380), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_3"}
  %select.38 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.24, f16[16,512,3072]{2,1,0} %multiply.286, f16[16,512,3072]{2,1,0} %broadcast.530), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/Mul_1"}
  ROOT %bitcast.289 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %select.38)
}

%fused_computation.225 (param_0.1013: f16[8192,3072], param_1.1108: f32[3072]) -> f16[16,512,3072] {
  %param_1.1108 = f32[3072]{0} parameter(1)
  %convert.375 = f16[3072]{0} convert(f32[3072]{0} %param_1.1108), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast"}
  %broadcast.1039 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.375), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %param_0.1013 = f16[8192,3072]{1,0} parameter(0)
  %bitcast.344 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_0.1013), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum"}
  %add.210 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.1039, f16[16,512,3072]{2,1,0} %bitcast.344), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %multiply.381 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.210, f16[16,512,3072]{2,1,0} %add.210)
  %multiply.289 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.210, f16[16,512,3072]{2,1,0} %multiply.381), metadata={op_type="Pow" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow"}
  %constant_345 = f16[] constant(0.044708), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %broadcast.531 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_345), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %multiply.288 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.289, f16[16,512,3072]{2,1,0} %broadcast.531), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_1"}
  %add.57 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.288, f16[16,512,3072]{2,1,0} %add.210), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add"}
  %constant_346 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.532 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_346), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.287 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.57, f16[16,512,3072]{2,1,0} %broadcast.532), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_2"}
  ROOT %tanh.1 = f16[16,512,3072]{2,1,0} tanh(f16[16,512,3072]{2,1,0} %multiply.287), metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Tanh"}
}

%fused_computation.227 (param_0.1005: f32[16,512], param_1.1098: f32[768], param_2.637: f32[768], param_3.412: f32[16,512], param_4.265: f16[16,512,768]) -> f16[8192,768] {
  %param_4.265 = f16[16,512,768]{2,1,0} parameter(4)
  %convert.367 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_4.265), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_3.412 = f32[16,512]{1,0} parameter(3)
  %constant_715 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1015 = f32[16,512]{1,0} broadcast(f32[] %constant_715), dimensions={}
  %multiply.564 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.412, f32[16,512]{1,0} %broadcast.1015), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_716 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1014 = f32[16,512]{1,0} broadcast(f32[] %constant_716), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.200 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.564, f32[16,512]{1,0} %broadcast.1014), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.57 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.200), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1013 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.57), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_2.637 = f32[768]{0} parameter(2)
  %broadcast.1012 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.637), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.562 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1013, f32[16,512,768]{2,1,0} %broadcast.1012), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.561 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.367, f32[16,512,768]{2,1,0} %multiply.562), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_1.1098 = f32[768]{0} parameter(1)
  %broadcast.1011 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_1.1098), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %param_0.1005 = f32[16,512]{1,0} parameter(0)
  %multiply.560 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1005, f32[16,512]{1,0} %broadcast.1015), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1009 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.560), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.559 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.562, f32[16,512,768]{2,1,0} %broadcast.1009), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.56 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1011, f32[16,512,768]{2,1,0} %multiply.559), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.199 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.561, f32[16,512,768]{2,1,0} %subtract.56), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %convert.192 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.199), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_1"}
  ROOT %bitcast.291 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %convert.192)
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_moments_variance-reduction.532 (x.533: f32[], y.534: f32[]) -> f32[] {
  %x.533 = f32[] parameter(0)
  %y.534 = f32[] parameter(1)
  ROOT %add.535 = f32[] add(f32[] %x.533, f32[] %y.534)
}

%fused_computation.230 (param_0.996: f32[16,512], param_1.1087: f16[16,512,768]) -> f32[16,512] {
  %param_1.1087 = f16[16,512,768]{2,1,0} parameter(1)
  %convert.361 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_1.1087), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_0.996 = f32[16,512]{1,0} parameter(0)
  %constant_685 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.980 = f32[16,512]{1,0} broadcast(f32[] %constant_685), dimensions={}
  %multiply.540 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.996, f32[16,512]{1,0} %broadcast.980), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.979 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.540), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %subtract.52 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.361, f32[16,512,768]{2,1,0} %broadcast.979), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.293 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %subtract.52, f32[16,512,768]{2,1,0} %subtract.52), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %constant_349 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.210 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.293, f32[] %constant_349), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_moments_variance-reduction.532, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_moments_mean-reduction.514 (x.515: f32[], y.516: f32[]) -> f32[] {
  %x.515 = f32[] parameter(0)
  %y.516 = f32[] parameter(1)
  ROOT %add.517 = f32[] add(f32[] %x.515, f32[] %y.516)
}

%fused_computation.233 (param_0.1219: f16[16,512,768], param_1.1424: f16[8192,768], param_2.1001: f32[768], param_3.815: f16[16,512,768]) -> (f32[16,512], f16[16,512,768]) {
  %param_0.1219 = f16[16,512,768]{2,1,0} parameter(0)
  %param_3.815 = f16[16,512,768]{2,1,0} parameter(3)
  %constant_355_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.541.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_355_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.25.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_3.815, f16[16,512,768]{2,1,0} %broadcast.541.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/GreaterEqual"}
  %constant_353_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.540.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_353_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_352_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.538.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_352_clone_1), dimensions={}
  %select.39.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.25.clone.1, f16[16,512,768]{2,1,0} %broadcast.540.clone.1, f16[16,512,768]{2,1,0} %broadcast.538.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/Mul"}
  %param_2.1001 = f32[768]{0} parameter(2)
  %convert.196.clone.1 = f16[768]{0} convert(f32[768]{0} %param_2.1001), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add/Cast"}
  %broadcast.305.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.196.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add"}
  %param_1.1424 = f16[8192,768]{1,0} parameter(1)
  %bitcast.292.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_1.1424), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum"}
  %add.62.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.305.clone.1, f16[16,512,768]{2,1,0} %bitcast.292.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/add"}
  %multiply.295.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.39.clone.1, f16[16,512,768]{2,1,0} %add.62.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/Mul_1"}
  %add.61.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %param_0.1219, f16[16,512,768]{2,1,0} %multiply.295.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add"}
  %convert.195 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %add.61.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %constant_351 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.211 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %convert.195, f32[] %constant_351), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_layer_norm_moments_mean-reduction.514, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  ROOT %tuple.81 = (f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.211, f16[16,512,768]{2,1,0} %add.61.clone.1)
}

%fused_computation.235 (param_0.505: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.505 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.154 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.505), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  %copy.112 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.154), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  ROOT %bitcast.293 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.112)
}

%fused_computation.237 (param_0.980: f16[16,12,512,512], param_1.1068: f32[16,12,512], param_2.614: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_2.614 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.125 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.614), metadata={op_name="XLA_Args"}
  %constant_669 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.954 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_669), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.57 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.125, f16[16,12,512,512]{2,3,1,0} %broadcast.954), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/GreaterEqual"}
  %constant_357 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.543 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_357), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_356 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.542 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_356), dimensions={}
  %select.40 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.57, f16[16,12,512,512]{2,3,1,0} %broadcast.543, f16[16,12,512,512]{2,3,1,0} %broadcast.542), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul"}
  %param_0.980 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.1068 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.349 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.1068), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %broadcast.945 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.349), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %divide.72 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.980, f16[16,12,512,512]{2,3,1,0} %broadcast.945), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %multiply.297 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.40, f16[16,12,512,512]{2,3,1,0} %divide.72), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  ROOT %copy.113 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.297), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
}

%fused_computation.239 (param_0.513: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_0.513 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %convert.199 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %param_0.513), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  ROOT %bitcast.294 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.199), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
}

%fused_computation.240 (param_0.515: f16[16,12,512,512], param_1.527: f16[16,12,512]) -> f16[16,12,512,512] {
  %param_0.515 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.527 = f16[16,12,512]{2,1,0} parameter(1)
  %broadcast.309 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %param_1.527), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %subtract.18 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %param_0.515, f16[16,12,512,512]{2,3,1,0} %broadcast.309), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  ROOT %exponential.0 = f16[16,12,512,512]{2,3,1,0} exponential(f16[16,12,512,512]{2,3,1,0} %subtract.18), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
}

%fused_computation.241 (param_0.896: f16[16,12,512,512], param_1.970: s32[16,512]) -> f16[16,12,512,512] {
  %param_0.896 = f16[16,12,512,512]{3,2,1,0} parameter(0)
  %transpose.155 = f16[16,12,512,512]{2,3,1,0} transpose(f16[16,12,512,512]{3,2,1,0} %param_0.896), dimensions={0,1,3,2}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  %constant_482 = f16[] constant(-65504), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.690 = f16[16,512]{1,0} broadcast(f16[] %constant_482), dimensions={}
  %constant_481 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.689 = f16[16,512]{1,0} broadcast(f16[] %constant_481), dimensions={}
  %param_1.970 = s32[16,512]{1,0} parameter(1)
  %convert.283 = f16[16,512]{1,0} convert(s32[16,512]{1,0} %param_1.970), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/self_attention_mask/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int32_Cast"}
  %subtract.33 = f16[16,512]{1,0} subtract(f16[16,512]{1,0} %broadcast.689, f16[16,512]{1,0} %convert.283), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %multiply.416 = f16[16,512]{1,0} multiply(f16[16,512]{1,0} %broadcast.690, f16[16,512]{1,0} %subtract.33), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.688 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,512]{1,0} %multiply.416), dimensions={0,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
  ROOT %add.64 = f16[16,12,512,512]{2,3,1,0} add(f16[16,12,512,512]{2,3,1,0} %transpose.155, f16[16,12,512,512]{2,3,1,0} %broadcast.688), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/add"}
}

%fused_computation.243 (param_0.521: f16[8192,768], param_1.536: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.536 = f32[12,64]{1,0} parameter(1)
  %convert.214 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.536), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Cast"}
  %broadcast.311 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.214), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add"}
  %param_0.521 = f16[8192,768]{1,0} parameter(0)
  %reshape.434 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.521), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum"}
  %add.66 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.311, f16[16,512,12,64]{3,1,2,0} %reshape.434), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add"}
  ROOT %transpose.156 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.66), dimensions={0,2,1,3}
}

%fused_computation.245 (param_0.959: f32[768], param_1.1045: f32[768], param_2.586: f32[16,512], param_3.384: f32[768], param_4.248: f32[768], param_5.204: f32[16,512], param_6.201: f16[16,512,768], param_7.165: f16[8192,768], param_8.135: f32[768], param_9.88: f16[16,512,768], param_10.41: f32[16,512], param_11.32: f32[16,512]) -> f16[16,512,768] {
  %param_9.88 = f16[16,512,768]{2,1,0} parameter(9)
  %constant_603 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.876 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_603), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.51 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_9.88, f16[16,512,768]{2,1,0} %broadcast.876), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/GreaterEqual"}
  %constant_602 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.875 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_602), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_601 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.873 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_601), dimensions={}
  %select.60 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.51, f16[16,512,768]{2,1,0} %broadcast.875, f16[16,512,768]{2,1,0} %broadcast.873), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul"}
  %param_8.135 = f32[768]{0} parameter(8)
  %convert.332 = f16[768]{0} convert(f32[768]{0} %param_8.135), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Cast"}
  %broadcast.870 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.332), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %param_7.165 = f16[8192,768]{1,0} parameter(7)
  %bitcast.336 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_7.165), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %add.164 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.870, f16[16,512,768]{2,1,0} %bitcast.336), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %multiply.500 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.60, f16[16,512,768]{2,1,0} %add.164), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
  %convert.331 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.500), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_2"}
  %param_6.201 = f16[16,512,768]{2,1,0} parameter(6)
  %convert.330 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_6.201), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_5.204 = f32[16,512]{1,0} parameter(5)
  %constant_597 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.869 = f32[16,512]{1,0} broadcast(f32[] %constant_597), dimensions={}
  %multiply.499 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.204, f32[16,512]{1,0} %broadcast.869), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_599 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.868 = f32[16,512]{1,0} broadcast(f32[] %constant_599), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.163 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.499, f32[16,512]{1,0} %broadcast.868), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.37 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.163), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.867 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.37), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_4.248 = f32[768]{0} parameter(4)
  %broadcast.866 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_4.248), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.498 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.867, f32[16,512,768]{2,1,0} %broadcast.866), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.496 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.330, f32[16,512,768]{2,1,0} %multiply.498), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1"}
  %param_3.384 = f32[768]{0} parameter(3)
  %broadcast.865 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_3.384), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %param_2.586 = f32[16,512]{1,0} parameter(2)
  %multiply.495 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_2.586, f32[16,512]{1,0} %broadcast.869), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.863 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.495), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.494 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.498, f32[16,512,768]{2,1,0} %broadcast.863), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.48 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.865, f32[16,512,768]{2,1,0} %multiply.494), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %add.162 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.496, f32[16,512,768]{2,1,0} %subtract.48), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add_1"}
  %add.161 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.331, f32[16,512,768]{2,1,0} %add.162), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/add_1"}
  %param_11.32 = f32[16,512]{1,0} parameter(11)
  %multiply.523 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_11.32, f32[16,512]{1,0} %broadcast.869), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  %add.176 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.523, f32[16,512]{1,0} %broadcast.868), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/add"}
  %rsqrt.43 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.176), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/Rsqrt"}
  %broadcast.550 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.43), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %param_1.1045 = f32[768]{0} parameter(1)
  %broadcast.548 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_1.1045), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.383 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.550, f32[16,512,768]{2,1,0} %broadcast.548), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul"}
  %multiply.300 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %add.161, f32[16,512,768]{2,1,0} %multiply.383), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_1"}
  %param_0.959 = f32[768]{0} parameter(0)
  %broadcast.312 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.959), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/sub"}
  %param_10.41 = f32[16,512]{1,0} parameter(10)
  %multiply.517 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_10.41, f32[16,512]{1,0} %broadcast.869), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mean"}
  %broadcast.912 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.517), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/SquaredDifference"}
  %multiply.299 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.383, f32[16,512,768]{2,1,0} %broadcast.912), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_2"}
  %subtract.19 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.312, f32[16,512,768]{2,1,0} %multiply.299), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/sub"}
  %add.67 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.300, f32[16,512,768]{2,1,0} %subtract.19), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/add_1"}
  ROOT %convert.215 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.67), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_2"}
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_moments_variance-reduction.435 (x.436: f32[], y.437: f32[]) -> f32[] {
  %x.436 = f32[] parameter(0)
  %y.437 = f32[] parameter(1)
  ROOT %add.438 = f32[] add(f32[] %x.436, f32[] %y.437)
}

%fused_computation.247 (param_0.1221: f32[16,512], param_1.1429: f32[16,512], param_2.1008: f32[768], param_3.821: f32[768], param_4.670: f32[16,512], param_5.602: f16[16,512,768], param_6.588: f16[8192,768], param_7.493: f32[768], param_8.417: f16[16,512,768]) -> (f32[16,512], f32[16,512,768]) {
  %param_8.417 = f16[16,512,768]{2,1,0} parameter(8)
  %constant_588_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.851.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_588_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.49.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_8.417, f16[16,512,768]{2,1,0} %broadcast.851.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/GreaterEqual"}
  %constant_587_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.850.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_587_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_586_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.848.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_586_clone_1), dimensions={}
  %select.58.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.49.clone.1, f16[16,512,768]{2,1,0} %broadcast.850.clone.1, f16[16,512,768]{2,1,0} %broadcast.848.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul"}
  %param_7.493 = f32[768]{0} parameter(7)
  %convert.326.clone.1 = f16[768]{0} convert(f32[768]{0} %param_7.493), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Cast"}
  %broadcast.846.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.326.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %param_6.588 = f16[8192,768]{1,0} parameter(6)
  %bitcast.334.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_6.588), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %add.156.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.846.clone.1, f16[16,512,768]{2,1,0} %bitcast.334.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %multiply.487.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.58.clone.1, f16[16,512,768]{2,1,0} %add.156.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
  %convert.325.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.487.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_2"}
  %param_5.602 = f16[16,512,768]{2,1,0} parameter(5)
  %convert.324.clone.1 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_5.602), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_4.670 = f32[16,512]{1,0} parameter(4)
  %constant_621_clone_1 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.845.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_621_clone_1), dimensions={}
  %multiply.486.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.670, f32[16,512]{1,0} %broadcast.845.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_584_clone_1 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.844.clone.1 = f32[16,512]{1,0} broadcast(f32[] %constant_584_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.155.clone.1 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.486.clone.1, f32[16,512]{1,0} %broadcast.844.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.35.clone.1 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.155.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.843.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.35.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_3.821 = f32[768]{0} parameter(3)
  %broadcast.842.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_3.821), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.485.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.843.clone.1, f32[16,512,768]{2,1,0} %broadcast.842.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.484.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.324.clone.1, f32[16,512,768]{2,1,0} %multiply.485.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1"}
  %param_2.1008 = f32[768]{0} parameter(2)
  %broadcast.841.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.1008), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %param_1.1429 = f32[16,512]{1,0} parameter(1)
  %multiply.483.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_1.1429, f32[16,512]{1,0} %broadcast.845.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.839.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.483.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.482.clone.1 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.485.clone.1, f32[16,512,768]{2,1,0} %broadcast.839.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.46.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.841.clone.1, f32[16,512,768]{2,1,0} %multiply.482.clone.1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %add.154.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.484.clone.1, f32[16,512,768]{2,1,0} %subtract.46.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add_1"}
  %add.153.clone.1 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.325.clone.1, f32[16,512,768]{2,1,0} %add.154.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/add_1"}
  %param_0.1221 = f32[16,512]{1,0} parameter(0)
  %multiply.515.clone.1 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1221, f32[16,512]{1,0} %broadcast.845.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mean"}
  %broadcast.907.clone.1 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.515.clone.1), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/SquaredDifference"}
  %subtract.44.clone.1 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %add.153.clone.1, f32[16,512,768]{2,1,0} %broadcast.907.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/SquaredDifference"}
  %multiply.302 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %subtract.44.clone.1, f32[16,512,768]{2,1,0} %subtract.44.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/SquaredDifference"}
  %constant_363 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.212 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.302, f32[] %constant_363), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_moments_variance-reduction.435, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  ROOT %tuple.82 = (f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.212, f32[16,512,768]{2,1,0} %subtract.44.clone.1)
}

%fused_computation.250 (param_0.944: f16[16,512,3072], param_1.1028: f16[16,512,3072], param_2.566: f16[8192,3072], param_3.360: f32[3072]) -> f16[8192,3072] {
  %param_1.1028 = f16[16,512,3072]{2,1,0} parameter(1)
  %constant_370 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.562 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_370), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.27 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_1.1028, f16[16,512,3072]{2,1,0} %broadcast.562), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %constant_371 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.560 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_371), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.944 = f16[16,512,3072]{2,1,0} parameter(0)
  %add.91 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.560, f16[16,512,3072]{2,1,0} %param_0.944), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/add_1"}
  %param_3.360 = f32[3072]{0} parameter(3)
  %convert.310 = f16[3072]{0} convert(f32[3072]{0} %param_3.360), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast"}
  %broadcast.799 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.310), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %param_2.566 = f16[8192,3072]{1,0} parameter(2)
  %bitcast.328 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_2.566), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum"}
  %add.138 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.799, f16[16,512,3072]{2,1,0} %bitcast.328), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %constant_369 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.559 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_369), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.384 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.138, f16[16,512,3072]{2,1,0} %broadcast.559), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.305 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.91, f16[16,512,3072]{2,1,0} %multiply.384), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_3"}
  %select.42 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.27, f16[16,512,3072]{2,1,0} %multiply.305, f16[16,512,3072]{2,1,0} %broadcast.562), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/Mul_1"}
  ROOT %bitcast.296 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %select.42)
}

%fused_computation.251 (param_0.947: f16[8192,3072], param_1.1033: f32[3072]) -> f16[16,512,3072] {
  %param_1.1033 = f32[3072]{0} parameter(1)
  %convert.313 = f16[3072]{0} convert(f32[3072]{0} %param_1.1033), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast"}
  %broadcast.803 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.313), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %param_0.947 = f16[8192,3072]{1,0} parameter(0)
  %bitcast.330 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_0.947), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum"}
  %add.140 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.803, f16[16,512,3072]{2,1,0} %bitcast.330), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %multiply.385 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.140, f16[16,512,3072]{2,1,0} %add.140)
  %multiply.308 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.140, f16[16,512,3072]{2,1,0} %multiply.385), metadata={op_type="Pow" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Pow"}
  %constant_372 = f16[] constant(0.044708), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %broadcast.563 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_372), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %multiply.307 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.308, f16[16,512,3072]{2,1,0} %broadcast.563), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %add.71 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.307, f16[16,512,3072]{2,1,0} %add.140), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/add"}
  %constant_373 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.564 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_373), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.306 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.71, f16[16,512,3072]{2,1,0} %broadcast.564), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  ROOT %tanh.2 = f16[16,512,3072]{2,1,0} tanh(f16[16,512,3072]{2,1,0} %multiply.306), metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Tanh"}
}

%fused_computation.253 (param_0.939: f32[16,512], param_1.1023: f32[768], param_2.561: f32[768], param_3.350: f32[16,512], param_4.208: f16[16,512,768]) -> f16[8192,768] {
  %param_4.208 = f16[16,512,768]{2,1,0} parameter(4)
  %convert.304 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_4.208), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_3.350 = f32[16,512]{1,0} parameter(3)
  %constant_545 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.780 = f32[16,512]{1,0} broadcast(f32[] %constant_545), dimensions={}
  %multiply.449 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.350, f32[16,512]{1,0} %broadcast.780), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_546 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.779 = f32[16,512]{1,0} broadcast(f32[] %constant_546), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.130 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.449, f32[16,512]{1,0} %broadcast.779), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.29 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.130), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.778 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.29), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_2.561 = f32[768]{0} parameter(2)
  %broadcast.777 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.561), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.448 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.778, f32[16,512,768]{2,1,0} %broadcast.777), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.447 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.304, f32[16,512,768]{2,1,0} %multiply.448), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1"}
  %param_1.1023 = f32[768]{0} parameter(1)
  %broadcast.776 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_1.1023), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %param_0.939 = f32[16,512]{1,0} parameter(0)
  %multiply.446 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.939, f32[16,512]{1,0} %broadcast.780), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.774 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.446), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.445 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.448, f32[16,512,768]{2,1,0} %broadcast.774), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.39 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.776, f32[16,512,768]{2,1,0} %multiply.445), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %add.129 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.447, f32[16,512,768]{2,1,0} %subtract.39), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add_1"}
  %convert.219 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.129), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_1"}
  ROOT %bitcast.298 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %convert.219)
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_moments_variance-reduction.356 (x.357: f32[], y.358: f32[]) -> f32[] {
  %x.357 = f32[] parameter(0)
  %y.358 = f32[] parameter(1)
  ROOT %add.359 = f32[] add(f32[] %x.357, f32[] %y.358)
}

%fused_computation.256 (param_0.930: f32[16,512], param_1.1012: f16[16,512,768]) -> f32[16,512] {
  %param_1.1012 = f16[16,512,768]{2,1,0} parameter(1)
  %convert.300 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_1.1012), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_0.930 = f32[16,512]{1,0} parameter(0)
  %constant_513 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.743 = f32[16,512]{1,0} broadcast(f32[] %constant_513), dimensions={}
  %multiply.428 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.930, f32[16,512]{1,0} %broadcast.743), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.742 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.428), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %subtract.35 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.300, f32[16,512,768]{2,1,0} %broadcast.742), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.312 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %subtract.35, f32[16,512,768]{2,1,0} %subtract.35), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %constant_376 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.213 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.312, f32[] %constant_376), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_moments_variance-reduction.356, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_moments_mean-reduction.338 (x.339: f32[], y.340: f32[]) -> f32[] {
  %x.339 = f32[] parameter(0)
  %y.340 = f32[] parameter(1)
  ROOT %add.341 = f32[] add(f32[] %x.339, f32[] %y.340)
}

%fused_computation.259 (param_0.1222: f16[16,512,768], param_1.1432: f16[8192,768], param_2.1012: f32[768], param_3.827: f16[16,512,768]) -> (f32[16,512], f16[16,512,768]) {
  %param_0.1222 = f16[16,512,768]{2,1,0} parameter(0)
  %param_3.827 = f16[16,512,768]{2,1,0} parameter(3)
  %constant_381_clone_1 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.575.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_381_clone_1), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.28.clone.1 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_3.827, f16[16,512,768]{2,1,0} %broadcast.575.clone.1), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/GreaterEqual"}
  %constant_380_clone_1 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.574.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_380_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_379_clone_1 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.573.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_379_clone_1), dimensions={}
  %select.43.clone.1 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.28.clone.1, f16[16,512,768]{2,1,0} %broadcast.574.clone.1, f16[16,512,768]{2,1,0} %broadcast.573.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/Mul"}
  %param_2.1012 = f32[768]{0} parameter(2)
  %convert.223.clone.1 = f16[768]{0} convert(f32[768]{0} %param_2.1012), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add/Cast"}
  %broadcast.318.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.223.clone.1), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add"}
  %param_1.1432 = f16[8192,768]{1,0} parameter(1)
  %bitcast.299.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_1.1432), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum"}
  %add.76.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.318.clone.1, f16[16,512,768]{2,1,0} %bitcast.299.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/add"}
  %multiply.314.clone.1 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.43.clone.1, f16[16,512,768]{2,1,0} %add.76.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/Mul_1"}
  %add.75.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %param_0.1222, f16[16,512,768]{2,1,0} %multiply.314.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/add"}
  %convert.222 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %add.75.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %constant_378 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.214 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %convert.222, f32[] %constant_378), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_layer_norm_moments_mean-reduction.338, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  ROOT %tuple.84 = (f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.214, f16[16,512,768]{2,1,0} %add.75.clone.1)
}

%fused_computation.261 (param_0.561: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.561 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.157 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.561), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  %copy.115 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.157), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  ROOT %bitcast.300 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.115)
}

%fused_computation.263 (param_0.914: f16[16,12,512,512], param_1.993: f32[16,12,512], param_2.538: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_2.538 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.121 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.538), metadata={op_name="XLA_Args"}
  %constant_498 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.715 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_498), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.45 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.121, f16[16,12,512,512]{2,3,1,0} %broadcast.715), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %constant_383 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.578 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_383), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_382 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.577 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_382), dimensions={}
  %select.44 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.45, f16[16,12,512,512]{2,3,1,0} %broadcast.578, f16[16,12,512,512]{2,3,1,0} %broadcast.577), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %param_0.914 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.993 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.290 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.993), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %broadcast.704 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.290), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %divide.66 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.914, f16[16,12,512,512]{2,3,1,0} %broadcast.704), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %multiply.315 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.44, f16[16,12,512,512]{2,3,1,0} %divide.66), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  ROOT %copy.116 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.315), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
}

%fused_computation.265 (param_0.569: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_0.569 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %convert.226 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %param_0.569), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  ROOT %bitcast.301 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.226), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
}

%fused_computation.266 (param_0.571: f16[16,12,512,512], param_1.587: f16[16,12,512]) -> f16[16,12,512,512] {
  %param_0.571 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.587 = f16[16,12,512]{2,1,0} parameter(1)
  %broadcast.322 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %param_1.587), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %subtract.22 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %param_0.571, f16[16,12,512,512]{2,3,1,0} %broadcast.322), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  ROOT %exponential.1 = f16[16,12,512,512]{2,3,1,0} exponential(f16[16,12,512,512]{2,3,1,0} %subtract.22), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
}

%fused_computation.267 (param_0.894: f16[16,12,512,512], param_1.967: s32[16,512]) -> f16[16,12,512,512] {
  %param_0.894 = f16[16,12,512,512]{3,2,1,0} parameter(0)
  %transpose.158 = f16[16,12,512,512]{2,3,1,0} transpose(f16[16,12,512,512]{3,2,1,0} %param_0.894), dimensions={0,1,3,2}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  %constant_476 = f16[] constant(-65504), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.684 = f16[16,512]{1,0} broadcast(f16[] %constant_476), dimensions={}
  %constant_475 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.683 = f16[16,512]{1,0} broadcast(f16[] %constant_475), dimensions={}
  %param_1.967 = s32[16,512]{1,0} parameter(1)
  %convert.281 = f16[16,512]{1,0} convert(s32[16,512]{1,0} %param_1.967), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/self_attention_mask/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int32_Cast"}
  %subtract.31 = f16[16,512]{1,0} subtract(f16[16,512]{1,0} %broadcast.683, f16[16,512]{1,0} %convert.281), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %multiply.414 = f16[16,512]{1,0} multiply(f16[16,512]{1,0} %broadcast.684, f16[16,512]{1,0} %subtract.31), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.682 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,512]{1,0} %multiply.414), dimensions={0,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
  ROOT %add.78 = f16[16,12,512,512]{2,3,1,0} add(f16[16,12,512,512]{2,3,1,0} %transpose.158, f16[16,12,512,512]{2,3,1,0} %broadcast.682), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
}

%fused_computation.270 (param_0.581: f16[8192,768], param_1.600: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.600 = f32[12,64]{1,0} parameter(1)
  %convert.232 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.600), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Cast"}
  %broadcast.328 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.232), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add"}
  %param_0.581 = f16[8192,768]{1,0} parameter(0)
  %reshape.437 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.581), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum"}
  %add.80 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.328, f16[16,512,12,64]{3,1,2,0} %reshape.437), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add"}
  ROOT %transpose.159 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.80), dimensions={0,2,1,3}
}

%fused_computation.272 (param_0.883: f32[768], param_1.953: f16[16,512,768], param_2.507: f32[768], param_3.316: f16[16,512,768], param_4.191: f32[16,512], param_5.145: f32[16,512]) -> f16[16,512,768] {
  %param_1.953 = f16[16,512,768]{2,1,0} parameter(1)
  %convert.234 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_1.953), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %param_5.145 = f32[16,512]{1,0} parameter(5)
  %constant_439 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.668 = f32[16,512]{1,0} broadcast(f32[] %constant_439), dimensions={}
  %multiply.407 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_5.145, f32[16,512]{1,0} %broadcast.668), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/variance"}
  %constant_459 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.667 = f32[16,512]{1,0} broadcast(f32[] %constant_459), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.106 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.407, f32[16,512]{1,0} %broadcast.667), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %rsqrt.15 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.106), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/Rsqrt"}
  %broadcast.583 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.15), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %param_2.507 = f32[768]{0} parameter(2)
  %broadcast.582 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.507), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.388 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.583, f32[16,512,768]{2,1,0} %broadcast.582), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul"}
  %multiply.320 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.234, f32[16,512,768]{2,1,0} %multiply.388), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_1"}
  %param_0.883 = f32[768]{0} parameter(0)
  %broadcast.329 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_0.883), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/sub"}
  %param_4.191 = f32[16,512]{1,0} parameter(4)
  %multiply.392 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_4.191, f32[16,512]{1,0} %broadcast.668), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  %broadcast.641 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.392), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %multiply.319 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.388, f32[16,512,768]{2,1,0} %broadcast.641), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_2"}
  %subtract.24 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.329, f32[16,512,768]{2,1,0} %multiply.319), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/sub"}
  %add.81 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.320, f32[16,512,768]{2,1,0} %subtract.24), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add_1"}
  %convert.233 = f16[16,512,768]{2,1,0} convert(f32[16,512,768]{2,1,0} %add.81), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_1"}
  %param_3.316 = f16[16,512,768]{2,1,0} parameter(3)
  %constant_390 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.587 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_390), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.29 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_3.316, f16[16,512,768]{2,1,0} %broadcast.587), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %constant_389 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.586 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_389), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_387 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.584 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_387), dimensions={}
  %select.45 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.29, f16[16,512,768]{2,1,0} %broadcast.586, f16[16,512,768]{2,1,0} %broadcast.584), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  ROOT %multiply.318 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %convert.233, f16[16,512,768]{2,1,0} %select.45), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul_1"}
}

%model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_moments_variance-reduction.258 (x.259: f32[], y.260: f32[]) -> f32[] {
  %x.259 = f32[] parameter(0)
  %y.260 = f32[] parameter(1)
  ROOT %add.261 = f32[] add(f32[] %x.259, f32[] %y.260)
}

%fused_computation.274 (param_0.887: f32[16,512], param_1.959: f16[16,512,768]) -> f32[16,512] {
  %param_1.959 = f16[16,512,768]{2,1,0} parameter(1)
  %convert.277 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_1.959), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %param_0.887 = f32[16,512]{1,0} parameter(0)
  %constant_447 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.654 = f32[16,512]{1,0} broadcast(f32[] %constant_447), dimensions={}
  %multiply.398 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.887, f32[16,512]{1,0} %broadcast.654), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  %broadcast.652 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.398), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %subtract.27 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %convert.277, f32[16,512,768]{2,1,0} %broadcast.652), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %multiply.323 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %subtract.27, f32[16,512,768]{2,1,0} %subtract.27), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/SquaredDifference"}
  %constant_394 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.215 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %multiply.323, f32[] %constant_394), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_moments_variance-reduction.258, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/variance"}
}

%model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_moments_mean-reduction.240 (x.241: f32[], y.242: f32[]) -> f32[] {
  %x.241 = f32[] parameter(0)
  %y.242 = f32[] parameter(1)
  ROOT %add.243 = f32[] add(f32[] %x.241, f32[] %y.242)
}

%fused_computation.277 (param_0.1224: s32[16,512], param_1.1437: f32[30522,768], param_2.1017: f16[8192,768], param_3.831: f32[512,768]) -> (f32[16,512], f16[16,512,768]) {
  %param_3.831 = f32[512,768]{1,0} parameter(3)
  %convert.240.clone.1 = f16[512,768]{1,0} convert(f32[512,768]{1,0} %param_3.831), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/position_embedding/Cast"}
  %broadcast.332.clone.1 = f16[16,512,768]{2,1,0} broadcast(f16[512,768]{1,0} %convert.240.clone.1), dimensions={1,2}, metadata={op_type="BroadcastTo" op_name="model/bert_pretrainer/bert_encoder_1/position_embedding/BroadcastTo"}
  %param_2.1017 = f16[8192,768]{1,0} parameter(2)
  %bitcast.303.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_2.1017), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/Reshape_1"}
  %add.84.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.332.clone.1, f16[16,512,768]{2,1,0} %bitcast.303.clone.1), metadata={op_type="AddN" op_name="model/bert_pretrainer/bert_encoder_1/add/ArithmeticOptimizer/AddOpsRewrite_add_1"}
  %param_1.1437 = f32[30522,768]{1,0} parameter(1)
  %convert.266.clone.1 = f16[30522,768]{1,0} convert(f32[30522,768]{1,0} %param_1.1437), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2/Cast"}
  %param_0.1224 = s32[16,512]{1,0} parameter(0)
  %bitcast.316.clone.1 = s32[8192]{0} bitcast(s32[16,512]{1,0} %param_0.1224), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %gather.1.clone.1 = f16[8192,768]{1,0} gather(f16[30522,768]{1,0} %convert.266.clone.1, s32[8192]{0} %bitcast.316.clone.1), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="GatherV2" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2"}
  %bitcast.302.clone.1 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %gather.1.clone.1), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape_1"}
  %add.83.clone.1 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.84.clone.1, f16[16,512,768]{2,1,0} %bitcast.302.clone.1), metadata={op_type="AddN" op_name="model/bert_pretrainer/bert_encoder_1/add/ArithmeticOptimizer/AddOpsRewrite_add_1"}
  %convert.238 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %add.83.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %constant_397 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.216 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %convert.238, f32[] %constant_397), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_embeddings_layer_norm_moments_mean-reduction.240, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  ROOT %tuple.85 = (f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) tuple(f32[16,512]{1,0} %reduce.216, f16[16,512,768]{2,1,0} %add.83.clone.1)
}

%fused_computation.279 (param_0.867: f32[2,768]) -> f16[8,768] {
  %param_0.867 = f32[2,768]{1,0} parameter(0)
  %convert.241 = f16[2,768]{1,0} convert(f32[2,768]{1,0} %param_0.867), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul/Cast"}
  %constant_398 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.18 = f16[8,768]{1,0} pad(f16[2,768]{1,0} %convert.241, f16[] %constant_398), padding=0_6x0_0, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
}

%fused_computation.280 (param_0.869: s32[16,512]) -> f16[8192,8] {
  %param_0.869 = s32[16,512]{1,0} parameter(0)
  %bitcast.304 = s32[8192]{0} bitcast(s32[16,512]{1,0} %param_0.869), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/Reshape"}
  %broadcast.335 = s32[8192,2]{1,0} broadcast(s32[8192]{0} %bitcast.304), dimensions={0}, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %iota.4 = s32[8192,2]{1,0} iota(), iota_dimension=1, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %compare.13 = pred[8192,2]{1,0} compare(s32[8192,2]{1,0} %broadcast.335, s32[8192,2]{1,0} %iota.4), direction=EQ, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %constant_401 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.334 = f16[8192,2]{1,0} broadcast(f16[] %constant_401), dimensions={}, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %constant_400 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.333 = f16[8192,2]{1,0} broadcast(f16[] %constant_400), dimensions={}, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %select.46 = f16[8192,2]{1,0} select(pred[8192,2]{1,0} %compare.13, f16[8192,2]{1,0} %broadcast.334, f16[8192,2]{1,0} %broadcast.333), metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  ROOT %pad.19 = f16[8192,8]{1,0} pad(f16[8192,2]{1,0} %select.46, f16[] %constant_400), padding=0_0x0_6, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
}

%gradient_tape_model_bert_pretrain_loss_and_metric_layer_Sum-reduction.962 (x.963: f32[], y.964: f32[]) -> f32[] {
  %x.963 = f32[] parameter(0)
  %y.964 = f32[] parameter(1)
  ROOT %add.965 = f32[] add(f32[] %x.963, f32[] %y.964)
}

%fused_computation.281 (param_0.810: f32[]) -> f32[] {
  %constant_319 = f32[] constant(0.0625)
  %param_0.810 = f32[] parameter(0)
  %multiply.325 = f32[] multiply(f32[] %constant_319, f32[] %param_0.810), metadata={op_type="Mul" op_name="gradient_tape/truediv"}
  %broadcast.336 = f32[16]{0} broadcast(f32[] %multiply.325), dimensions={}, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Sum"}
  %constant_320 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.217 = f32[] reduce(f32[16]{0} %broadcast.336, f32[] %constant_320), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrain_loss_and_metric_layer_Sum-reduction.962, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Sum"}
}

%model_bert_pretrain_loss_and_metric_layer_Sum_3-reduction.92 (x.93: f32[], y.94: f32[]) -> f32[] {
  %x.93 = f32[] parameter(0)
  %y.94 = f32[] parameter(1)
  ROOT %add.95 = f32[] add(f32[] %x.93, f32[] %y.94)
}

%model_bert_pretrain_loss_and_metric_layer_Sum-reduction.777 (x.778: f32[], y.779: f32[]) -> f32[] {
  %x.778 = f32[] parameter(0)
  %y.779 = f32[] parameter(1)
  ROOT %add.780 = f32[] add(f32[] %x.778, f32[] %y.779)
}

%model_bert_pretrain_loss_and_metric_layer_Sum_2-reduction.816 (x.817: f32[], y.818: f32[]) -> f32[] {
  %x.817 = f32[] parameter(0)
  %y.818 = f32[] parameter(1)
  ROOT %add.819 = f32[] add(f32[] %x.817, f32[] %y.818)
}

%fused_computation.282 (param_0.1066: f32[16,76], param_1.1403: f32[1216], param_2.977: s32[16,76], param_3.799: s64[16,76]) -> (f32[], f32[], f32[]) {
  %param_0.1066 = f32[16,76]{1,0} parameter(0)
  %convert.442 = s32[16,76]{1,0} convert(f32[16,76]{1,0} %param_0.1066), metadata={op_type="Cast" op_name="model/Cast"}
  %convert.440 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %convert.442), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast"}
  %bitcast.305 = f32[1216]{0} bitcast(f32[16,76]{1,0} %convert.440)
  %constant_321 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.218 = f32[] reduce(f32[1216]{0} %bitcast.305, f32[] %constant_321), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum_3-reduction.92, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %param_1.1403 = f32[1216]{0} parameter(1)
  %bitcast.167.clone.1 = f32[16,76]{1,0} bitcast(f32[1216]{0} %param_1.1403), metadata={op_type="Reshape" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/Reshape_2"}
  %multiply.96.clone.1 = f32[16,76]{1,0} multiply(f32[16,76]{1,0} %convert.440, f32[16,76]{1,0} %bitcast.167.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrain_loss_and_metric_layer/mul"}
  %bitcast.166.clone.1 = f32[1216]{0} bitcast(f32[16,76]{1,0} %multiply.96.clone.1)
  %reduce.115.clone.1 = f32[] reduce(f32[1216]{0} %bitcast.166.clone.1, f32[] %constant_321), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum-reduction.777, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum"}
  %param_2.977 = s32[16,76]{1,0} parameter(2)
  %param_3.799 = s64[16,76]{1,0} parameter(3)
  %convert.25.clone.1 = s32[16,76]{1,0} convert(s64[16,76]{1,0} %param_3.799), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_7"}
  %compare.0.clone.1 = pred[16,76]{1,0} compare(s32[16,76]{1,0} %param_2.977, s32[16,76]{1,0} %convert.25.clone.1), direction=EQ, metadata={op_type="Equal" op_name="model/bert_pretrain_loss_and_metric_layer/Equal"}
  %broadcast.224.clone.1 = f32[16,76]{1,0} broadcast(f32[] %constant_321), dimensions={}
  %select.19.clone.1 = f32[16,76]{1,0} select(pred[16,76]{1,0} %compare.0.clone.1, f32[16,76]{1,0} %convert.440, f32[16,76]{1,0} %broadcast.224.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrain_loss_and_metric_layer/mul_1"}
  %bitcast.164.clone.1 = f32[1216]{0} bitcast(f32[16,76]{1,0} %select.19.clone.1)
  %reduce.113.clone.1 = f32[] reduce(f32[1216]{0} %bitcast.164.clone.1, f32[] %constant_321), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum_2-reduction.816, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_2"}
  ROOT %tuple.75 = (f32[], f32[], f32[]) tuple(f32[] %reduce.218, f32[] %reduce.115.clone.1, f32[] %reduce.113.clone.1)
}

%fused_computation.287 (param_0.654: f16[8192,768]) -> f16[16,12,512,64] {
  %param_0.654 = f16[8192,768]{1,0} parameter(0)
  %reshape.438 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.654), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum"}
  ROOT %transpose.165 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.438), dimensions={0,2,1,3}
}

%fused_computation.292 (param_0.722: f16[8192,768]) -> f16[16,12,512,64] {
  %param_0.722 = f16[8192,768]{1,0} parameter(0)
  %reshape.440 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.722), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum"}
  ROOT %transpose.172 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.440), dimensions={0,2,1,3}
}

%fused_computation.298 (param_0.808: f32[768], param_1.900: f32[], param_2.949: f32[768], param_3.765: f32[768], param_4.634: f32[768], param_5.559: f32[768], param_6.540: f32[768], param_7.461: f32[768], param_8.396: f32[768], param_9.343: f32[768], param_10.267: f32[768], param_11.212: f32[768], param_12.145: f32[768]) -> (f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768]) {
  %param_0.808 = f32[768]{0} parameter(0)
  %constant_316 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_1.900 = f32[] parameter(1)
  %divide.64 = f32[] divide(f32[] %constant_316, f32[] %param_1.900), metadata={op_type="Reciprocal" op_name="truediv"}
  %broadcast.510 = f32[768]{0} broadcast(f32[] %divide.64), dimensions={}, metadata={op_type="Mul" op_name="mul_4"}
  %multiply.371 = f32[768]{0} multiply(f32[768]{0} %param_0.808, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_46"}
  %param_2.949 = f32[768]{0} parameter(2)
  %multiply.331.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_2.949, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_4"}
  %param_3.765 = f32[768]{0} parameter(3)
  %multiply.334.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_3.765, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_5"}
  %param_4.634 = f32[768]{0} parameter(4)
  %multiply.338.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_4.634, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_14"}
  %param_5.559 = f32[768]{0} parameter(5)
  %multiply.340.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_5.559, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_15"}
  %param_6.540 = f32[768]{0} parameter(6)
  %multiply.346.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_6.540, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_20"}
  %param_7.461 = f32[768]{0} parameter(7)
  %multiply.348.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_7.461, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_21"}
  %param_8.396 = f32[768]{0} parameter(8)
  %multiply.353.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_8.396, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_30"}
  %param_9.343 = f32[768]{0} parameter(9)
  %multiply.355.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_9.343, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_31"}
  %param_10.267 = f32[768]{0} parameter(10)
  %multiply.361.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_10.267, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_36"}
  %param_11.212 = f32[768]{0} parameter(11)
  %multiply.363.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_11.212, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_37"}
  %param_12.145 = f32[768]{0} parameter(12)
  %multiply.369.clone.1 = f32[768]{0} multiply(f32[768]{0} %param_12.145, f32[768]{0} %broadcast.510), metadata={op_type="Mul" op_name="mul_45"}
  ROOT %tuple.59 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %multiply.371, f32[768]{0} %multiply.331.clone.1, f32[768]{0} %multiply.334.clone.1, f32[768]{0} %multiply.338.clone.1, f32[768]{0} %multiply.340.clone.1, f32[768]{0} %multiply.346.clone.1, f32[768]{0} %multiply.348.clone.1, f32[768]{0} %multiply.353.clone.1, f32[768]{0} %multiply.355.clone.1, f32[768]{0} %multiply.361.clone.1, f32[768]{0} %multiply.363.clone.1, f32[768]{0} %multiply.369.clone.1)
}

%fused_computation.299 (param_0.815: f32[30522,768]) -> f16[30528,768] {
  %param_0.815 = f32[30522,768]{1,0} parameter(0)
  %convert.254 = f16[30522,768]{1,0} convert(f32[30522,768]{1,0} %param_0.815), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2/Cast"}
  %constant_323 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.20 = f16[30528,768]{1,0} pad(f16[30522,768]{1,0} %convert.254, f16[] %constant_323), padding=0_6x0_0, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
}

%fused_computation.301 (param_0.833: f32[12,64,768]) -> f16[768,768] {
  %param_0.833 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.256 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.833), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast"}
  ROOT %bitcast.308 = f16[768,768]{1,0} bitcast(f16[12,64,768]{2,1,0} %convert.256)
}

%fused_computation.302 (param_0.835: f32[768,12,64]) -> f16[768,768] {
  %param_0.835 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.258 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.835), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast"}
  ROOT %bitcast.309 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.258)
}

%fused_computation.303 (param_0.839: f32[768,12,64]) -> f16[768,768] {
  %param_0.839 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.259 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.839), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast"}
  ROOT %bitcast.310 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.259)
}

%fused_computation.304 (param_0.841: f32[768,12,64]) -> f16[768,768] {
  %param_0.841 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.260 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.841), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast"}
  ROOT %bitcast.311 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.260)
}

%fused_computation.305 (param_0.853: f32[12,64,768]) -> f16[768,768] {
  %param_0.853 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.261 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.853), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast"}
  ROOT %bitcast.312 = f16[768,768]{1,0} bitcast(f16[12,64,768]{2,1,0} %convert.261)
}

%fused_computation.306 (param_0.855: f32[768,12,64]) -> f16[768,768] {
  %param_0.855 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.262 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.855), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast"}
  ROOT %bitcast.313 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.262)
}

%fused_computation.307 (param_0.859: f32[768,12,64]) -> f16[768,768] {
  %param_0.859 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.263 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.859), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast"}
  ROOT %bitcast.314 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.263)
}

%fused_computation.308 (param_0.861: f32[768,12,64]) -> f16[768,768] {
  %param_0.861 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.265 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.861), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast"}
  ROOT %bitcast.315 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.265)
}

%fused_computation.309 (param_0.901: f16[8192,768], param_1.977: f32[12,64]) -> f16[16,12,64,512] {
  %param_1.977 = f32[12,64]{1,0} parameter(1)
  %convert.286 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.977), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast"}
  %broadcast.695 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.286), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %param_0.901 = f16[8192,768]{1,0} parameter(0)
  %reshape.443 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.901), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum"}
  %add.112 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.695, f16[16,512,12,64]{1,3,2,0} %reshape.443), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %constant_485 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.694 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_485), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %multiply.418 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %add.112, f16[16,512,12,64]{1,3,2,0} %broadcast.694), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  ROOT %transpose.174 = f16[16,12,64,512]{3,2,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.418), dimensions={0,2,3,1}
}

%fused_computation.310 (param_0.918: f16[8192,768], param_1.998: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.998 = f32[12,64]{1,0} parameter(1)
  %convert.296 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.998), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast"}
  %broadcast.717 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.296), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  %param_0.918 = f16[8192,768]{1,0} parameter(0)
  %reshape.448 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.918), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum"}
  %add.116 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.717, f16[16,512,12,64]{3,1,2,0} %reshape.448), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  ROOT %transpose.175 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.116), dimensions={0,2,1,3}
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_moments_mean-reduction.417 (x.418: f32[], y.419: f32[]) -> f32[] {
  %x.418 = f32[] parameter(0)
  %y.419 = f32[] parameter(1)
  ROOT %add.420 = f32[] add(f32[] %x.418, f32[] %y.419)
}

%fused_computation.311 (param_0.1235: f32[16,512], param_1.1442: f32[768], param_2.1021: f32[768], param_3.834: f32[16,512], param_4.679: f16[16,512,768], param_5.613: f16[8192,768], param_6.599: f32[768], param_7.504: f16[16,512,768]) -> f32[16,512] {
  %param_7.504 = f16[16,512,768]{2,1,0} parameter(7)
  %constant_573 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.825 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_573), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.47 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_7.504, f16[16,512,768]{2,1,0} %broadcast.825), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/GreaterEqual"}
  %constant_572 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.824 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_572), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_570 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.823 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_570), dimensions={}
  %select.56 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.47, f16[16,512,768]{2,1,0} %broadcast.824, f16[16,512,768]{2,1,0} %broadcast.823), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul"}
  %param_6.599 = f32[768]{0} parameter(6)
  %convert.320 = f16[768]{0} convert(f32[768]{0} %param_6.599), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Cast"}
  %broadcast.822 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.320), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %param_5.613 = f16[8192,768]{1,0} parameter(5)
  %bitcast.332 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_5.613), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %add.148 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.822, f16[16,512,768]{2,1,0} %bitcast.332), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add"}
  %multiply.475 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.56, f16[16,512,768]{2,1,0} %add.148), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
  %convert.319 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.475), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_2"}
  %param_4.679 = f16[16,512,768]{2,1,0} parameter(4)
  %convert.317 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_4.679), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast"}
  %param_3.834 = f32[16,512]{1,0} parameter(3)
  %constant_566 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.821 = f32[16,512]{1,0} broadcast(f32[] %constant_566), dimensions={}
  %multiply.474 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.834, f32[16,512]{1,0} %broadcast.821), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %constant_568 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.820 = f32[16,512]{1,0} broadcast(f32[] %constant_568), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.147 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.474, f32[16,512]{1,0} %broadcast.820), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.33 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.147), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.819 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.33), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %param_2.1021 = f32[768]{0} parameter(2)
  %broadcast.818 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.1021), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.473 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.819, f32[16,512,768]{2,1,0} %broadcast.818), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul"}
  %multiply.471 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.317, f32[16,512,768]{2,1,0} %multiply.473), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_1"}
  %param_1.1442 = f32[768]{0} parameter(1)
  %broadcast.817 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_1.1442), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %param_0.1235 = f32[16,512]{1,0} parameter(0)
  %multiply.470 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1235, f32[16,512]{1,0} %broadcast.821), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %broadcast.815 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.470), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.469 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.473, f32[16,512,768]{2,1,0} %broadcast.815), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.43 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.817, f32[16,512,768]{2,1,0} %multiply.469), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/sub"}
  %add.146 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.471, f32[16,512,768]{2,1,0} %subtract.43), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/add_1"}
  %add.145 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.319, f32[16,512,768]{2,1,0} %add.146), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/add_1"}
  %constant_1273 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.219 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %add.145, f32[] %constant_1273), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_0_output_layer_norm_moments_mean-reduction.417, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mean"}
}

%fused_computation.313 (param_0.968: f16[8192,768], param_1.1054: f32[12,64]) -> f16[16,12,64,512] {
  %constant_654 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.939 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_654), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %param_1.1054 = f32[12,64]{1,0} parameter(1)
  %convert.343 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.1054), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast"}
  %broadcast.937 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.343), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %param_0.968 = f16[8192,768]{1,0} parameter(0)
  %reshape.453 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.968), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum"}
  %add.182 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.937, f16[16,512,12,64]{1,3,2,0} %reshape.453), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %multiply.530 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %broadcast.939, f16[16,512,12,64]{1,3,2,0} %add.182), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  ROOT %transpose.176 = f16[16,12,64,512]{3,2,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.530), dimensions={0,2,3,1}
}

%fused_computation.314 (param_0.984: f16[8192,768], param_1.1073: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.1073 = f32[12,64]{1,0} parameter(1)
  %convert.357 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.1073), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast"}
  %broadcast.958 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.357), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  %param_0.984 = f16[8192,768]{1,0} parameter(0)
  %reshape.458 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.984), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum"}
  %add.186 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.958, f16[16,512,12,64]{3,1,2,0} %reshape.458), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  ROOT %transpose.177 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.186), dimensions={0,2,1,3}
}

%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_moments_mean-reduction.593 (x.594: f32[], y.595: f32[]) -> f32[] {
  %x.594 = f32[] parameter(0)
  %y.595 = f32[] parameter(1)
  ROOT %add.596 = f32[] add(f32[] %x.594, f32[] %y.595)
}

%fused_computation.315 (param_0.1234: f32[16,512], param_1.1441: f32[768], param_2.1020: f32[768], param_3.833: f32[16,512], param_4.678: f16[16,512,768], param_5.612: f16[8192,768], param_6.598: f32[768], param_7.503: f16[16,512,768]) -> f32[16,512] {
  %param_7.503 = f16[16,512,768]{2,1,0} parameter(7)
  %constant_747 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.1061 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_747), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.59 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_7.503, f16[16,512,768]{2,1,0} %broadcast.1061), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %constant_746 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.1060 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_746), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_745 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.1059 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_745), dimensions={}
  %select.64 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.59, f16[16,512,768]{2,1,0} %broadcast.1060, f16[16,512,768]{2,1,0} %broadcast.1059), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %param_6.598 = f32[768]{0} parameter(6)
  %convert.381 = f16[768]{0} convert(f32[768]{0} %param_6.598), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add/Cast"}
  %broadcast.1058 = f16[16,512,768]{2,1,0} broadcast(f16[768]{0} %convert.381), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %param_5.612 = f16[8192,768]{1,0} parameter(5)
  %bitcast.346 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_5.612), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %add.218 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.1058, f16[16,512,768]{2,1,0} %bitcast.346), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/add"}
  %multiply.592 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.64, f16[16,512,768]{2,1,0} %add.218), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %convert.380 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %multiply.592), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_2"}
  %param_4.678 = f16[16,512,768]{2,1,0} parameter(4)
  %convert.379 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %param_4.678), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast"}
  %param_3.833 = f32[16,512]{1,0} parameter(3)
  %constant_742 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.1057 = f32[16,512]{1,0} broadcast(f32[] %constant_742), dimensions={}
  %multiply.591 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_3.833, f32[16,512]{1,0} %broadcast.1057), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %constant_743 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %broadcast.1056 = f32[16,512]{1,0} broadcast(f32[] %constant_743), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/add"}
  %add.217 = f32[16,512]{1,0} add(f32[16,512]{1,0} %multiply.591, f32[16,512]{1,0} %broadcast.1056), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add"}
  %rsqrt.61 = f32[16,512]{1,0} rsqrt(f32[16,512]{1,0} %add.217), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/Rsqrt"}
  %broadcast.1055 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %rsqrt.61), dimensions={0,1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %param_2.1020 = f32[768]{0} parameter(2)
  %broadcast.1054 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_2.1020), dimensions={2}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.590 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %broadcast.1055, f32[16,512,768]{2,1,0} %broadcast.1054), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul"}
  %multiply.589 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %convert.379, f32[16,512,768]{2,1,0} %multiply.590), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_1"}
  %param_1.1441 = f32[768]{0} parameter(1)
  %broadcast.1053 = f32[16,512,768]{2,1,0} broadcast(f32[768]{0} %param_1.1441), dimensions={2}, metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %param_0.1234 = f32[16,512]{1,0} parameter(0)
  %multiply.588 = f32[16,512]{1,0} multiply(f32[16,512]{1,0} %param_0.1234, f32[16,512]{1,0} %broadcast.1057), metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %broadcast.1051 = f32[16,512,768]{2,1,0} broadcast(f32[16,512]{1,0} %multiply.588), dimensions={0,1}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/SquaredDifference"}
  %multiply.586 = f32[16,512,768]{2,1,0} multiply(f32[16,512,768]{2,1,0} %multiply.590, f32[16,512,768]{2,1,0} %broadcast.1051), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2"}
  %subtract.60 = f32[16,512,768]{2,1,0} subtract(f32[16,512,768]{2,1,0} %broadcast.1053, f32[16,512,768]{2,1,0} %multiply.586), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/sub"}
  %add.216 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %multiply.589, f32[16,512,768]{2,1,0} %subtract.60), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/add_1"}
  %add.215 = f32[16,512,768]{2,1,0} add(f32[16,512,768]{2,1,0} %convert.380, f32[16,512,768]{2,1,0} %add.216), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/add_1"}
  %constant_1272 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.220 = f32[16,512]{1,0} reduce(f32[16,512,768]{2,1,0} %add.215, f32[] %constant_1272), dimensions={2}, to_apply=%model_bert_pretrainer_bert_encoder_1_transformer_layer_1_output_layer_norm_moments_mean-reduction.593, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
}

%fused_computation.318 (param_0.1232: s32[16,76], param_1.1439: f32[1216,30522], param_2.1018: f32[1216], param_3.832: f32[16,76], param_4.677: f32[], param_5.611: f32[]) -> f16[1216,30528] {
  %param_5.611 = f32[] parameter(5)
  %constant_944 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %compare.85 = pred[] compare(f32[] %param_5.611, f32[] %constant_944), direction=EQ, metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %param_4.677 = f32[] parameter(4)
  %divide.84 = f32[] divide(f32[] %param_4.677, f32[] %param_5.611), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %select.86 = f32[] select(pred[] %compare.85, f32[] %constant_944, f32[] %divide.84), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %broadcast.1367 = f32[16,76]{1,0} broadcast(f32[] %select.86), dimensions={}, metadata={op_type="Tile" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Tile_1"}
  %param_3.832 = f32[16,76]{1,0} parameter(3)
  %convert.476 = s32[16,76]{1,0} convert(f32[16,76]{1,0} %param_3.832), metadata={op_type="Cast" op_name="model/Cast"}
  %convert.475 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %convert.476), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast"}
  %multiply.725 = f32[16,76]{1,0} multiply(f32[16,76]{1,0} %broadcast.1367, f32[16,76]{1,0} %convert.475), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/mul/Mul"}
  %bitcast.364 = f32[1216]{0} bitcast(f32[16,76]{1,0} %multiply.725), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %broadcast.1366 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %bitcast.364), dimensions={0}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %param_1.1439 = f32[1216,30522]{1,0} parameter(1)
  %param_2.1018 = f32[1216]{0} parameter(2)
  %broadcast.1365 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_2.1018), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %divide.83 = f32[1216,30522]{1,0} divide(f32[1216,30522]{1,0} %param_1.1439, f32[1216,30522]{1,0} %broadcast.1365), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.1232 = s32[16,76]{1,0} parameter(0)
  %convert.474 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %param_0.1232), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_2"}
  %convert.473 = s64[16,76]{1,0} convert(f32[16,76]{1,0} %convert.474), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %bitcast.363 = s64[1216]{0} bitcast(s64[16,76]{1,0} %convert.473), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %broadcast.1364 = s64[1216,30522]{1,0} broadcast(s64[1216]{0} %bitcast.363), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.12 = s64[1216,30522]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.84 = pred[1216,30522]{1,0} compare(s64[1216,30522]{1,0} %broadcast.1364, s64[1216,30522]{1,0} %iota.12), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_945 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1363 = f32[1216,30522]{1,0} broadcast(f32[] %constant_945), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1362 = f32[1216,30522]{1,0} broadcast(f32[] %constant_944), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.85 = f32[1216,30522]{1,0} select(pred[1216,30522]{1,0} %compare.84, f32[1216,30522]{1,0} %broadcast.1363, f32[1216,30522]{1,0} %broadcast.1362), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_943 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1361 = s64[1216]{0} broadcast(s64[] %constant_943), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.83 = pred[1216]{0} compare(s64[1216]{0} %broadcast.1361, s64[1216]{0} %bitcast.363), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_942 = s64[] constant(30522), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1360 = s64[1216]{0} broadcast(s64[] %constant_942), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.82 = pred[1216]{0} compare(s64[1216]{0} %bitcast.363, s64[1216]{0} %broadcast.1360), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %and.99 = pred[1216]{0} and(pred[1216]{0} %compare.83, pred[1216]{0} %compare.82), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1359 = f32[1216]{0} broadcast(f32[] %constant_944), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_941 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1358 = f32[1216]{0} broadcast(f32[] %constant_941), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.84 = f32[1216]{0} select(pred[1216]{0} %and.99, f32[1216]{0} %broadcast.1359, f32[1216]{0} %broadcast.1358), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.1357 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %select.84), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %add.305 = f32[1216,30522]{1,0} add(f32[1216,30522]{1,0} %select.85, f32[1216,30522]{1,0} %broadcast.1357), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.87 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %divide.83, f32[1216,30522]{1,0} %add.305), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.724 = f32[1216,30522]{1,0} multiply(f32[1216,30522]{1,0} %broadcast.1366, f32[1216,30522]{1,0} %subtract.87), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %convert.472 = f16[1216,30522]{1,0} convert(f32[1216,30522]{1,0} %multiply.724), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %constant_1269 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.21 = f16[1216,30528]{1,0} pad(f16[1216,30522]{1,0} %convert.472, f16[] %constant_1269), padding=0_0x0_6, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul_1"}
}

%fused_computation.319 (param_0.1100: f32[16], param_1.1205: f32[2], param_2.757: f16[16,8]) -> f32[16,2] {
  %param_2.757 = f16[16,8]{1,0} parameter(2)
  %slice.31 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_2.757), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_1.1205 = f32[2]{0} parameter(1)
  %convert.515 = f16[2]{0} convert(f32[2]{0} %param_1.1205), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.1418 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.515), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.322 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.31, f16[16,2]{1,0} %broadcast.1418), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.514 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.322), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %param_0.1100 = f32[16]{0} parameter(0)
  %broadcast.1417 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_0.1100), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.89 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %convert.514, f32[16,2]{1,0} %broadcast.1417), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %exponential.3 = f32[16,2]{1,0} exponential(f32[16,2]{1,0} %subtract.89), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
}

%fused_computation.321 (param_0.1227: f16[16,12,512,64], param_1.1456: f16[16,12,512,64], param_2.1032: f16[16,12,512,64], param_3.843: f16[16,12,512,64]) -> (f32[16,12,64], f32[16,12,64], f32[16,12,64], f32[16,12,64]) {
  %param_0.1227 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.181 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.1227), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  %convert.586 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.181), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %bitcast.432 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.586), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %constant_1264 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.222 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.432, f32[] %constant_1264), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_key_add_Sum-reduction.1357, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %param_1.1456 = f16[16,12,512,64]{3,2,1,0} parameter(1)
  %transpose.183.clone.1 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_1.1456), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  %convert.589.clone.1 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.183.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %bitcast.434.clone.1 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.589.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %reduce.223.clone.1 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.434.clone.1, f32[] %constant_1264), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_query_add_Sum-reduction.1377, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %param_2.1032 = f16[16,12,512,64]{3,2,1,0} parameter(2)
  %transpose.189.clone.1 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_2.1032), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %convert.644.clone.1 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.189.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %bitcast.468.clone.1 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.644.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %reduce.226.clone.1 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.468.clone.1, f32[] %constant_1264), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_query_add_Sum-reduction.1635, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %param_3.843 = f16[16,12,512,64]{3,2,1,0} parameter(3)
  %transpose.187.clone.1 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_3.843), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  %convert.642.clone.1 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.187.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %bitcast.466.clone.1 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.642.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %reduce.225.clone.1 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.466.clone.1, f32[] %constant_1264), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_key_add_Sum-reduction.1615, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  ROOT %tuple.93 = (f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}) tuple(f32[16,12,64]{2,1,0} %reduce.222, f32[16,12,64]{2,1,0} %reduce.223.clone.1, f32[16,12,64]{2,1,0} %reduce.226.clone.1, f32[16,12,64]{2,1,0} %reduce.225.clone.1)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_value_add_Sum-reduction.1652 (x.1653: f32[], y.1654: f32[]) -> f32[] {
  %x.1653 = f32[] parameter(0)
  %y.1654 = f32[] parameter(1)
  ROOT %add.1655 = f32[] add(f32[] %x.1653, f32[] %y.1654)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_value_add_Sum-reduction.1394 (x.1395: f32[], y.1396: f32[]) -> f32[] {
  %x.1395 = f32[] parameter(0)
  %y.1396 = f32[] parameter(1)
  ROOT %add.1397 = f32[] add(f32[] %x.1395, f32[] %y.1396)
}

%fused_computation.323 (param_0.1229: f16[16,12,64,512], param_1.1452: f16[16,12,64,512]) -> (f32[12,64], f32[12,64]) {
  %param_0.1229 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.185 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.1229), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}
  %convert.640 = f32[16,512,12,64]{1,3,2,0} convert(f16[16,512,12,64]{1,3,2,0} %transpose.185), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %bitcast.464 = f32[16,12,64,512]{3,2,1,0} bitcast(f32[16,512,12,64]{1,3,2,0} %convert.640), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %constant_1266 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.224 = f32[12,64]{1,0} reduce(f32[16,12,64,512]{3,2,1,0} %bitcast.464, f32[] %constant_1266), dimensions={0,3}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_value_add_Sum-reduction.1652, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %param_1.1452 = f16[16,12,64,512]{3,2,1,0} parameter(1)
  %transpose.179.clone.1 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_1.1452), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}
  %convert.584.clone.1 = f32[16,512,12,64]{1,3,2,0} convert(f16[16,512,12,64]{1,3,2,0} %transpose.179.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  %bitcast.430.clone.1 = f32[16,12,64,512]{3,2,1,0} bitcast(f32[16,512,12,64]{1,3,2,0} %convert.584.clone.1), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  %reduce.221.clone.1 = f32[12,64]{1,0} reduce(f32[16,12,64,512]{3,2,1,0} %bitcast.430.clone.1, f32[] %constant_1266), dimensions={0,3}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_value_add_Sum-reduction.1394, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  ROOT %tuple.90 = (f32[12,64]{1,0}, f32[12,64]{1,0}) tuple(f32[12,64]{1,0} %reduce.224, f32[12,64]{1,0} %reduce.221.clone.1)
}

%max_F32.794 (lhs.795: f32[], rhs.796: f32[]) -> f32[] {
  %lhs.795 = f32[] parameter(0)
  %rhs.796 = f32[] parameter(1)
  ROOT %maximum.797 = f32[] maximum(f32[] %lhs.795, f32[] %rhs.796)
}

%fused_computation.327 (param_0.1225: f32[30522], param_1.1438: f16[1216,30528]) -> f32[16,76] {
  %param_1.1438 = f16[1216,30528]{1,0} parameter(1)
  %slice.35 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_1.1438), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_0.1225 = f32[30522]{0} parameter(0)
  %convert.667 = f16[30522]{0} convert(f32[30522]{0} %param_0.1225), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.1798 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.667), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.458 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.35, f16[1216,30522]{1,0} %broadcast.1798), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.665 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.458), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %bitcast.470 = f32[16,76,30522]{2,1,0} bitcast(f32[1216,30522]{1,0} %convert.665), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_1"}
  %constant_1261 = f32[] constant(-inf), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %reduce.227 = f32[16,76]{1,0} reduce(f32[16,76,30522]{2,1,0} %bitcast.470, f32[] %constant_1261), dimensions={2}, to_apply=%max_F32.794, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
}

%add_float_.902 (x.903: f32[], y.904: f32[]) -> f32[] {
  %x.903 = f32[] parameter(0)
  %y.904 = f32[] parameter(1)
  ROOT %add.905 = f32[] add(f32[] %x.903, f32[] %y.904)
}

%fused_computation.328 (param_0.1204: f32[16,2]) -> (f32[16], f32[16]) {
  %param_0.1204 = f32[16,2]{1,0} parameter(0)
  %constant_1271 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.228 = f32[16]{0} reduce(f32[16,2]{1,0} %param_0.1204, f32[] %constant_1271), dimensions={1}, to_apply=%add_float_.902, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %log.0 = f32[16]{0} log(f32[16]{0} %reduce.228), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %tuple.23 = (f32[16]{0}, f32[16]{0}) tuple(f32[16]{0} %log.0, f32[16]{0} %reduce.228)
}

%add_float_.750 (x.751: f32[], y.752: f32[]) -> f32[] {
  %x.751 = f32[] parameter(0)
  %y.752 = f32[] parameter(1)
  ROOT %add.753 = f32[] add(f32[] %x.751, f32[] %y.752)
}

%fused_computation.329 (param_0.1233: f32[1216], param_1.1440: f32[30522], param_2.1019: f16[1216,30528]) -> (f32[1216], f32[1216,30522]) {
  %param_2.1019 = f16[1216,30528]{1,0} parameter(2)
  %slice.21.clone.1 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_2.1019), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_1.1440 = f32[30522]{0} parameter(1)
  %convert.431.clone.1 = f16[30522]{0} convert(f32[30522]{0} %param_1.1440), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.1317.clone.1 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.431.clone.1), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.298.clone.1 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.21.clone.1, f16[1216,30522]{1,0} %broadcast.1317.clone.1), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.430.clone.1 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.298.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %param_0.1233 = f32[1216]{0} parameter(0)
  %broadcast.1315.clone.1 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_0.1233), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.81.clone.1 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %convert.430.clone.1, f32[1216,30522]{1,0} %broadcast.1315.clone.1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %exponential.2.clone.1 = f32[1216,30522]{1,0} exponential(f32[1216,30522]{1,0} %subtract.81.clone.1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_1270 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.229 = f32[1216]{0} reduce(f32[1216,30522]{1,0} %exponential.2.clone.1, f32[] %constant_1270), dimensions={1}, to_apply=%add_float_.750, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %tuple.76 = (f32[1216]{0}, f32[1216,30522]{1,0}) tuple(f32[1216]{0} %reduce.229, f32[1216,30522]{1,0} %exponential.2.clone.1)
}

ENTRY %cluster_0__XlaCompiledKernel_true__XlaHasReferenceVars_false__XlaNumConstantArgs_173__XlaNumResourceArgs_52_.2592 (arg0.1: f16[16,512,3072], arg1.2: f16[16,512,3072], arg2.3: f16[16,12,512,512], arg3.4: f16[16,12,512,512], arg4.5: f16[16,512,768], arg5.6: f16[16,512,768], arg6.7: f16[16,512,768], arg7.8: f16[16,512,768], arg8.9: f16[16,512,768], arg9.10: s32[16,512], arg10.11: s32[16,76], arg11.12: s32[16,76], arg12.13: f32[16,76], arg13.14: s32[16,1], arg14.15: s32[16,512], arg15.16: s32[16,512], arg16.17: f32[], arg17.18: f32[768], arg18.19: f32[768], arg19.20: f32[768], arg20.21: f32[768,768], arg21.22: f32[512,768], arg22.23: f32[3072], arg23.24: f32[768,3072], arg24.25: f32[768], arg25.26: f32[3072,768], arg26.27: f32[768], arg27.28: f32[768], arg28.29: f32[768], arg29.30: f32[12,64,768], arg30.31: f32[12,64], arg31.32: f32[768,12,64], arg32.33: f32[12,64], arg33.34: f32[768,12,64], arg34.35: f32[12,64], arg35.36: f32[768,12,64], arg36.37: f32[768], arg37.38: f32[768], arg38.39: f32[3072], arg39.40: f32[768,3072], arg40.41: f32[768], arg41.42: f32[3072,768], arg42.43: f32[768], arg43.44: f32[768], arg44.45: f32[768], arg45.46: f32[12,64,768], arg46.47: f32[12,64], arg47.48: f32[768,12,64], arg48.49: f32[12,64], arg49.50: f32[768,12,64], arg50.51: f32[12,64], arg51.52: f32[768,12,64], arg52.53: f32[768], arg53.54: f32[768], arg54.55: f32[2,768], arg55.56: f32[2], arg56.57: f32[768,2], arg57.58: f32[30522], arg58.59: f32[30522,768], arg59.60: f32[768], arg60.61: f32[768], arg61.62: f32[768], arg62.63: f32[768,768], arg63.64: f32[], arg64.65: f32[], arg65.66: f32[], arg66.67: f32[], arg67.68: f32[]) -> (f32[], pred[], f32[768,12,64], f32[30522,768], f32[512,768], f32[12,64], f32[12,64,768], f32[768], f32[768], f32[768], f32[768,3072], f32[3072,768], f32[768], f32[768], f32[2,768], f32[768], f32[768,12,64], f32[12,64], f32[768,12,64], f32[12,64], f32[768,12,64], f32[12,64], f32[12,64,768], f32[768], f32[768], f32[768], f32[768], f32[768,3072], f32[3072], f32[3072,768], f32[768], f32[768], f32[768], f32[768,768], f32[768], f32[768,2], f32[768], f32[2], f32[30522], f32[768,768], f32[768], f32[768], f32[768], f32[768,12,64], f32[12,64], f32[768,12,64], f32[12,64], f32[3072], f32[], f32[], f32[], f32[], f32[]) {
  %constant_718 = f32[] constant(1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %copy.142 = f32[] copy(f32[] %constant_718)
  %arg26.27 = f32[768]{0} parameter(26), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg27.28 = f32[768]{0} parameter(27), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg17.18 = f32[768]{0} parameter(17), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg14.15 = s32[16,512]{1,0} parameter(14), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg58.59 = f32[30522,768]{1,0} parameter(58), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg15.16 = s32[16,512]{1,0} parameter(15), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.280 = f16[8192,8]{1,0} fusion(s32[16,512]{1,0} %arg15.16), kind=kLoop, calls=%fused_computation.280, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
  %arg54.55 = f32[2,768]{1,0} parameter(54), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.279 = f16[8,768]{1,0} fusion(f32[2,768]{1,0} %arg54.55), kind=kLoop, calls=%fused_computation.279, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
  %custom-call = f16[8192,768]{1,0} custom-call(f16[8192,8]{1,0} %fusion.280, f16[8,768]{1,0} %fusion.279), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"115\"}"
  %arg21.22 = f32[512,768]{1,0} parameter(21), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.277 = (f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) fusion(s32[16,512]{1,0} %arg14.15, f32[30522,768]{1,0} %arg58.59, f16[8192,768]{1,0} %custom-call, f32[512,768]{1,0} %arg21.22), kind=kInput, calls=%fused_computation.277, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/mean"}
  %get-tuple-element.131 = f16[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.277), index=1
  %arg18.19 = f32[768]{0} parameter(18), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg8.9 = f16[16,512,768]{2,1,0} parameter(8), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.130 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.277), index=0
  %fusion.274 = f32[16,512]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.130, f16[16,512,768]{2,1,0} %get-tuple-element.131), kind=kInput, calls=%fused_computation.274, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/moments/variance"}
  %fusion.272 = f16[16,512,768]{2,1,0} fusion(f32[768]{0} %arg17.18, f16[16,512,768]{2,1,0} %get-tuple-element.131, f32[768]{0} %arg18.19, f16[16,512,768]{2,1,0} %arg8.9, f32[16,512]{1,0} %get-tuple-element.130, f32[16,512]{1,0} %fusion.274), kind=kLoop, calls=%fused_computation.272, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul_1"}
  %bitcast.61 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.272)
  %arg31.32 = f32[768,12,64]{2,1,0} parameter(31), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.308 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg31.32), kind=kLoop, calls=%fused_computation.308
  %custom-call.1 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.61, f16[768,768]{1,0} %fusion.308), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg30.31 = f32[12,64]{1,0} parameter(30), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.270 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.1, f32[12,64]{1,0} %arg30.31), kind=kLoop, calls=%fused_computation.270
  %arg33.34 = f32[768,12,64]{2,1,0} parameter(33), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.307 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg33.34), kind=kLoop, calls=%fused_computation.307
  %custom-call.2 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.61, f16[768,768]{1,0} %fusion.307), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg32.33 = f32[12,64]{1,0} parameter(32), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.309 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.2, f32[12,64]{1,0} %arg32.33), kind=kLoop, calls=%fused_computation.309
  %custom-call.3 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.270, f16[16,12,64,512]{3,2,1,0} %fusion.309), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %arg9.10 = s32[16,512]{1,0} parameter(9), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.267 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{3,2,1,0} %custom-call.3, s32[16,512]{1,0} %arg9.10), kind=kLoop, calls=%fused_computation.267, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
  %bitcast = f16[16,12,512,512]{3,2,1,0} bitcast(f16[16,12,512,512]{2,3,1,0} %fusion.267), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %constant_302 = f16[] constant(-inf), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %reduce.47 = f16[16,12,512]{2,1,0} reduce(f16[16,12,512,512]{3,2,1,0} %bitcast, f16[] %constant_302), dimensions={2}, to_apply=%max_half_.303, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %fusion.266 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.267, f16[16,12,512]{2,1,0} %reduce.47), kind=kLoop, calls=%fused_computation.266, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %fusion.265 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.266), kind=kLoop, calls=%fused_computation.265, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %constant_90 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.48 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.265, f32[] %constant_90), dimensions={2}, to_apply=%add_float_.313, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %arg3.4 = f16[16,12,512,512]{3,2,1,0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.263 = f16[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.266, f32[16,12,512]{2,1,0} %reduce.48, f16[16,12,512,512]{3,2,1,0} %arg3.4), kind=kLoop, calls=%fused_computation.263, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  %arg35.36 = f32[768,12,64]{2,1,0} parameter(35), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.306 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg35.36), kind=kLoop, calls=%fused_computation.306
  %custom-call.4 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.61, f16[768,768]{1,0} %fusion.306), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg34.35 = f32[12,64]{1,0} parameter(34), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.310 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.4, f32[12,64]{1,0} %arg34.35), kind=kLoop, calls=%fused_computation.310
  %custom-call.5 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.263, f16[16,12,512,64]{3,2,1,0} %fusion.310), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.261 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.5), kind=kLoop, calls=%fused_computation.261
  %arg29.30 = f32[12,64,768]{2,1,0} parameter(29), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.305 = f16[768,768]{1,0} fusion(f32[12,64,768]{2,1,0} %arg29.30), kind=kLoop, calls=%fused_computation.305
  %custom-call.6 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.261, f16[768,768]{1,0} %fusion.305), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg28.29 = f32[768]{0} parameter(28), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg7.8 = f16[16,512,768]{2,1,0} parameter(7), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.259 = (f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) fusion(f16[16,512,768]{2,1,0} %fusion.272, f16[8192,768]{1,0} %custom-call.6, f32[768]{0} %arg28.29, f16[16,512,768]{2,1,0} %arg7.8), kind=kInput, calls=%fused_computation.259, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/mean"}
  %get-tuple-element.128 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.259), index=0
  %arg36.37 = f32[768]{0} parameter(36), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg37.38 = f32[768]{0} parameter(37), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.129 = f16[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.259), index=1
  %fusion.256 = f32[16,512]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.128, f16[16,512,768]{2,1,0} %get-tuple-element.129), kind=kInput, calls=%fused_computation.256, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/moments/variance"}
  %fusion.253 = f16[8192,768]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.128, f32[768]{0} %arg36.37, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f16[16,512,768]{2,1,0} %get-tuple-element.129), kind=kLoop, calls=%fused_computation.253
  %arg23.24 = f32[768,3072]{1,0} parameter(23), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.144 = f16[768,3072]{1,0} convert(f32[768,3072]{1,0} %arg23.24), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum/Cast"}
  %custom-call.7 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %fusion.253, f16[768,3072]{1,0} %convert.144), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg22.23 = f32[3072]{0} parameter(22), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.251 = f16[16,512,3072]{2,1,0} fusion(f16[8192,3072]{1,0} %custom-call.7, f32[3072]{0} %arg22.23), kind=kLoop, calls=%fused_computation.251, metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Tanh"}
  %arg1.2 = f16[16,512,3072]{2,1,0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.250 = f16[8192,3072]{1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.251, f16[16,512,3072]{2,1,0} %arg1.2, f16[8192,3072]{1,0} %custom-call.7, f32[3072]{0} %arg22.23), kind=kLoop, calls=%fused_computation.250
  %arg25.26 = f32[3072,768]{1,0} parameter(25), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.146 = f16[3072,768]{1,0} convert(f32[3072,768]{1,0} %arg25.26), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum/Cast"}
  %custom-call.8 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.250, f16[3072,768]{1,0} %convert.146), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg24.25 = f32[768]{0} parameter(24), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg6.7 = f16[16,512,768]{2,1,0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.311 = f32[16,512]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.128, f32[768]{0} %arg36.37, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f16[16,512,768]{2,1,0} %get-tuple-element.129, f16[8192,768]{1,0} %custom-call.8, f32[768]{0} %arg24.25, f16[16,512,768]{2,1,0} %arg6.7), kind=kInput, calls=%fused_computation.311, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/mean"}
  %fusion.247 = (f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) fusion(f32[16,512]{1,0} %fusion.311, f32[16,512]{1,0} %get-tuple-element.128, f32[768]{0} %arg36.37, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f16[16,512,768]{2,1,0} %get-tuple-element.129, f16[8192,768]{1,0} %custom-call.8, f32[768]{0} %arg24.25, f16[16,512,768]{2,1,0} %arg6.7), kind=kInput, calls=%fused_computation.247, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/moments/variance"}
  %get-tuple-element.124 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.247), index=0
  %fusion.245 = f16[16,512,768]{2,1,0} fusion(f32[768]{0} %arg26.27, f32[768]{0} %arg27.28, f32[16,512]{1,0} %get-tuple-element.128, f32[768]{0} %arg36.37, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f16[16,512,768]{2,1,0} %get-tuple-element.129, f16[8192,768]{1,0} %custom-call.8, f32[768]{0} %arg24.25, f16[16,512,768]{2,1,0} %arg6.7, f32[16,512]{1,0} %fusion.311, f32[16,512]{1,0} %get-tuple-element.124), kind=kLoop, calls=%fused_computation.245, metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_2"}
  %bitcast.72 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.245)
  %arg47.48 = f32[768,12,64]{2,1,0} parameter(47), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.304 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg47.48), kind=kLoop, calls=%fused_computation.304
  %custom-call.9 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.72, f16[768,768]{1,0} %fusion.304), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg46.47 = f32[12,64]{1,0} parameter(46), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.243 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.9, f32[12,64]{1,0} %arg46.47), kind=kLoop, calls=%fused_computation.243
  %arg49.50 = f32[768,12,64]{2,1,0} parameter(49), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.303 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg49.50), kind=kLoop, calls=%fused_computation.303
  %custom-call.10 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.72, f16[768,768]{1,0} %fusion.303), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg48.49 = f32[12,64]{1,0} parameter(48), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.313 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.10, f32[12,64]{1,0} %arg48.49), kind=kLoop, calls=%fused_computation.313
  %custom-call.11 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.243, f16[16,12,64,512]{3,2,1,0} %fusion.313), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.241 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{3,2,1,0} %custom-call.11, s32[16,512]{1,0} %arg9.10), kind=kLoop, calls=%fused_computation.241, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/add"}
  %bitcast.2 = f16[16,12,512,512]{3,2,1,0} bitcast(f16[16,12,512,512]{2,3,1,0} %fusion.241), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %reduce.49 = f16[16,12,512]{2,1,0} reduce(f16[16,12,512,512]{3,2,1,0} %bitcast.2, f16[] %constant_302), dimensions={2}, to_apply=%max_half_.479, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %fusion.240 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.241, f16[16,12,512]{2,1,0} %reduce.49), kind=kLoop, calls=%fused_computation.240, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %fusion.239 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.240), kind=kLoop, calls=%fused_computation.239, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %reduce.50 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.239, f32[] %constant_90), dimensions={2}, to_apply=%add_float_.489, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %arg5.6 = f16[16,512,768]{2,1,0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg53.54 = f32[768]{0} parameter(53), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg2.3 = f16[16,12,512,512]{3,2,1,0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.237 = f16[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.240, f32[16,12,512]{2,1,0} %reduce.50, f16[16,12,512,512]{3,2,1,0} %arg2.3), kind=kLoop, calls=%fused_computation.237, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  %arg51.52 = f32[768,12,64]{2,1,0} parameter(51), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.302 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg51.52), kind=kLoop, calls=%fused_computation.302
  %custom-call.12 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.72, f16[768,768]{1,0} %fusion.302), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg50.51 = f32[12,64]{1,0} parameter(50), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.314 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.12, f32[12,64]{1,0} %arg50.51), kind=kLoop, calls=%fused_computation.314
  %custom-call.13 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.237, f16[16,12,512,64]{3,2,1,0} %fusion.314), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.235 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.13), kind=kLoop, calls=%fused_computation.235
  %arg45.46 = f32[12,64,768]{2,1,0} parameter(45), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.301 = f16[768,768]{1,0} fusion(f32[12,64,768]{2,1,0} %arg45.46), kind=kLoop, calls=%fused_computation.301
  %custom-call.14 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.235, f16[768,768]{1,0} %fusion.301), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg44.45 = f32[768]{0} parameter(44), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.233 = (f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) fusion(f16[16,512,768]{2,1,0} %fusion.245, f16[8192,768]{1,0} %custom-call.14, f32[768]{0} %arg44.45, f16[16,512,768]{2,1,0} %arg5.6), kind=kInput, calls=%fused_computation.233, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/mean"}
  %get-tuple-element.122 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.233), index=0
  %get-tuple-element.123 = f16[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.233), index=1
  %fusion.230 = f32[16,512]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.122, f16[16,512,768]{2,1,0} %get-tuple-element.123), kind=kInput, calls=%fused_computation.230, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/moments/variance"}
  %arg52.53 = f32[768]{0} parameter(52), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.227 = f16[8192,768]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.122, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f16[16,512,768]{2,1,0} %get-tuple-element.123), kind=kLoop, calls=%fused_computation.227
  %arg39.40 = f32[768,3072]{1,0} parameter(39), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.181 = f16[768,3072]{1,0} convert(f32[768,3072]{1,0} %arg39.40), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum/Cast"}
  %custom-call.15 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %fusion.227, f16[768,3072]{1,0} %convert.181), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg38.39 = f32[3072]{0} parameter(38), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.225 = f16[16,512,3072]{2,1,0} fusion(f16[8192,3072]{1,0} %custom-call.15, f32[3072]{0} %arg38.39), kind=kLoop, calls=%fused_computation.225, metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Tanh"}
  %arg4.5 = f16[16,512,768]{2,1,0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg43.44 = f32[768]{0} parameter(43), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg0.1 = f16[16,512,3072]{2,1,0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.224 = f16[8192,3072]{1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.225, f16[16,512,3072]{2,1,0} %arg0.1, f16[8192,3072]{1,0} %custom-call.15, f32[3072]{0} %arg38.39), kind=kLoop, calls=%fused_computation.224
  %arg41.42 = f32[3072,768]{1,0} parameter(41), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.183 = f16[3072,768]{1,0} convert(f32[3072,768]{1,0} %arg41.42), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum/Cast"}
  %custom-call.16 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.224, f16[3072,768]{1,0} %convert.183), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg40.41 = f32[768]{0} parameter(40), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.315 = f32[16,512]{1,0} fusion(f32[16,512]{1,0} %get-tuple-element.122, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f16[16,512,768]{2,1,0} %get-tuple-element.123, f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg40.41, f16[16,512,768]{2,1,0} %arg4.5), kind=kInput, calls=%fused_computation.315, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/mean"}
  %fusion.221 = (f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) fusion(f32[16,512]{1,0} %fusion.315, f32[16,512]{1,0} %get-tuple-element.122, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f16[16,512,768]{2,1,0} %get-tuple-element.123, f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg40.41, f16[16,512,768]{2,1,0} %arg4.5), kind=kInput, calls=%fused_computation.221, metadata={op_type="Mean" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/moments/variance"}
  %get-tuple-element.118 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.221), index=0
  %arg55.56 = f32[2]{0} parameter(55), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg42.43 = f32[768]{0} parameter(42), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.197 = f16[16,768]{1,0} fusion(f32[16,512]{1,0} %fusion.315, f32[768]{0} %arg42.43, f32[768]{0} %arg43.44, f32[16,512]{1,0} %get-tuple-element.118, f32[16,512]{1,0} %get-tuple-element.122, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f16[16,512,768]{2,1,0} %get-tuple-element.123, f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg40.41, f16[16,512,768]{2,1,0} %arg4.5), kind=kLoop, calls=%fused_computation.197, metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_3"}
  %arg20.21 = f32[768,768]{1,0} parameter(20), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.114 = f16[768,768]{1,0} convert(f32[768,768]{1,0} %arg20.21), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul/Cast"}
  %arg19.20 = f32[768]{0} parameter(19), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.196 = f16[16,768]{1,0} fusion(f32[768]{0} %arg19.20), kind=kLoop, calls=%fused_computation.196, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd"}
  %custom-call.24 = f16[16,768]{1,0} custom-call(f16[16,768]{1,0} %fusion.197, f16[768,768]{1,0} %convert.114, f16[16,768]{1,0} %fusion.196), custom_call_target="__cublas$gemm", metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"114\"}"
  %tanh.831 = f16[16,768]{1,0} tanh(f16[16,768]{1,0} %custom-call.24), metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/Tanh"}
  %arg56.57 = f32[768,2]{1,0} parameter(56), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.195 = f16[768,8]{1,0} fusion(f32[768,2]{1,0} %arg56.57), kind=kLoop, calls=%fused_computation.195, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %custom-call.25 = f16[16,8]{1,0} custom-call(f16[16,768]{1,0} %tanh.831, f16[768,8]{1,0} %fusion.195), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"114\"}"
  %fusion.193 = f32[16]{0} fusion(f32[2]{0} %arg55.56, f16[16,8]{1,0} %custom-call.25), kind=kLoop, calls=%fused_computation.193, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.319 = f32[16,2]{1,0} fusion(f32[16]{0} %fusion.193, f32[2]{0} %arg55.56, f16[16,8]{1,0} %custom-call.25), kind=kLoop, calls=%fused_computation.319, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.328 = (f32[16]{0}, f32[16]{0}) fusion(f32[16,2]{1,0} %fusion.319), kind=kLoop, calls=%fused_computation.328, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %get-tuple-element.42 = f32[16]{0} get-tuple-element((f32[16]{0}, f32[16]{0}) %fusion.328), index=1
  %arg16.17 = f32[] parameter(16), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.281 = f32[] fusion(f32[] %arg16.17), kind=kLoop, calls=%fused_computation.281, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Sum"}
  %arg13.14 = s32[16,1]{1,0} parameter(13), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.190 = f16[16,2]{1,0} fusion(f32[16,2]{1,0} %fusion.319, f32[16]{0} %get-tuple-element.42, f32[] %fusion.281, s32[16,1]{1,0} %arg13.14), kind=kLoop, calls=%fused_computation.190, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_4/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %constant_125 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %pad.6 = f16[16,8]{1,0} pad(f16[16,2]{1,0} %fusion.190, f16[] %constant_125), padding=0_0x0_6, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %custom-call.26 = f16[16,768]{1,0} custom-call(f16[16,8]{1,0} %pad.6, f16[768,8]{1,0} %fusion.195), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"114\"}"
  %fusion.189 = f16[16,768]{1,0} fusion(f16[16,768]{1,0} %custom-call.26, f16[16,768]{1,0} %tanh.831), kind=kLoop, calls=%fused_computation.189, metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  %custom-call.27 = f16[16,768]{1,0} custom-call(f16[16,768]{1,0} %fusion.189, f16[768,768]{1,0} %convert.114), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"115\"}"
  %arg11.12 = s32[16,76]{1,0} parameter(11), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant_36 = s32[16]{0} constant({...}), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %fusion.217 = f16[1216,768]{1,0} fusion(s32[16,76]{1,0} %arg11.12, s32[16]{0} %constant_36, f32[16,512]{1,0} %fusion.315, f32[768]{0} %arg42.43, f32[768]{0} %arg43.44, f32[16,512]{1,0} %get-tuple-element.118, f32[16,512]{1,0} %get-tuple-element.122, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f16[16,512,768]{2,1,0} %get-tuple-element.123, f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg40.41, f16[16,512,768]{2,1,0} %arg4.5), kind=kLoop, calls=%fused_computation.217, metadata={op_type="GatherV2" op_name="model/bert_pretrainer/cls/predictions/GatherV2"}
  %arg62.63 = f32[768,768]{1,0} parameter(62), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.231 = f16[768,768]{1,0} convert(f32[768,768]{1,0} %arg62.63), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/dense/MatMul/Cast"}
  %arg61.62 = f32[768]{0} parameter(61), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.216 = f16[1216,768]{1,0} fusion(f32[768]{0} %arg61.62), kind=kLoop, calls=%fused_computation.216, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd"}
  %custom-call.18 = f16[1216,768]{1,0} custom-call(f16[1216,768]{1,0} %fusion.217, f16[768,768]{1,0} %convert.231, f16[1216,768]{1,0} %fusion.216), custom_call_target="__cublas$gemm", metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %fusion.214 = (f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) fusion(f16[1216,768]{1,0} %custom-call.18), kind=kInput, calls=%fused_computation.214, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %get-tuple-element.117 = f16[1216,768]{1,0} get-tuple-element((f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) %fusion.214), index=2
  %arg60.61 = f32[768]{0} parameter(60), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.115 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) %fusion.214), index=0
  %get-tuple-element.116 = f16[1216,768]{1,0} get-tuple-element((f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) %fusion.214), index=1
  %fusion.211 = f32[1216]{0} fusion(f32[1216]{0} %get-tuple-element.115, f16[1216,768]{1,0} %get-tuple-element.116), kind=kInput, calls=%fused_computation.211, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %arg10.11 = s32[16,76]{1,0} parameter(10), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg57.58 = f32[30522]{0} parameter(57), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg59.60 = f32[768]{0} parameter(59), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.209 = f16[1216,768]{1,0} fusion(f32[768]{0} %arg59.60, f16[1216,768]{1,0} %get-tuple-element.116, f32[768]{0} %arg60.61, f32[1216]{0} %get-tuple-element.115, f32[1216]{0} %fusion.211), kind=kLoop, calls=%fused_computation.209, metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1"}
  %fusion.299 = f16[30528,768]{1,0} fusion(f32[30522,768]{1,0} %arg58.59), kind=kLoop, calls=%fused_computation.299, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %custom-call.19 = f16[1216,30528]{1,0} custom-call(f16[1216,768]{1,0} %fusion.209, f16[30528,768]{1,0} %fusion.299), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.207 = f32[1216]{0} fusion(f32[30522]{0} %arg57.58, f16[1216,30528]{1,0} %custom-call.19), kind=kInput, calls=%fused_computation.207, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.329 = (f32[1216]{0}, f32[1216,30522]{1,0}) fusion(f32[1216]{0} %fusion.207, f32[30522]{0} %arg57.58, f16[1216,30528]{1,0} %custom-call.19), kind=kInput, calls=%fused_computation.329, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %get-tuple-element.114 = f32[1216,30522]{1,0} get-tuple-element((f32[1216]{0}, f32[1216,30522]{1,0}) %fusion.329), index=1
  %get-tuple-element.113 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f32[1216,30522]{1,0}) %fusion.329), index=0
  %arg12.13 = f32[16,76]{1,0} parameter(12), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %log.756 = f32[1216]{0} log(f32[1216]{0} %get-tuple-element.113), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.7 = f32[1216]{0} fusion(f32[1216]{0} %log.756, s32[16,76]{1,0} %arg10.11, f32[1216]{0} %fusion.207, f32[30522]{0} %arg57.58, f16[1216,30528]{1,0} %custom-call.19), kind=kInput, calls=%fused_computation.7, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.327 = f32[16,76]{1,0} fusion(f32[30522]{0} %arg57.58, f16[1216,30528]{1,0} %custom-call.19), kind=kInput, calls=%fused_computation.327, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %fusion.3 = s64[16,76]{1,0} fusion(f32[16,76]{1,0} %fusion.327, f32[30522]{0} %arg57.58, f16[1216,30528]{1,0} %custom-call.19), kind=kInput, calls=%fused_computation.3, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %fusion.282 = (f32[], f32[], f32[]) fusion(f32[16,76]{1,0} %arg12.13, f32[1216]{0} %fusion.7, s32[16,76]{1,0} %arg10.11, s64[16,76]{1,0} %fusion.3), kind=kInput, calls=%fused_computation.282, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %get-tuple-element.110 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.282), index=0
  %fusion.318 = f16[1216,30528]{1,0} fusion(s32[16,76]{1,0} %arg10.11, f32[1216,30522]{1,0} %get-tuple-element.114, f32[1216]{0} %get-tuple-element.113, f32[16,76]{1,0} %arg12.13, f32[] %fusion.281, f32[] %get-tuple-element.110), kind=kLoop, calls=%fused_computation.318, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul_1"}
  %custom-call.21 = f16[1216,768]{1,0} custom-call(f16[1216,30528]{1,0} %fusion.318, f16[30528,768]{1,0} %fusion.299), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.200 = (f32[1216]{0}, f32[1216]{0}) fusion(f32[768]{0} %arg60.61, f32[1216]{0} %fusion.211, f16[1216,768]{1,0} %custom-call.21, f32[1216]{0} %get-tuple-element.115, f16[1216,768]{1,0} %get-tuple-element.116), kind=kInput, calls=%fused_computation.200, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Sum"}
  %get-tuple-element.51 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f32[1216]{0}) %fusion.200), index=0
  %get-tuple-element.52 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f32[1216]{0}) %fusion.200), index=1
  %fusion.18 = (f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) fusion(f16[1216,768]{1,0} %get-tuple-element.117, f16[1216,768]{1,0} %custom-call.18, f32[1216]{0} %get-tuple-element.51, f32[1216]{0} %get-tuple-element.115, f16[1216,768]{1,0} %get-tuple-element.116, f32[1216]{0} %get-tuple-element.52, f32[768]{0} %arg60.61, f32[1216]{0} %fusion.211, f16[1216,768]{1,0} %custom-call.21), kind=kInput, calls=%fused_computation.18, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %get-tuple-element.48 = f16[1216,768]{1,0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.18), index=1
  %custom-call.22 = f16[1216,768]{1,0} custom-call(f16[1216,768]{1,0} %get-tuple-element.48, f16[768,768]{1,0} %convert.231), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.198 = f16[8192,768]{1,0} fusion(f16[1216,768]{1,0} %custom-call.22, s32[16,76]{1,0} %arg11.12, s32[16]{0} %constant_36), kind=kInput, calls=%fused_computation.198, metadata={op_type="UnsortedSegmentSum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/UnsortedSegmentSum"}
  %fusion.185 = (f32[16,512]{1,0}, f32[16,512]{1,0}) fusion(f32[768]{0} %arg43.44, f32[16,512]{1,0} %get-tuple-element.118, f16[16,768]{1,0} %custom-call.27, f16[8192,768]{1,0} %fusion.198, f32[16,512]{1,0} %fusion.315, f32[16,512]{1,0} %get-tuple-element.122, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f16[16,512,768]{2,1,0} %get-tuple-element.123, f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg40.41, f16[16,512,768]{2,1,0} %arg4.5), kind=kInput, calls=%fused_computation.185, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/batchnorm/mul_2/Sum"}
  %get-tuple-element.37 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}) %fusion.185), index=0
  %get-tuple-element.119 = f32[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.221), index=1
  %get-tuple-element.38 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}) %fusion.185), index=1
  %fusion.183 = f16[16,512,768]{2,1,0} fusion(f16[16,512,768]{2,1,0} %arg4.5, f32[16,512]{1,0} %get-tuple-element.37, f32[16,512,768]{2,1,0} %get-tuple-element.119, f32[16,512]{1,0} %get-tuple-element.38, f32[768]{0} %arg43.44, f32[16,512]{1,0} %get-tuple-element.118, f16[16,768]{1,0} %custom-call.27, f16[8192,768]{1,0} %fusion.198), kind=kLoop, calls=%fused_computation.183, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul_1"}
  %bitcast.97 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.183)
  %custom-call.28 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.97, f16[3072,768]{1,0} %convert.183), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.182 = f16[16,512,3072]{2,1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.225, f16[8192,3072]{1,0} %custom-call.28, f16[16,512,3072]{2,1,0} %arg0.1, f16[8192,3072]{1,0} %custom-call.15, f32[3072]{0} %arg38.39), kind=kLoop, calls=%fused_computation.182, metadata={op_type="AddN" op_name="AddN_6"}
  %bitcast.99 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %fusion.182)
  %custom-call.29 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %bitcast.99, f16[768,3072]{1,0} %convert.181), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %fusion.178 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) fusion(f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230, f32[16,512]{1,0} %get-tuple-element.122, f16[16,512,768]{2,1,0} %get-tuple-element.123, f16[8192,768]{1,0} %custom-call.29, f32[16,512]{1,0} %get-tuple-element.37, f32[16,512,768]{2,1,0} %get-tuple-element.119, f32[16,512]{1,0} %get-tuple-element.38, f32[768]{0} %arg43.44, f32[16,512]{1,0} %get-tuple-element.118, f16[16,768]{1,0} %custom-call.27, f16[8192,768]{1,0} %fusion.198), kind=kInput, calls=%fused_computation.178, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul_2/Sum"}
  %get-tuple-element.28 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.178), index=0
  %get-tuple-element.29 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.178), index=1
  %get-tuple-element.30 = f32[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.178), index=2
  %fusion.176 = f16[16,512,768]{2,1,0} fusion(f16[16,512,768]{2,1,0} %arg5.6, f32[16,512]{1,0} %get-tuple-element.28, f32[16,512]{1,0} %get-tuple-element.122, f16[16,512,768]{2,1,0} %get-tuple-element.123, f32[16,512]{1,0} %get-tuple-element.29, f32[16,512,768]{2,1,0} %get-tuple-element.30, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230), kind=kLoop, calls=%fused_computation.176, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout/dropout/Mul_1"}
  %bitcast.104 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.176)
  %fusion.175 = f16[768,768]{0,1} fusion(f32[12,64,768]{2,1,0} %arg45.46), kind=kLoop, calls=%fused_computation.175
  %custom-call.30 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.104, f16[768,768]{0,1} %fusion.175), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.292 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.30), kind=kLoop, calls=%fused_computation.292
  %fusion.174 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.12, f32[12,64]{1,0} %arg50.51), kind=kLoop, calls=%fused_computation.174
  %custom-call.31 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.292, f16[16,12,64,512]{3,2,1,0} %fusion.174), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.172 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.240, f32[16,12,512]{2,1,0} %reduce.50, f16[16,12,512,512]{3,2,1,0} %custom-call.31, f16[16,12,512,512]{3,2,1,0} %arg2.3), kind=kLoop, calls=%fused_computation.172, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  %reduce.51 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.172, f32[] %constant_90), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_softmax_Sum-reduction.1341, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  %fusion.171 = f16[16,12,512,512]{3,2,1,0} fusion(f32[16,12,512]{2,1,0} %reduce.51, f16[16,12,512,512]{2,3,1,0} %fusion.240, f32[16,12,512]{2,1,0} %reduce.50, f16[16,12,512,512]{3,2,1,0} %custom-call.31, f16[16,12,512,512]{3,2,1,0} %arg2.3), kind=kLoop, calls=%fused_computation.171, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul_1"}
  %fusion.170 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.10, f32[12,64]{1,0} %arg48.49), kind=kLoop, calls=%fused_computation.170
  %custom-call.32 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.171, f16[16,12,512,64]{3,2,1,0} %fusion.170), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %custom-call.34 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.171, f16[16,12,512,64]{3,2,1,0} %fusion.243), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum_1"}, backend_config="{\"alpha_real\":0.125,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.165 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.30), kind=kLoop, calls=%fused_computation.165
  %custom-call.36 = f16[16,12,64,512]{3,2,1,0} custom-call(f16[16,12,64,512]{3,2,1,0} %fusion.165, f16[16,12,512,512]{3,2,1,0} %fusion.237), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.164 = f16[8192,768]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.36), kind=kLoop, calls=%fused_computation.164
  %fusion.163 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg51.52), kind=kLoop, calls=%fused_computation.163
  %custom-call.37 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.164, f16[768,768]{0,1} %fusion.163), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.167 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.34), kind=kLoop, calls=%fused_computation.167
  %fusion.166 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg49.50), kind=kLoop, calls=%fused_computation.166
  %custom-call.35 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.167, f16[768,768]{0,1} %fusion.166), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.169 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.32), kind=kLoop, calls=%fused_computation.169
  %fusion.168 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg47.48), kind=kLoop, calls=%fused_computation.168
  %custom-call.33 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.169, f16[768,768]{0,1} %fusion.168), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.158 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) fusion(f32[768]{0} %arg27.28, f32[16,512]{1,0} %get-tuple-element.124, f32[16,512]{1,0} %fusion.311, f32[16,512]{1,0} %get-tuple-element.128, f32[768]{0} %arg36.37, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f16[16,512,768]{2,1,0} %get-tuple-element.129, f16[8192,768]{1,0} %custom-call.8, f32[768]{0} %arg24.25, f16[16,512,768]{2,1,0} %arg6.7, f16[8192,768]{1,0} %custom-call.37, f16[8192,768]{1,0} %custom-call.35, f16[8192,768]{1,0} %custom-call.33, f32[16,512]{1,0} %get-tuple-element.28, f32[16,512]{1,0} %get-tuple-element.122, f16[16,512,768]{2,1,0} %get-tuple-element.123, f32[16,512]{1,0} %get-tuple-element.29, f32[16,512,768]{2,1,0} %get-tuple-element.30, f32[768]{0} %arg53.54, f32[16,512]{1,0} %fusion.230), kind=kInput, calls=%fused_computation.158, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/batchnorm/mul_2/Sum"}
  %get-tuple-element.19 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.158), index=0
  %get-tuple-element.125 = f32[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.247), index=1
  %get-tuple-element.20 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.158), index=1
  %get-tuple-element.23 = f16[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.158), index=2
  %fusion.156 = f16[16,512,768]{2,1,0} fusion(f16[16,512,768]{2,1,0} %arg6.7, f32[16,512]{1,0} %get-tuple-element.19, f32[16,512,768]{2,1,0} %get-tuple-element.125, f32[16,512]{1,0} %get-tuple-element.20, f32[768]{0} %arg27.28, f32[16,512]{1,0} %get-tuple-element.124, f16[16,512,768]{2,1,0} %get-tuple-element.23), kind=kLoop, calls=%fused_computation.156, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_2/dropout/Mul_1"}
  %bitcast.118 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.156)
  %custom-call.38 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.118, f16[3072,768]{1,0} %convert.146), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.155 = f16[16,512,3072]{2,1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.251, f16[8192,3072]{1,0} %custom-call.38, f16[16,512,3072]{2,1,0} %arg1.2, f16[8192,3072]{1,0} %custom-call.7, f32[3072]{0} %arg22.23), kind=kLoop, calls=%fused_computation.155, metadata={op_type="AddN" op_name="AddN_13"}
  %bitcast.120 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %fusion.155)
  %custom-call.39 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %bitcast.120, f16[768,3072]{1,0} %convert.144), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %fusion.151 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) fusion(f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f32[16,512]{1,0} %get-tuple-element.128, f16[16,512,768]{2,1,0} %get-tuple-element.129, f16[8192,768]{1,0} %custom-call.39, f32[16,512]{1,0} %get-tuple-element.19, f32[16,512,768]{2,1,0} %get-tuple-element.125, f32[16,512]{1,0} %get-tuple-element.20, f32[768]{0} %arg27.28, f32[16,512]{1,0} %get-tuple-element.124, f16[16,512,768]{2,1,0} %get-tuple-element.23), kind=kInput, calls=%fused_computation.151, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul_2/Sum"}
  %get-tuple-element.12 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.151), index=0
  %get-tuple-element.13 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.151), index=1
  %get-tuple-element.14 = f32[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f32[16,512,768]{2,1,0}) %fusion.151), index=2
  %fusion.149 = f16[16,512,768]{2,1,0} fusion(f16[16,512,768]{2,1,0} %arg7.8, f32[16,512]{1,0} %get-tuple-element.12, f32[16,512]{1,0} %get-tuple-element.128, f16[16,512,768]{2,1,0} %get-tuple-element.129, f32[16,512]{1,0} %get-tuple-element.13, f32[16,512,768]{2,1,0} %get-tuple-element.14, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256), kind=kLoop, calls=%fused_computation.149, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout/dropout/Mul_1"}
  %bitcast.125 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.149)
  %fusion.148 = f16[768,768]{0,1} fusion(f32[12,64,768]{2,1,0} %arg29.30), kind=kLoop, calls=%fused_computation.148
  %custom-call.40 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %bitcast.125, f16[768,768]{0,1} %fusion.148), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.287 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.40), kind=kLoop, calls=%fused_computation.287
  %fusion.147 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.4, f32[12,64]{1,0} %arg34.35), kind=kLoop, calls=%fused_computation.147
  %custom-call.41 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.287, f16[16,12,64,512]{3,2,1,0} %fusion.147), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.145 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.266, f32[16,12,512]{2,1,0} %reduce.48, f16[16,12,512,512]{3,2,1,0} %custom-call.41, f16[16,12,512,512]{3,2,1,0} %arg3.4), kind=kLoop, calls=%fused_computation.145, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  %reduce.52 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.145, f32[] %constant_90), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_softmax_Sum-reduction.1599, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  %fusion.144 = f16[16,12,512,512]{3,2,1,0} fusion(f32[16,12,512]{2,1,0} %reduce.52, f16[16,12,512,512]{2,3,1,0} %fusion.266, f32[16,12,512]{2,1,0} %reduce.48, f16[16,12,512,512]{3,2,1,0} %custom-call.41, f16[16,12,512,512]{3,2,1,0} %arg3.4), kind=kLoop, calls=%fused_computation.144, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul_1"}
  %custom-call.44 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.144, f16[16,12,512,64]{3,2,1,0} %fusion.270), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum_1"}, backend_config="{\"alpha_real\":0.125,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.143 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.2, f32[12,64]{1,0} %arg32.33), kind=kLoop, calls=%fused_computation.143
  %custom-call.42 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.144, f16[16,12,512,64]{3,2,1,0} %fusion.143), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.321 = (f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}) fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.32, f16[16,12,512,64]{3,2,1,0} %custom-call.34, f16[16,12,512,64]{3,2,1,0} %custom-call.44, f16[16,12,512,64]{3,2,1,0} %custom-call.42), kind=kInput, calls=%fused_computation.321, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %get-tuple-element.142 = f32[16,12,64]{2,1,0} get-tuple-element((f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}) %fusion.321), index=2
  %get-tuple-element.143 = f32[16,12,64]{2,1,0} get-tuple-element((f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}) %fusion.321), index=3
  %fusion.138 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.40), kind=kLoop, calls=%fused_computation.138
  %custom-call.46 = f16[16,12,64,512]{3,2,1,0} custom-call(f16[16,12,64,512]{3,2,1,0} %fusion.138, f16[16,12,512,512]{3,2,1,0} %fusion.263), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.323 = (f32[12,64]{1,0}, f32[12,64]{1,0}) fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.46, f16[16,12,64,512]{3,2,1,0} %custom-call.36), kind=kInput, calls=%fused_computation.323, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %get-tuple-element.138 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.323), index=0
  %get-tuple-element.141 = f32[16,12,64]{2,1,0} get-tuple-element((f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}) %fusion.321), index=1
  %get-tuple-element.140 = f32[16,12,64]{2,1,0} get-tuple-element((f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}, f32[16,12,64]{2,1,0}) %fusion.321), index=0
  %get-tuple-element.139 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.323), index=1
  %fusion.114 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) fusion(f32[16,12,64]{2,1,0} %get-tuple-element.142, f32[] %arg16.17, f32[16,12,64]{2,1,0} %get-tuple-element.143, f32[12,64]{1,0} %get-tuple-element.138, f32[16,12,64]{2,1,0} %get-tuple-element.141, f32[16,12,64]{2,1,0} %get-tuple-element.140, f32[12,64]{1,0} %get-tuple-element.139), kind=kLoop, calls=%fused_computation.114, metadata={op_type="Mul" op_name="mul_7"}
  %get-tuple-element.99 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.114), index=3
  %get-tuple-element.101 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.114), index=5
  %get-tuple-element.96 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.114), index=0
  %get-tuple-element.98 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.114), index=2
  %get-tuple-element.100 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.114), index=4
  %get-tuple-element.97 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %fusion.114), index=1
  %get-tuple-element.49 = f32[768]{0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.18), index=2
  %get-tuple-element.50 = f32[768]{0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.18), index=3
  %fusion.50 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) fusion(f32[16,512]{1,0} %fusion.230, f32[16,512]{1,0} %get-tuple-element.122, f32[16,512,768]{2,1,0} %get-tuple-element.30, f16[16,512,768]{2,1,0} %get-tuple-element.123, f32[16,512]{1,0} %get-tuple-element.118, f32[16,512]{1,0} %fusion.315, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg40.41, f16[16,512,768]{2,1,0} %arg4.5, f16[16,768]{1,0} %custom-call.27, f16[8192,768]{1,0} %fusion.198), kind=kInput, calls=%fused_computation.50, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/batchnorm/mul/Sum_1"}
  %get-tuple-element.120 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.50), index=2
  %get-tuple-element.121 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.50), index=3
  %get-tuple-element.27 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.50), index=1
  %get-tuple-element.26 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.50), index=0
  %fusion.94 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) fusion(f32[16,512]{1,0} %fusion.256, f32[16,512]{1,0} %get-tuple-element.128, f32[16,512,768]{2,1,0} %get-tuple-element.14, f16[16,512,768]{2,1,0} %get-tuple-element.129, f32[16,512]{1,0} %get-tuple-element.124, f32[16,512]{1,0} %fusion.311, f16[16,512,768]{2,1,0} %get-tuple-element.23, f32[768]{0} %arg36.37, f32[768]{0} %arg37.38, f16[8192,768]{1,0} %custom-call.8, f32[768]{0} %arg24.25, f16[16,512,768]{2,1,0} %arg6.7), kind=kInput, calls=%fused_computation.94, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/batchnorm/mul/Sum_1"}
  %get-tuple-element.126 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.94), index=2
  %get-tuple-element.127 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.94), index=3
  %get-tuple-element.11 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.94), index=1
  %get-tuple-element.10 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.94), index=0
  %fusion.137 = f16[8192,768]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.46), kind=kLoop, calls=%fused_computation.137
  %fusion.136 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg35.36), kind=kLoop, calls=%fused_computation.136
  %custom-call.47 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.137, f16[768,768]{0,1} %fusion.136), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.140 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.44), kind=kLoop, calls=%fused_computation.140
  %fusion.139 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg33.34), kind=kLoop, calls=%fused_computation.139
  %custom-call.45 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.140, f16[768,768]{0,1} %fusion.139), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.142 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.42), kind=kLoop, calls=%fused_computation.142
  %fusion.141 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg31.32), kind=kLoop, calls=%fused_computation.141
  %custom-call.43 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.142, f16[768,768]{0,1} %fusion.141), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.131 = (f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) fusion(f32[768]{0} %arg18.19, f32[16,512]{1,0} %fusion.274, f32[16,512]{1,0} %get-tuple-element.130, f16[16,512,768]{2,1,0} %get-tuple-element.131, f16[8192,768]{1,0} %custom-call.47, f16[8192,768]{1,0} %custom-call.45, f16[8192,768]{1,0} %custom-call.43, f32[16,512]{1,0} %get-tuple-element.12, f32[16,512]{1,0} %get-tuple-element.128, f16[16,512,768]{2,1,0} %get-tuple-element.129, f32[16,512]{1,0} %get-tuple-element.13, f32[16,512,768]{2,1,0} %get-tuple-element.14, f32[768]{0} %arg37.38, f32[16,512]{1,0} %fusion.256, f16[16,512,768]{2,1,0} %arg8.9), kind=kInput, calls=%fused_computation.131, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/mul_2/Sum"}
  %get-tuple-element.7 = f16[16,512,768]{2,1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.131), index=2
  %fusion.120 = (f32[768]{0}, f32[768]{0}) fusion(f16[16,512,768]{2,1,0} %get-tuple-element.7, f32[16,512]{1,0} %fusion.274, f32[16,512]{1,0} %get-tuple-element.130, f16[16,512,768]{2,1,0} %get-tuple-element.131), kind=kInput, calls=%fused_computation.120, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/batchnorm/sub/Sum"}
  %get-tuple-element.5 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}) %fusion.120), index=0
  %get-tuple-element.6 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}) %fusion.120), index=1
  %get-tuple-element.47 = f32[768]{0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.18), index=0
  %fusion.81 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) fusion(f16[16,512,768]{2,1,0} %fusion.156, f16[16,512,768]{2,1,0} %fusion.183, f16[16,512,768]{2,1,0} %fusion.176, f16[16,512,768]{2,1,0} %fusion.149), kind=kInput, calls=%fused_computation.81, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/add/Sum"}
  %get-tuple-element.133 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.81), index=1
  %get-tuple-element.134 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.81), index=2
  %get-tuple-element.132 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.81), index=0
  %get-tuple-element.135 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.81), index=3
  %fusion.69 = (pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) fusion(f32[12,64]{1,0} %get-tuple-element.99, f32[12,64]{1,0} %get-tuple-element.101, f32[12,64]{1,0} %get-tuple-element.96, f32[12,64]{1,0} %get-tuple-element.98, f32[12,64]{1,0} %get-tuple-element.100, f32[12,64]{1,0} %get-tuple-element.97, f32[768]{0} %get-tuple-element.49, f32[768]{0} %get-tuple-element.50, f32[768]{0} %get-tuple-element.120, f32[768]{0} %get-tuple-element.121, f32[768]{0} %get-tuple-element.27, f32[768]{0} %get-tuple-element.26, f32[768]{0} %get-tuple-element.126, f32[768]{0} %get-tuple-element.127, f32[768]{0} %get-tuple-element.11, f32[768]{0} %get-tuple-element.10, f32[768]{0} %get-tuple-element.5, f32[768]{0} %get-tuple-element.6, f32[768]{0} %get-tuple-element.47, f16[16,768]{1,0} %fusion.189, f32[768]{0} %get-tuple-element.133, f32[768]{0} %get-tuple-element.134, f32[768]{0} %get-tuple-element.132, f32[] %arg16.17, f32[768]{0} %get-tuple-element.135), kind=kInput, calls=%fused_computation.69, metadata={op_type="All" op_name="All_22"}
  %get-tuple-element.173 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=29
  %get-tuple-element.172 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=28
  %get-tuple-element.160 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=16
  %custom-call.61 = f16[768,768]{1,0} custom-call(f16[16,768]{1,0} %fusion.197, f16[16,768]{1,0} %fusion.189), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %custom-call.63 = f16[768,768]{1,0} custom-call(f16[1216,768]{1,0} %fusion.217, f16[1216,768]{1,0} %get-tuple-element.48), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"115\"}"
  %fusion.30 = (f32[768,768]{1,0}, f32[768,768]{1,0}) fusion(f16[768,768]{1,0} %custom-call.61, f32[] %arg16.17, f16[768,768]{1,0} %custom-call.63), kind=kLoop, calls=%fused_computation.30, metadata={op_type="Mul" op_name="mul_38"}
  %get-tuple-element.109 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}) %fusion.30), index=1
  %fusion.118 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.44), kind=kLoop, calls=%fused_computation.118
  %custom-call.49 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.118, f16[8192,768]{1,0} %bitcast.61), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.112 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.42), kind=kLoop, calls=%fused_computation.112
  %custom-call.50 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.112, f16[8192,768]{1,0} %bitcast.61), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.106 = f16[768,8192]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.46), kind=kLoop, calls=%fused_computation.106
  %custom-call.51 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.106, f16[8192,768]{1,0} %bitcast.61), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.74 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.34), kind=kLoop, calls=%fused_computation.74
  %custom-call.55 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.74, f16[8192,768]{1,0} %bitcast.72), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.68 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.32), kind=kLoop, calls=%fused_computation.68
  %custom-call.56 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.68, f16[8192,768]{1,0} %bitcast.72), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.62 = f16[768,8192]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.36), kind=kLoop, calls=%fused_computation.62
  %custom-call.57 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.62, f16[8192,768]{1,0} %bitcast.72), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.117 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) fusion(f16[768,768]{1,0} %custom-call.49, f32[] %arg16.17, f16[768,768]{1,0} %custom-call.50, f16[768,768]{1,0} %custom-call.51, f16[768,768]{1,0} %custom-call.55, f16[768,768]{1,0} %custom-call.56, f16[768,768]{1,0} %custom-call.57), kind=kLoop, calls=%fused_computation.117, metadata={op_type="Mul" op_name="mul_6"}
  %get-tuple-element.90 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %fusion.117), index=0
  %get-tuple-element.91 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %fusion.117), index=1
  %get-tuple-element.92 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %fusion.117), index=2
  %fusion.100 = f16[768,8192]{0,1} fusion(f16[16,512,768]{2,1,0} %fusion.149), kind=kLoop, calls=%fused_computation.100
  %custom-call.52 = f16[768,768]{1,0} custom-call(f16[768,8192]{0,1} %fusion.100, f16[8192,768]{1,0} %fusion.261), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %fusion.56 = f16[768,8192]{0,1} fusion(f16[16,512,768]{2,1,0} %fusion.176), kind=kLoop, calls=%fused_computation.56
  %custom-call.58 = f16[768,768]{1,0} custom-call(f16[768,8192]{0,1} %fusion.56, f16[8192,768]{1,0} %fusion.235), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %fusion.99 = (f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) fusion(f16[768,768]{1,0} %custom-call.52, f32[] %arg16.17, f16[768,768]{1,0} %custom-call.58), kind=kLoop, calls=%fused_computation.99, metadata={op_type="Mul" op_name="mul_12"}
  %get-tuple-element.103 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %fusion.99), index=1
  %get-tuple-element.108 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}) %fusion.30), index=0
  %get-tuple-element.93 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %fusion.117), index=3
  %get-tuple-element.95 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %fusion.117), index=5
  %get-tuple-element.102 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %fusion.99), index=0
  %get-tuple-element.94 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %fusion.117), index=4
  %fusion.19 = (pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) fusion(f32[768,768]{1,0} %get-tuple-element.109, f32[768,12,64]{2,1,0} %get-tuple-element.90, f32[768,12,64]{2,1,0} %get-tuple-element.91, f32[768,12,64]{2,1,0} %get-tuple-element.92, f32[12,64,768]{2,1,0} %get-tuple-element.103, f32[768,768]{1,0} %get-tuple-element.108, f32[768,12,64]{2,1,0} %get-tuple-element.93, f32[768,12,64]{2,1,0} %get-tuple-element.95, f32[12,64,768]{2,1,0} %get-tuple-element.102, f32[768,12,64]{2,1,0} %get-tuple-element.94), kind=kInput, calls=%fused_computation.19, metadata={op_type="All" op_name="All_42"}
  %get-tuple-element.176 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=0
  %fusion.23 = f32[30522]{0} fusion(s32[16,76]{1,0} %arg10.11, f32[1216,30522]{1,0} %get-tuple-element.114, f32[1216]{0} %get-tuple-element.113, f32[16,76]{1,0} %arg12.13, f32[] %fusion.281, f32[] %get-tuple-element.110), kind=kInput, calls=%fused_computation.23, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
  %fusion.21 = (pred[], f32[30522]{0}) fusion(f32[] %arg16.17, f32[30522]{0} %fusion.23), kind=kInput, calls=%fused_computation.21, metadata={op_type="All" op_name="All_41"}
  %get-tuple-element.43 = pred[] get-tuple-element((pred[], f32[30522]{0}) %fusion.21), index=0
  %fusion.24 = f32[2]{0} fusion(f16[16,2]{1,0} %fusion.190, f32[] %arg16.17), kind=kLoop, calls=%fused_computation.24, metadata={op_type="Mul" op_name="mul_41"}
  %get-tuple-element.3 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.131), index=0
  %get-tuple-element.4 = f32[16,512]{1,0} get-tuple-element((f32[16,512]{1,0}, f32[16,512]{1,0}, f16[16,512,768]{2,1,0}) %fusion.131), index=1
  %fusion.130 = f16[16,512,768]{2,1,0} fusion(f32[16,512]{1,0} %get-tuple-element.3, f32[16,512]{1,0} %get-tuple-element.4, f16[16,512,768]{2,1,0} %get-tuple-element.7, f32[768]{0} %arg18.19, f32[16,512]{1,0} %get-tuple-element.130, f16[16,512,768]{2,1,0} %get-tuple-element.131, f32[16,512]{1,0} %fusion.274), kind=kLoop, calls=%fused_computation.130, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast/Cast"}
  %bitcast.139 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %fusion.130), metadata={op_type="Reshape" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %custom-call.48 = f16[8,768]{1,0} custom-call(f16[8192,8]{1,0} %fusion.280, f16[8192,768]{1,0} %bitcast.139), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %fusion.124 = f32[2,768]{1,0} fusion(f16[8,768]{1,0} %custom-call.48, f32[] %arg16.17), kind=kLoop, calls=%fused_computation.124, metadata={op_type="Mul" op_name="mul_3"}
  %custom-call.62 = f16[768,8]{1,0} custom-call(f16[16,768]{1,0} %tanh.831, f16[16,8]{1,0} %pad.6), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %fusion.26 = f32[768,2]{1,0} fusion(f16[768,8]{1,0} %custom-call.62, f32[] %arg16.17), kind=kLoop, calls=%fused_computation.26, metadata={op_type="Mul" op_name="mul_40"}
  %fusion.123 = (pred[], pred[]) fusion(f32[2,768]{1,0} %fusion.124, f32[768,2]{1,0} %fusion.26), kind=kInput, calls=%fused_computation.123, metadata={op_type="All" op_name="All_2"}
  %get-tuple-element.175 = pred[] get-tuple-element((pred[], pred[]) %fusion.123), index=1
  %get-tuple-element.158 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=14
  %get-tuple-element.181 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=5
  %get-tuple-element.171 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=27
  %get-tuple-element.170 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=26
  %get-tuple-element.156 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=12
  %fusion.90 = f16[3072,8192]{0,1} fusion(f16[16,512,3072]{2,1,0} %fusion.155), kind=kLoop, calls=%fused_computation.90
  %custom-call.53 = f16[768,3072]{1,0} custom-call(f16[8192,768]{1,0} %fusion.253, f16[3072,8192]{0,1} %fusion.90), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.46 = f16[3072,8192]{0,1} fusion(f16[16,512,3072]{2,1,0} %fusion.182), kind=kLoop, calls=%fused_computation.46
  %custom-call.59 = f16[768,3072]{1,0} custom-call(f16[8192,768]{1,0} %fusion.227, f16[3072,8192]{0,1} %fusion.46), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.89 = (f32[768,3072]{1,0}, f32[768,3072]{1,0}) fusion(f16[768,3072]{1,0} %custom-call.53, f32[] %arg16.17, f16[768,3072]{1,0} %custom-call.59), kind=kLoop, calls=%fused_computation.89, metadata={op_type="Mul" op_name="mul_16"}
  %get-tuple-element.105 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}) %fusion.89), index=1
  %get-tuple-element.104 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}) %fusion.89), index=0
  %fusion.84 = f16[768,8192]{0,1} fusion(f16[16,512,768]{2,1,0} %fusion.156), kind=kLoop, calls=%fused_computation.84
  %custom-call.54 = f16[3072,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.250, f16[768,8192]{0,1} %fusion.84), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.40 = f16[768,8192]{0,1} fusion(f16[16,512,768]{2,1,0} %fusion.183), kind=kLoop, calls=%fused_computation.40
  %custom-call.60 = f16[3072,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.224, f16[768,8192]{0,1} %fusion.40), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.83 = (f32[3072,768]{1,0}, f32[3072,768]{1,0}) fusion(f16[3072,768]{1,0} %custom-call.54, f32[] %arg16.17, f16[3072,768]{1,0} %custom-call.60), kind=kLoop, calls=%fused_computation.83, metadata={op_type="Mul" op_name="mul_18"}
  %get-tuple-element.107 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}) %fusion.83), index=1
  %get-tuple-element.106 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}) %fusion.83), index=0
  %fusion.44 = (pred[], pred[], pred[], pred[]) fusion(f32[768,3072]{1,0} %get-tuple-element.105, f32[768,3072]{1,0} %get-tuple-element.104, f32[3072,768]{1,0} %get-tuple-element.107, f32[3072,768]{1,0} %get-tuple-element.106), kind=kInput, calls=%fused_computation.44, metadata={op_type="All" op_name="All_31"}
  %get-tuple-element.188 = pred[] get-tuple-element((pred[], pred[], pred[], pred[]) %fusion.44), index=2
  %fusion.87 = (f32[3072]{0}, f32[3072]{0}) fusion(f16[16,512,3072]{2,1,0} %fusion.155, f16[16,512,3072]{2,1,0} %fusion.182), kind=kInput, calls=%fused_computation.87, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  %get-tuple-element.136 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}) %fusion.87), index=0
  %get-tuple-element.137 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}) %fusion.87), index=1
  %fusion.85 = (pred[], f32[3072]{0}, pred[], f32[3072]{0}) fusion(f32[] %arg16.17, f32[3072]{0} %get-tuple-element.136, f32[3072]{0} %get-tuple-element.137), kind=kInput, calls=%fused_computation.85, metadata={op_type="All" op_name="All_16"}
  %get-tuple-element.76 = pred[] get-tuple-element((pred[], f32[3072]{0}, pred[], f32[3072]{0}) %fusion.85), index=2
  %get-tuple-element.186 = pred[] get-tuple-element((pred[], pred[], pred[], pred[]) %fusion.44), index=0
  %get-tuple-element.169 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=25
  %get-tuple-element.168 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=24
  %get-tuple-element.154 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=10
  %get-tuple-element.180 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=4
  %get-tuple-element.145 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=1
  %get-tuple-element.183 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=7
  %get-tuple-element.148 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=4
  %get-tuple-element.185 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=9
  %get-tuple-element.144 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=0
  %get-tuple-element.182 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=6
  %get-tuple-element.167 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=23
  %get-tuple-element.166 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=22
  %get-tuple-element.152 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=8
  %get-tuple-element.189 = pred[] get-tuple-element((pred[], pred[], pred[], pred[]) %fusion.44), index=3
  %get-tuple-element.15 = pred[] get-tuple-element((pred[], f32[3072]{0}, pred[], f32[3072]{0}) %fusion.85), index=0
  %get-tuple-element.187 = pred[] get-tuple-element((pred[], pred[], pred[], pred[]) %fusion.44), index=1
  %get-tuple-element.165 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=21
  %get-tuple-element.164 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=20
  %get-tuple-element.150 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=6
  %get-tuple-element.184 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=8
  %get-tuple-element.147 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=3
  %get-tuple-element.179 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=3
  %get-tuple-element.149 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=5
  %get-tuple-element.178 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=2
  %get-tuple-element.146 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=2
  %get-tuple-element.177 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.19), index=1
  %get-tuple-element.163 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=19
  %get-tuple-element.162 = pred[] get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=18
  %get-tuple-element.174 = pred[] get-tuple-element((pred[], pred[]) %fusion.123), index=0
  %custom-call.20 = f16[30528,768]{1,0} custom-call(f16[1216,30528]{1,0} %fusion.318, f16[1216,768]{1,0} %fusion.209), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.129 = f32[30522,768]{1,0} fusion(f16[30528,768]{1,0} %custom-call.20, f16[16,512,768]{2,1,0} %fusion.130, s32[16,512]{1,0} %arg14.15), kind=kInput, calls=%fused_computation.129, metadata={op_type="UnsortedSegmentSum" op_name="AddN_20/inputs_1"}
  %fusion.128 = f32[30522,768]{1,0} fusion(f32[30522,768]{1,0} %fusion.129, f32[] %arg16.17), kind=kLoop, calls=%fused_computation.128, metadata={op_type="Mul" op_name="mul_1"}
  %fusion.127 = pred[] fusion(f32[30522,768]{1,0} %fusion.128), kind=kInput, calls=%fused_computation.127, metadata={op_type="All" op_name="All"}
  %fusion.126 = f32[512,768]{1,0} fusion(f16[16,512,768]{2,1,0} %fusion.130, f32[] %arg16.17), kind=kLoop, calls=%fused_computation.126, metadata={op_type="Mul" op_name="mul_2"}
  %fusion.125 = pred[] fusion(f32[512,768]{1,0} %fusion.126), kind=kInput, calls=%fused_computation.125, metadata={op_type="All" op_name="All_1"}
  %fusion.11 = pred[] fusion(pred[] %get-tuple-element.173, pred[] %get-tuple-element.172, pred[] %get-tuple-element.160, pred[] %get-tuple-element.176, pred[] %get-tuple-element.43, f32[2]{0} %fusion.24, pred[] %get-tuple-element.175, pred[] %get-tuple-element.158, pred[] %get-tuple-element.181, pred[] %get-tuple-element.171, pred[] %get-tuple-element.170, pred[] %get-tuple-element.156, pred[] %get-tuple-element.188, pred[] %get-tuple-element.76, pred[] %get-tuple-element.186, pred[] %get-tuple-element.169, pred[] %get-tuple-element.168, pred[] %get-tuple-element.154, pred[] %get-tuple-element.180, pred[] %get-tuple-element.145, pred[] %get-tuple-element.183, pred[] %get-tuple-element.148, pred[] %get-tuple-element.185, pred[] %get-tuple-element.144, pred[] %get-tuple-element.182, pred[] %get-tuple-element.167, pred[] %get-tuple-element.166, pred[] %get-tuple-element.152, pred[] %get-tuple-element.189, pred[] %get-tuple-element.15, pred[] %get-tuple-element.187, pred[] %get-tuple-element.165, pred[] %get-tuple-element.164, pred[] %get-tuple-element.150, pred[] %get-tuple-element.184, pred[] %get-tuple-element.147, pred[] %get-tuple-element.179, pred[] %get-tuple-element.149, pred[] %get-tuple-element.178, pred[] %get-tuple-element.146, pred[] %get-tuple-element.177, pred[] %get-tuple-element.163, pred[] %get-tuple-element.162, pred[] %get-tuple-element.174, pred[] %fusion.127, pred[] %fusion.125), kind=kLoop, calls=%fused_computation.11, metadata={op_type="Equal" op_name="Equal"}
  %get-tuple-element.151 = f32[768]{0} get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=7
  %fusion.298 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) fusion(f32[768]{0} %get-tuple-element.49, f32[] %arg16.17, f32[768]{0} %get-tuple-element.6, f32[768]{0} %get-tuple-element.5, f32[768]{0} %get-tuple-element.10, f32[768]{0} %get-tuple-element.11, f32[768]{0} %get-tuple-element.127, f32[768]{0} %get-tuple-element.126, f32[768]{0} %get-tuple-element.26, f32[768]{0} %get-tuple-element.27, f32[768]{0} %get-tuple-element.121, f32[768]{0} %get-tuple-element.120, f32[768]{0} %get-tuple-element.50), kind=kLoop, calls=%fused_computation.298, metadata={op_type="Mul" op_name="mul_46"}
  %get-tuple-element.81 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=3
  %get-tuple-element.82 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=4
  %get-tuple-element.153 = f32[768]{0} get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=9
  %get-tuple-element.83 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=5
  %get-tuple-element.84 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=6
  %get-tuple-element.155 = f32[768]{0} get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=11
  %get-tuple-element.85 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=7
  %get-tuple-element.79 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=1
  %get-tuple-element.86 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=8
  %get-tuple-element.77 = f32[3072]{0} get-tuple-element((pred[], f32[3072]{0}, pred[], f32[3072]{0}) %fusion.85), index=3
  %get-tuple-element.157 = f32[768]{0} get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=13
  %get-tuple-element.87 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=9
  %get-tuple-element.88 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=10
  %get-tuple-element.159 = f32[768]{0} get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=15
  %get-tuple-element.80 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=2
  %get-tuple-element.44 = f32[30522]{0} get-tuple-element((pred[], f32[30522]{0}) %fusion.21), index=1
  %get-tuple-element.161 = f32[768]{0} get-tuple-element((pred[], pred[], pred[], pred[], pred[], pred[], pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], f32[768]{0}, pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[], pred[]) %fusion.69), index=17
  %get-tuple-element.89 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=11
  %get-tuple-element.78 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.298), index=0
  %get-tuple-element.16 = f32[3072]{0} get-tuple-element((pred[], f32[3072]{0}, pred[], f32[3072]{0}) %fusion.85), index=1
  %arg67.68 = f32[] parameter(67), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.41 = f32[16]{0} get-tuple-element((f32[16]{0}, f32[16]{0}) %fusion.328), index=0
  %fusion.10 = f32[] fusion(f32[16]{0} %get-tuple-element.41, s32[16,1]{1,0} %arg13.14, f32[16]{0} %fusion.193, f32[2]{0} %arg55.56, f16[16,8]{1,0} %custom-call.25), kind=kInput, calls=%fused_computation.10, metadata={op_type="Mean" op_name="model/bert_pretrain_loss_and_metric_layer/Mean"}
  %get-tuple-element.111 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.282), index=1
  %arg65.66 = f32[] parameter(65), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg63.64 = f32[] parameter(63), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg66.67 = f32[] parameter(66), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.112 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.282), index=2
  %fusion = (f32[], f32[], f32[], f32[]) fusion(f32[] %arg67.68, f32[] %fusion.10, f32[] %get-tuple-element.111, f32[] %get-tuple-element.110, f32[] %arg65.66, f32[] %arg63.64, f32[] %arg66.67, f32[] %get-tuple-element.112), kind=kLoop, calls=%fused_computation, metadata={op_type="AssignAddVariableOp" op_name="AssignAddVariableOp"}
  %get-tuple-element.2 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=2
  %arg64.65 = f32[] parameter(64), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.9 = s64[16]{0} fusion(f32[16]{0} %fusion.193, f32[2]{0} %arg55.56, f16[16,8]{1,0} %custom-call.25), kind=kLoop, calls=%fused_computation.9, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %fusion.8 = f32[] fusion(f32[] %arg64.65, s32[16,1]{1,0} %arg13.14, s64[16]{0} %fusion.9), kind=kLoop, calls=%fused_computation.8, metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_4"}
  %get-tuple-element.1 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=1
  %get-tuple-element.53 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=3
  %get-tuple-element = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=0
  ROOT %tuple.113 = (f32[], pred[], f32[768,12,64]{2,1,0}, f32[30522,768]{1,0}, f32[512,768]{1,0}, f32[12,64]{1,0}, f32[12,64,768]{2,1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,3072]{1,0}, f32[3072,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[2,768]{1,0}, f32[768]{0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[12,64,768]{2,1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,3072]{1,0}, f32[3072]{0}, f32[3072,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,768]{1,0}, f32[768]{0}, f32[768,2]{1,0}, f32[768]{0}, f32[2]{0}, f32[30522]{0}, f32[768,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[3072]{0}, f32[], f32[], f32[], f32[], f32[]) tuple(f32[] %copy.142, pred[] %fusion.11, f32[768,12,64]{2,1,0} %get-tuple-element.92, f32[30522,768]{1,0} %fusion.128, f32[512,768]{1,0} %fusion.126, f32[12,64]{1,0} %get-tuple-element.98, f32[12,64,768]{2,1,0} %get-tuple-element.102, f32[768]{0} %get-tuple-element.151, f32[768]{0} %get-tuple-element.81, f32[768]{0} %get-tuple-element.82, f32[768,3072]{1,0} %get-tuple-element.104, f32[3072,768]{1,0} %get-tuple-element.106, f32[768]{0} %get-tuple-element.153, f32[768]{0} %get-tuple-element.83, f32[2,768]{1,0} %fusion.124, f32[768]{0} %get-tuple-element.84, f32[768,12,64]{2,1,0} %get-tuple-element.93, f32[12,64]{1,0} %get-tuple-element.99, f32[768,12,64]{2,1,0} %get-tuple-element.94, f32[12,64]{1,0} %get-tuple-element.100, f32[768,12,64]{2,1,0} %get-tuple-element.95, f32[12,64]{1,0} %get-tuple-element.101, f32[12,64,768]{2,1,0} %get-tuple-element.103, f32[768]{0} %get-tuple-element.155, f32[768]{0} %get-tuple-element.85, f32[768]{0} %get-tuple-element.79, f32[768]{0} %get-tuple-element.86, f32[768,3072]{1,0} %get-tuple-element.105, f32[3072]{0} %get-tuple-element.77, f32[3072,768]{1,0} %get-tuple-element.107, f32[768]{0} %get-tuple-element.157, f32[768]{0} %get-tuple-element.87, f32[768]{0} %get-tuple-element.88, f32[768,768]{1,0} %get-tuple-element.108, f32[768]{0} %get-tuple-element.159, f32[768,2]{1,0} %fusion.26, f32[768]{0} %get-tuple-element.80, f32[2]{0} %fusion.24, f32[30522]{0} %get-tuple-element.44, f32[768,768]{1,0} %get-tuple-element.109, f32[768]{0} %get-tuple-element.161, f32[768]{0} %get-tuple-element.89, f32[768]{0} %get-tuple-element.78, f32[768,12,64]{2,1,0} %get-tuple-element.90, f32[12,64]{1,0} %get-tuple-element.96, f32[768,12,64]{2,1,0} %get-tuple-element.91, f32[12,64]{1,0} %get-tuple-element.97, f32[3072]{0} %get-tuple-element.16, f32[] %get-tuple-element.2, f32[] %fusion.8, f32[] %get-tuple-element.1, f32[] %get-tuple-element.53, f32[] %get-tuple-element)
}

