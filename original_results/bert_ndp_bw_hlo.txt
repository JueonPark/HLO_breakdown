HloModule cluster_0__XlaCompiledKernel_true__XlaHasReferenceVars_false__XlaNumConstantArgs_123__XlaNumResourceArgs_156_.2342

%max_half_.413 (x.414: f16[], y.415: f16[]) -> f16[] {
  %x.414 = f16[] parameter(0)
  %y.415 = f16[] parameter(1)
  ROOT %maximum.416 = f16[] maximum(f16[] %x.414, f16[] %y.415)
}

%add_float_.423 (x.424: f32[], y.425: f32[]) -> f32[] {
  %x.424 = f32[] parameter(0)
  %y.425 = f32[] parameter(1)
  ROOT %add.426 = f32[] add(f32[] %x.424, f32[] %y.425)
}

%max_half_.557 (x.558: f16[], y.559: f16[]) -> f16[] {
  %x.558 = f16[] parameter(0)
  %y.559 = f16[] parameter(1)
  ROOT %maximum.560 = f16[] maximum(f16[] %x.558, f16[] %y.559)
}

%add_float_.567 (x.568: f32[], y.569: f32[]) -> f32[] {
  %x.568 = f32[] parameter(0)
  %y.569 = f32[] parameter(1)
  ROOT %add.570 = f32[] add(f32[] %x.568, f32[] %y.569)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_softmax_Sum-reduction.1306 (x.1307: f32[], y.1308: f32[]) -> f32[] {
  %x.1307 = f32[] parameter(0)
  %y.1308 = f32[] parameter(1)
  ROOT %add.1309 = f32[] add(f32[] %x.1307, f32[] %y.1308)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_softmax_Sum-reduction.1566 (x.1567: f32[], y.1568: f32[]) -> f32[] {
  %x.1567 = f32[] parameter(0)
  %y.1568 = f32[] parameter(1)
  ROOT %add.1569 = f32[] add(f32[] %x.1567, f32[] %y.1568)
}

%Mean-reduction.1836 (x.1837: f32[], y.1838: f32[]) -> f32[] {
  %x.1837 = f32[] parameter(0)
  %y.1838 = f32[] parameter(1)
  ROOT %add.1839 = f32[] add(f32[] %x.1837, f32[] %y.1838)
}

%fused_computation (param_0: f32[], param_1.581: f32[], param_2.416: f32[], param_3.315: f32[], param_4.223: f32[], param_5.141: f32[], param_6.96: f32[], param_7.60: f32[]) -> (f32[], f32[], f32[], f32[]) {
  %param_0 = f32[] parameter(0)
  %param_1.581 = f32[] parameter(1)
  %constant_97 = f32[] constant(0.0625), metadata={op_type="Const" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/ExpandDims"}
  %multiply.133 = f32[] multiply(f32[] %param_1.581, f32[] %constant_97), metadata={op_type="Mean" op_name="model/bert_pretrain_loss_and_metric_layer/Mean"}
  %param_3.315 = f32[] parameter(3)
  %constant_98 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %compare.83 = pred[] compare(f32[] %param_3.315, f32[] %constant_98), direction=EQ, metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %param_2.416 = f32[] parameter(2)
  %divide.30 = f32[] divide(f32[] %param_2.416, f32[] %param_3.315), metadata={op_type="DivNoNan" op_name="model/bert_pretrain_loss_and_metric_layer/div_no_nan"}
  %select.78 = f32[] select(pred[] %compare.83, f32[] %constant_98, f32[] %divide.30), metadata={op_type="DivNoNan" op_name="model/bert_pretrain_loss_and_metric_layer/div_no_nan"}
  %add.38 = f32[] add(f32[] %multiply.133, f32[] %select.78), metadata={op_type="AddV2" op_name="model/bert_pretrain_loss_and_metric_layer/add"}
  %broadcast.109 = f32[16]{0} broadcast(f32[] %add.38), dimensions={}
  %reduce.33 = f32[] reduce(f32[16]{0} %broadcast.109, f32[] %constant_98), dimensions={0}, to_apply=%Mean-reduction.1836, metadata={op_type="Mean" op_name="Mean"}
  %multiply.36 = f32[] multiply(f32[] %reduce.33, f32[] %constant_97), metadata={op_type="Mean" op_name="Mean"}
  %add.37 = f32[] add(f32[] %param_0, f32[] %multiply.36), metadata={op_type="AssignAddVariableOp" op_name="AssignAddVariableOp"}
  %param_4.223 = f32[] parameter(4)
  %add.179.clone.1 = f32[] add(f32[] %param_4.223, f32[] %select.78), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_2"}
  %param_5.141 = f32[] parameter(5)
  %add.92.clone.1 = f32[] add(f32[] %param_5.141, f32[] %multiply.133), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_6"}
  %param_6.96 = f32[] parameter(6)
  %param_7.60 = f32[] parameter(7)
  %constant_89_clone_1 = f32[] constant(1e-05), metadata={op_type="AddV2" op_name="model/bert_pretrain_loss_and_metric_layer/add_1"}
  %add.40.clone.1 = f32[] add(f32[] %param_3.315, f32[] %constant_89_clone_1), metadata={op_type="AddV2" op_name="model/bert_pretrain_loss_and_metric_layer/add_1"}
  %divide.0.clone.1 = f32[] divide(f32[] %param_7.60, f32[] %add.40.clone.1), metadata={op_type="RealDiv" op_name="model/bert_pretrain_loss_and_metric_layer/truediv"}
  %add.39.clone.1 = f32[] add(f32[] %param_6.96, f32[] %divide.0.clone.1), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp"}
  ROOT %tuple.16 = (f32[], f32[], f32[], f32[]) tuple(f32[] %add.37, f32[] %add.179.clone.1, f32[] %add.92.clone.1, f32[] %add.39.clone.1)
}

%model_bert_pretrain_loss_and_metric_layer_Sum_2-reduction.1872 (x.1873: f32[], y.1874: f32[]) -> f32[] {
  %x.1873 = f32[] parameter(0)
  %y.1874 = f32[] parameter(1)
  ROOT %add.1875 = f32[] add(f32[] %x.1873, f32[] %y.1874)
}

%model_bert_pretrain_loss_and_metric_layer_Sum_3-reduction.210 (x.211: f32[], y.212: f32[]) -> f32[] {
  %x.211 = f32[] parameter(0)
  %y.212 = f32[] parameter(1)
  ROOT %add.213 = f32[] add(f32[] %x.211, f32[] %y.212)
}

%model_bert_pretrain_loss_and_metric_layer_Sum-reduction.1818 (x.1819: f32[], y.1820: f32[]) -> f32[] {
  %x.1819 = f32[] parameter(0)
  %y.1820 = f32[] parameter(1)
  ROOT %add.1821 = f32[] add(f32[] %x.1819, f32[] %y.1820)
}

%fused_computation.2 (param_0.605: s32[16,76], param_1.524: s64[16,76], param_2.376: f32[16,76], param_3.336: f32[1216]) -> (f32[], f32[], f32[]) {
  %param_0.605 = s32[16,76]{1,0} parameter(0)
  %param_1.524 = s64[16,76]{1,0} parameter(1)
  %convert.33 = s32[16,76]{1,0} convert(s64[16,76]{1,0} %param_1.524), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_7"}
  %compare.0 = pred[16,76]{1,0} compare(s32[16,76]{1,0} %param_0.605, s32[16,76]{1,0} %convert.33), direction=EQ, metadata={op_type="Equal" op_name="model/bert_pretrain_loss_and_metric_layer/Equal"}
  %param_2.376 = f32[16,76]{1,0} parameter(2)
  %convert.296 = s32[16,76]{1,0} convert(f32[16,76]{1,0} %param_2.376), metadata={op_type="Cast" op_name="model/Cast"}
  %convert.295 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %convert.296), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast"}
  %constant_99 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.110 = f32[16,76]{1,0} broadcast(f32[] %constant_99), dimensions={}
  %select.19 = f32[16,76]{1,0} select(pred[16,76]{1,0} %compare.0, f32[16,76]{1,0} %convert.295, f32[16,76]{1,0} %broadcast.110), metadata={op_type="Mul" op_name="model/bert_pretrain_loss_and_metric_layer/mul_1"}
  %bitcast.137 = f32[1216]{0} bitcast(f32[16,76]{1,0} %select.19)
  %reduce.34 = f32[] reduce(f32[1216]{0} %bitcast.137, f32[] %constant_99), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum_2-reduction.1872, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_2"}
  %bitcast.198.clone.1 = f32[1216]{0} bitcast(f32[16,76]{1,0} %convert.295)
  %reduce.58.clone.1 = f32[] reduce(f32[1216]{0} %bitcast.198.clone.1, f32[] %constant_99), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum_3-reduction.210, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %param_3.336 = f32[1216]{0} parameter(3)
  %bitcast.140.clone.1 = f32[16,76]{1,0} bitcast(f32[1216]{0} %param_3.336), metadata={op_type="Reshape" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/Reshape_2"}
  %multiply.37.clone.1 = f32[16,76]{1,0} multiply(f32[16,76]{1,0} %convert.295, f32[16,76]{1,0} %bitcast.140.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrain_loss_and_metric_layer/mul"}
  %bitcast.139.clone.1 = f32[1216]{0} bitcast(f32[16,76]{1,0} %multiply.37.clone.1)
  %reduce.36.clone.1 = f32[] reduce(f32[1216]{0} %bitcast.139.clone.1, f32[] %constant_99), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum-reduction.1818, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum"}
  ROOT %tuple.18 = (f32[], f32[], f32[]) tuple(f32[] %reduce.34, f32[] %reduce.58.clone.1, f32[] %reduce.36.clone.1)
}

%min_S64.1860 (lhs.1861: s64[], rhs.1862: s64[]) -> s64[] {
  %lhs.1861 = s64[] parameter(0)
  %rhs.1862 = s64[] parameter(1)
  ROOT %minimum.1863 = s64[] minimum(s64[] %lhs.1861, s64[] %rhs.1862)
}

%fused_computation.3 (param_0.654: f32[16,76], param_1.592: f32[30522], param_2.424: f16[1216,30528]) -> s64[16,76] {
  %param_2.424 = f16[1216,30528]{1,0} parameter(2)
  %slice.35 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_2.424), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_1.592 = f32[30522]{0} parameter(1)
  %convert.378 = f16[30522]{0} convert(f32[30522]{0} %param_1.592), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.623 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.378), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.183 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.35, f16[1216,30522]{1,0} %broadcast.623), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.377 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.183), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %bitcast.277 = f32[16,76,30522]{2,1,0} bitcast(f32[1216,30522]{1,0} %convert.377), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_1"}
  %param_0.654 = f32[16,76]{1,0} parameter(0)
  %broadcast.112 = f32[16,76,30522]{2,1,0} broadcast(f32[16,76]{1,0} %param_0.654), dimensions={0,1}, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %compare.1 = pred[16,76,30522]{2,1,0} compare(f32[16,76,30522]{2,1,0} %bitcast.277, f32[16,76,30522]{2,1,0} %broadcast.112), direction=EQ, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %iota.0 = s64[16,76,30522]{2,1,0} iota(), iota_dimension=2, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %constant_100 = s64[] constant(9223372036854775807), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %broadcast.111 = s64[16,76,30522]{2,1,0} broadcast(s64[] %constant_100), dimensions={}, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %select.20 = s64[16,76,30522]{2,1,0} select(pred[16,76,30522]{2,1,0} %compare.1, s64[16,76,30522]{2,1,0} %iota.0, s64[16,76,30522]{2,1,0} %broadcast.111), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  ROOT %reduce.35 = s64[16,76]{1,0} reduce(s64[16,76,30522]{2,1,0} %select.20, s64[] %constant_100), dimensions={2}, to_apply=%min_S64.1860, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
}

%add_float_.1004 (x.1005: f32[], y.1006: f32[]) -> f32[] {
  %x.1005 = f32[] parameter(0)
  %y.1006 = f32[] parameter(1)
  ROOT %add.1007 = f32[] add(f32[] %x.1005, f32[] %y.1006)
}

%fused_computation.7 (param_0.597: f32[1216], param_1.521: s32[16,76], param_2.373: f32[1216], param_3.255: f32[30522], param_4.156: f16[1216,30528]) -> f32[1216] {
  %param_1.521 = s32[16,76]{1,0} parameter(1)
  %convert.238 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %param_1.521), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_2"}
  %convert.237 = s64[16,76]{1,0} convert(f32[16,76]{1,0} %convert.238), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %bitcast.251 = s64[1216]{0} bitcast(s64[16,76]{1,0} %convert.237), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %broadcast.443 = s64[1216,30522]{1,0} broadcast(s64[1216]{0} %bitcast.251), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.10 = s64[1216,30522]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.49 = pred[1216,30522]{1,0} compare(s64[1216,30522]{1,0} %broadcast.443, s64[1216,30522]{1,0} %iota.10), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_362 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %broadcast.442 = f32[1216,30522]{1,0} broadcast(f32[] %constant_362), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_103 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.441 = f32[1216,30522]{1,0} broadcast(f32[] %constant_103), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.50 = f32[1216,30522]{1,0} select(pred[1216,30522]{1,0} %compare.49, f32[1216,30522]{1,0} %broadcast.442, f32[1216,30522]{1,0} %broadcast.441), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_360 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.439 = s64[1216]{0} broadcast(s64[] %constant_360), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.48 = pred[1216]{0} compare(s64[1216]{0} %broadcast.439, s64[1216]{0} %bitcast.251), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_359 = s64[] constant(30522), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.438 = s64[1216]{0} broadcast(s64[] %constant_359), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.47 = pred[1216]{0} compare(s64[1216]{0} %bitcast.251, s64[1216]{0} %broadcast.438), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %and.7 = pred[1216]{0} and(pred[1216]{0} %compare.48, pred[1216]{0} %compare.47), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.437 = f32[1216]{0} broadcast(f32[] %constant_103), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_358 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.436 = f32[1216]{0} broadcast(f32[] %constant_358), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.49 = f32[1216]{0} select(pred[1216]{0} %and.7, f32[1216]{0} %broadcast.437, f32[1216]{0} %broadcast.436), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.435 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %select.49), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %add.150 = f32[1216,30522]{1,0} add(f32[1216,30522]{1,0} %select.50, f32[1216,30522]{1,0} %broadcast.435), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %negate.10 = f32[1216,30522]{1,0} negate(f32[1216,30522]{1,0} %add.150), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_4.156 = f16[1216,30528]{1,0} parameter(4)
  %slice.31 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_4.156), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_3.255 = f32[30522]{0} parameter(3)
  %convert.266 = f16[30522]{0} convert(f32[30522]{0} %param_3.255), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.525 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.266), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.170 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.31, f16[1216,30522]{1,0} %broadcast.525), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.265 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.170), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %param_2.373 = f32[1216]{0} parameter(2)
  %broadcast.523 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_2.373), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.39 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %convert.265, f32[1216,30522]{1,0} %broadcast.523), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.597 = f32[1216]{0} parameter(0)
  %broadcast.113 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_0.597), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.6 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %subtract.39, f32[1216,30522]{1,0} %broadcast.113), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.38 = f32[1216,30522]{1,0} multiply(f32[1216,30522]{1,0} %negate.10, f32[1216,30522]{1,0} %subtract.6), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %reduce.37 = f32[1216]{0} reduce(f32[1216,30522]{1,0} %multiply.38, f32[] %constant_103), dimensions={1}, to_apply=%add_float_.1004, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
}

%model_bert_pretrain_loss_and_metric_layer_Sum_6-reduction.725 (x.726: f32[], y.727: f32[]) -> f32[] {
  %x.726 = f32[] parameter(0)
  %y.727 = f32[] parameter(1)
  ROOT %add.728 = f32[] add(f32[] %x.726, f32[] %y.727)
}

%fused_computation.8 (param_0.13: f32[], param_1.284: s32[16,1], param_2.174: s64[16]) -> f32[] {
  %param_0.13 = f32[] parameter(0)
  %param_2.174 = s64[16]{0} parameter(2)
  %convert.36 = s32[16]{0} convert(s64[16]{0} %param_2.174), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_11"}
  %param_1.284 = s32[16,1]{1,0} parameter(1)
  %bitcast.141 = s32[16]{0} bitcast(s32[16,1]{1,0} %param_1.284), metadata={op_type="Squeeze" op_name="model/bert_pretrain_loss_and_metric_layer/Squeeze"}
  %compare.2 = pred[16]{0} compare(s32[16]{0} %convert.36, s32[16]{0} %bitcast.141), direction=EQ, metadata={op_type="Equal" op_name="model/bert_pretrain_loss_and_metric_layer/Equal_1"}
  %convert.35 = f32[16]{0} convert(pred[16]{0} %compare.2), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_12"}
  %constant_104 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.38 = f32[] reduce(f32[16]{0} %convert.35, f32[] %constant_104), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Sum_6-reduction.725, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_6"}
  ROOT %add.41 = f32[] add(f32[] %param_0.13, f32[] %reduce.38), metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_4"}
}

%min_S64.714 (lhs.715: s64[], rhs.716: s64[]) -> s64[] {
  %lhs.715 = s64[] parameter(0)
  %rhs.716 = s64[] parameter(1)
  ROOT %minimum.717 = s64[] minimum(s64[] %lhs.715, s64[] %rhs.716)
}

%fused_computation.9 (param_0.556: f32[16], param_1.468: f32[2], param_2.325: f16[16,8]) -> s64[16] {
  %param_2.325 = f16[16,8]{1,0} parameter(2)
  %slice.15 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_2.325), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_1.468 = f32[2]{0} parameter(1)
  %convert.223 = f16[2]{0} convert(f32[2]{0} %param_1.468), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.408 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.223), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.138 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.15, f16[16,2]{1,0} %broadcast.408), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.37 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.138), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %param_0.556 = f32[16]{0} parameter(0)
  %broadcast.177 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_0.556), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.3 = pred[16,2]{1,0} compare(f32[16,2]{1,0} %convert.37, f32[16,2]{1,0} %broadcast.177), direction=EQ, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %iota.1 = s64[16,2]{1,0} iota(), iota_dimension=1, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %constant_105 = s64[] constant(9223372036854775807), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %broadcast.114 = s64[16,2]{1,0} broadcast(s64[] %constant_105), dimensions={}, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %select.22 = s64[16,2]{1,0} select(pred[16,2]{1,0} %compare.3, s64[16,2]{1,0} %iota.1, s64[16,2]{1,0} %broadcast.114), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  ROOT %reduce.39 = s64[16]{0} reduce(s64[16,2]{1,0} %select.22, s64[] %constant_105), dimensions={1}, to_apply=%min_S64.714, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
}

%model_bert_pretrain_loss_and_metric_layer_Mean-reduction.854 (x.855: f32[], y.856: f32[]) -> f32[] {
  %x.855 = f32[] parameter(0)
  %y.856 = f32[] parameter(1)
  ROOT %add.857 = f32[] add(f32[] %x.855, f32[] %y.856)
}

%fused_computation.10 (param_0.564: f32[16], param_1.479: s32[16,1], param_2.337: f32[16], param_3.231: f32[2], param_4.127: f16[16,8]) -> f32[] {
  %param_1.479 = s32[16,1]{1,0} parameter(1)
  %convert.160 = f32[16,1]{1,0} convert(s32[16,1]{1,0} %param_1.479), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_5"}
  %convert.159 = s64[16,1]{1,0} convert(f32[16,1]{1,0} %convert.160), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %bitcast.229 = s64[16]{0} bitcast(s64[16,1]{1,0} %convert.159), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %broadcast.296 = s64[16,2]{1,0} broadcast(s64[16]{0} %bitcast.229), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.6 = s64[16,2]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.25 = pred[16,2]{1,0} compare(s64[16,2]{1,0} %broadcast.296, s64[16,2]{1,0} %iota.6), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_259 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %broadcast.294 = f32[16,2]{1,0} broadcast(f32[] %constant_259), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_107 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.293 = f32[16,2]{1,0} broadcast(f32[] %constant_107), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.42 = f32[16,2]{1,0} select(pred[16,2]{1,0} %compare.25, f32[16,2]{1,0} %broadcast.294, f32[16,2]{1,0} %broadcast.293), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_256 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.292 = s64[16]{0} broadcast(s64[] %constant_256), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.24 = pred[16]{0} compare(s64[16]{0} %broadcast.292, s64[16]{0} %bitcast.229), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_255 = s64[] constant(2), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.291 = s64[16]{0} broadcast(s64[] %constant_255), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.23 = pred[16]{0} compare(s64[16]{0} %bitcast.229, s64[16]{0} %broadcast.291), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %and.3 = pred[16]{0} and(pred[16]{0} %compare.24, pred[16]{0} %compare.23), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.290 = f32[16]{0} broadcast(f32[] %constant_107), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_254 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.289 = f32[16]{0} broadcast(f32[] %constant_254), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.41 = f32[16]{0} select(pred[16]{0} %and.3, f32[16]{0} %broadcast.290, f32[16]{0} %broadcast.289), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.288 = f32[16,2]{1,0} broadcast(f32[16]{0} %select.41), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %add.104 = f32[16,2]{1,0} add(f32[16,2]{1,0} %select.42, f32[16,2]{1,0} %broadcast.288), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %negate.11 = f32[16,2]{1,0} negate(f32[16,2]{1,0} %add.104), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %param_4.127 = f16[16,8]{1,0} parameter(4)
  %slice.21 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_4.127), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_3.231 = f32[2]{0} parameter(3)
  %convert.234 = f16[2]{0} convert(f32[2]{0} %param_3.231), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.420 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.234), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.144 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.21, f16[16,2]{1,0} %broadcast.420), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.233 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.144), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %param_2.337 = f32[16]{0} parameter(2)
  %broadcast.419 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_2.337), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.31 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %convert.233, f32[16,2]{1,0} %broadcast.419), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.564 = f32[16]{0} parameter(0)
  %broadcast.115 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_0.564), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.7 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %subtract.31, f32[16,2]{1,0} %broadcast.115), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.39 = f32[16,2]{1,0} multiply(f32[16,2]{1,0} %negate.11, f32[16,2]{1,0} %subtract.7), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %bitcast.142 = f32[32]{0} bitcast(f32[16,2]{1,0} %multiply.39)
  ROOT %reduce.40 = f32[] reduce(f32[32]{0} %bitcast.142, f32[] %constant_107), dimensions={0}, to_apply=%model_bert_pretrain_loss_and_metric_layer_Mean-reduction.854, metadata={op_type="Mean" op_name="model/bert_pretrain_loss_and_metric_layer/Mean"}
}

%add_float_.1113 (x.1114: f32[], y.1115: f32[]) -> f32[] {
  %x.1114 = f32[] parameter(0)
  %y.1115 = f32[] parameter(1)
  ROOT %add.1116 = f32[] add(f32[] %x.1114, f32[] %y.1115)
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_sub_Sum-reduction.1793 (x.1794: f32[], y.1795: f32[]) -> f32[] {
  %x.1794 = f32[] parameter(0)
  %y.1795 = f32[] parameter(1)
  ROOT %add.1796 = f32[] add(f32[] %x.1794, f32[] %y.1795)
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum_1-reduction.1779 (x.1780: f32[], y.1781: f32[]) -> f32[] {
  %x.1780 = f32[] parameter(0)
  %y.1781 = f32[] parameter(1)
  ROOT %add.1782 = f32[] add(f32[] %x.1780, f32[] %y.1781)
}

%fused_computation.11 (param_0.658: f16[1216,768], param_1.599: f16[1216,768], param_2.434: f32[1216], param_3.325: f32[1216], param_4.234: f16[1216,768], param_5.157: f32[1216], param_6.93: f32[768], param_7.58: f32[1216], param_8.51: f16[1216,768]) -> (f32[768], f16[1216,768], f32[768], f32[768]) {
  %constant_168_clone_1 = f16[] constant(0.13416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %broadcast.124.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_168_clone_1), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %param_1.599 = f16[1216,768]{1,0} parameter(1)
  %multiply.141.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_1.599, f16[1216,768]{1,0} %param_1.599), metadata={op_type="Square" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/Pow"}
  %param_8.51 = f16[1216,768]{1,0} parameter(8)
  %convert.89.clone.1 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_8.51), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1/Cast"}
  %constant_397_clone_1 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %broadcast.494.clone.1 = f32[1216]{0} broadcast(f32[] %constant_397_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %param_7.58 = f32[1216]{0} parameter(7)
  %constant_392_clone_1 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.492.clone.1 = f32[1216]{0} broadcast(f32[] %constant_392_clone_1), dimensions={}
  %multiply.174.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_7.58, f32[1216]{0} %broadcast.492.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %add.154.clone.1 = f32[1216]{0} add(f32[1216]{0} %broadcast.494.clone.1, f32[1216]{0} %multiply.174.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %rsqrt.8.clone.1 = f32[1216]{0} rsqrt(f32[1216]{0} %add.154.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %broadcast.230.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %rsqrt.8.clone.1), dimensions={0}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %param_6.93 = f32[768]{0} parameter(6)
  %broadcast.229.clone.1 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_6.93), dimensions={1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.139.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.230.clone.1, f32[1216,768]{1,0} %broadcast.229.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.100.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.89.clone.1, f32[1216,768]{1,0} %multiply.139.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1/Mul"}
  %constant_91_clone_1 = f32[] constant(2), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.123.clone.1 = f32[1216,1]{1,0} broadcast(f32[] %constant_91_clone_1), dimensions={}
  %broadcast.122.clone.1 = f32[1216,1]{1,0} broadcast(f32[] %constant_392_clone_1), dimensions={}
  %bitcast.195.clone.1 = f32[1216,1]{1,0} bitcast(f32[1216]{0} %rsqrt.8.clone.1), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %multiply.99.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %bitcast.195.clone.1, f32[1216,1]{1,0} %bitcast.195.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %multiply.98.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %multiply.99.clone.1, f32[1216,1]{1,0} %bitcast.195.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %param_5.157 = f32[1216]{0} parameter(5)
  %constant_90_clone_1 = f32[] constant(-0.5)
  %broadcast.121.clone.1 = f32[1216]{0} broadcast(f32[] %constant_90_clone_1), dimensions={}
  %multiply.97.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_5.157, f32[1216]{0} %broadcast.121.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %bitcast.194.clone.1 = f32[1216,1]{1,0} bitcast(f32[1216]{0} %multiply.97.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %multiply.96.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %multiply.98.clone.1, f32[1216,1]{1,0} %bitcast.194.clone.1), metadata={op_type="RsqrtGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/RsqrtGrad"}
  %multiply.95.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %broadcast.122.clone.1, f32[1216,1]{1,0} %multiply.96.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv"}
  %multiply.94.clone.1 = f32[1216,1]{1,0} multiply(f32[1216,1]{1,0} %broadcast.123.clone.1, f32[1216,1]{1,0} %multiply.95.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %bitcast.193.clone.1 = f32[1216]{0} bitcast(f32[1216,1]{1,0} %multiply.94.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/Mul"}
  %broadcast.120.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %bitcast.193.clone.1), dimensions={0}
  %param_4.234 = f16[1216,768]{1,0} parameter(4)
  %convert.250.clone.1 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_4.234), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %param_3.325 = f32[1216]{0} parameter(3)
  %multiply.172.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_3.325, f32[1216]{0} %broadcast.492.clone.1), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.487.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.172.clone.1), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %subtract.35.clone.1 = f32[1216,768]{1,0} subtract(f32[1216,768]{1,0} %convert.250.clone.1, f32[1216,768]{1,0} %broadcast.487.clone.1), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.93.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.120.clone.1, f32[1216,768]{1,0} %subtract.35.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mul_1"}
  %add.67.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.100.clone.1, f32[1216,768]{1,0} %multiply.93.clone.1), metadata={op_type="AddN" op_name="AddN_1"}
  %param_2.434 = f32[1216]{0} parameter(2)
  %multiply.92.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %broadcast.492.clone.1, f32[1216]{0} %param_2.434), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.119.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.92.clone.1), dimensions={0}
  %add.66.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %add.67.clone.1, f32[1216,768]{1,0} %broadcast.119.clone.1), metadata={op_type="AddN" op_name="AddN_1"}
  %convert.88.clone.1 = f16[1216,768]{1,0} convert(f32[1216,768]{1,0} %add.66.clone.1), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast/Cast"}
  %constant_170_clone_1 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.232.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_170_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.140.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_1.599, f16[1216,768]{1,0} %broadcast.232.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.91.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %convert.88.clone.1, f16[1216,768]{1,0} %multiply.140.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_3/Mul_1"}
  %constant_172_clone_1 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.234.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_172_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %param_0.658 = f16[1216,768]{1,0} parameter(0)
  %multiply.90.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.658, f16[1216,768]{1,0} %param_0.658), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/TanhGrad"}
  %subtract.12.clone.1 = f16[1216,768]{1,0} subtract(f16[1216,768]{1,0} %broadcast.234.clone.1, f16[1216,768]{1,0} %multiply.90.clone.1), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/TanhGrad"}
  %multiply.89.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.91.clone.1, f16[1216,768]{1,0} %subtract.12.clone.1), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/TanhGrad"}
  %constant_171_clone_1 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.233.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_171_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2"}
  %multiply.88.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.89.clone.1, f16[1216,768]{1,0} %broadcast.233.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2/Mul"}
  %multiply.87.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.141.clone.1, f16[1216,768]{1,0} %multiply.88.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul"}
  %multiply.86.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %broadcast.124.clone.1, f16[1216,768]{1,0} %multiply.87.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %multiply.85.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %convert.88.clone.1, f16[1216,768]{1,0} %broadcast.232.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_3/Mul"}
  %add.95.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %broadcast.234.clone.1, f16[1216,768]{1,0} %param_0.658), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %multiply.84.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %multiply.85.clone.1, f16[1216,768]{1,0} %add.95.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul/Mul"}
  %add.65.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %multiply.86.clone.1, f16[1216,768]{1,0} %multiply.84.clone.1), metadata={op_type="AddN" op_name="AddN_2"}
  %add.64.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %add.65.clone.1, f16[1216,768]{1,0} %multiply.88.clone.1), metadata={op_type="AddN" op_name="AddN_2"}
  %convert.38 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %add.64.clone.1), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %constant_108 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.41 = f32[768]{0} reduce(f32[1216,768]{1,0} %convert.38, f32[] %constant_108), dimensions={0}, to_apply=%add_float_.1113, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %reduce.43.clone.1 = f32[768]{0} reduce(f32[1216,768]{1,0} %convert.89.clone.1, f32[] %constant_108), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_sub_Sum-reduction.1793, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub/Sum"}
  %multiply.201.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.89.clone.1, f32[1216,768]{1,0} %convert.250.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1/Mul_1"}
  %negate.20.clone.1 = f32[1216,768]{1,0} negate(f32[1216,768]{1,0} %convert.89.clone.1), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub/Neg"}
  %multiply.199.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %negate.20.clone.1, f32[1216,768]{1,0} %broadcast.487.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Mul_1"}
  %add.178.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.201.clone.1, f32[1216,768]{1,0} %multiply.199.clone.1), metadata={op_type="AddN" op_name="AddN"}
  %multiply.40.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %add.178.clone.1, f32[1216,768]{1,0} %broadcast.230.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Mul_1"}
  %reduce.42.clone.1 = f32[768]{0} reduce(f32[1216,768]{1,0} %multiply.40.clone.1, f32[] %constant_108), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum_1-reduction.1779, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Sum_1"}
  ROOT %tuple.14 = (f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %reduce.41, f16[1216,768]{1,0} %add.64.clone.1, f32[768]{0} %reduce.43.clone.1, f32[768]{0} %reduce.42.clone.1)
}

%scatter-combiner.1727 (p0.1728: f32[], p1.1729: f32[]) -> f32[] {
  %p0.1728 = f32[] parameter(0)
  %p1.1729 = f32[] parameter(1)
  ROOT %add.1730 = f32[] add(f32[] %p0.1728, f32[] %p1.1729)
}

%fused_computation.14 (param_0.370: f16[30528,768], param_1.292: f16[1,16,512,768], param_2.179: s32[16,512]) -> f32[30522,768] {
  %param_0.370 = f16[30528,768]{1,0} parameter(0)
  %slice.8 = f16[30522,768]{1,0} slice(f16[30528,768]{1,0} %param_0.370), slice={[0:30522], [0:768]}, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul_1"}
  %convert.41 = f32[30522,768]{1,0} convert(f16[30522,768]{1,0} %slice.8), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/Cast/Cast"}
  %param_2.179 = s32[16,512]{1,0} parameter(2)
  %bitcast.218 = s32[8192]{0} bitcast(s32[16,512]{1,0} %param_2.179), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %param_1.292 = f16[1,16,512,768]{3,2,1,0} parameter(1)
  %bitcast.217 = f16[8192,768]{1,0} bitcast(f16[1,16,512,768]{3,2,1,0} %param_1.292), metadata={op_type="Reshape" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %convert.40 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %bitcast.217), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2/Cast/Cast"}
  ROOT %scatter.0 = f32[30522,768]{1,0} scatter(f32[30522,768]{1,0} %convert.41, s32[8192]{0} %bitcast.218, f32[8192,768]{1,0} %convert.40), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1727, metadata={op_type="UnsortedSegmentSum" op_name="AddN_10/inputs_1"}
}

%add_float_.1019 (x.1020: f32[], y.1021: f32[]) -> f32[] {
  %x.1020 = f32[] parameter(0)
  %y.1021 = f32[] parameter(1)
  ROOT %add.1022 = f32[] add(f32[] %x.1020, f32[] %y.1021)
}

%fused_computation.15 (param_0.609: s32[16,76], param_1.529: f32[1216,30522], param_2.386: f32[1216], param_3.270: f32[16,76], param_4.174: f32[]) -> f32[30522] {
  %param_4.174 = f32[] parameter(4)
  %constant_111 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %compare.63 = pred[] compare(f32[] %param_4.174, f32[] %constant_111), direction=EQ, metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %constant_437 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %divide.22 = f32[] divide(f32[] %constant_437, f32[] %param_4.174), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %select.60 = f32[] select(pred[] %compare.63, f32[] %constant_111, f32[] %divide.22), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %broadcast.553 = f32[16,76]{1,0} broadcast(f32[] %select.60), dimensions={}, metadata={op_type="Tile" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Tile_1"}
  %param_3.270 = f32[16,76]{1,0} parameter(3)
  %convert.309 = s32[16,76]{1,0} convert(f32[16,76]{1,0} %param_3.270), metadata={op_type="Cast" op_name="model/Cast"}
  %convert.307 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %convert.309), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast"}
  %multiply.184 = f32[16,76]{1,0} multiply(f32[16,76]{1,0} %broadcast.553, f32[16,76]{1,0} %convert.307), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/mul/Mul"}
  %bitcast.257 = f32[1216]{0} bitcast(f32[16,76]{1,0} %multiply.184), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %broadcast.552 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %bitcast.257), dimensions={0}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %param_1.529 = f32[1216,30522]{1,0} parameter(1)
  %param_2.386 = f32[1216]{0} parameter(2)
  %broadcast.551 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_2.386), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %divide.21 = f32[1216,30522]{1,0} divide(f32[1216,30522]{1,0} %param_1.529, f32[1216,30522]{1,0} %broadcast.551), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.609 = s32[16,76]{1,0} parameter(0)
  %convert.306 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %param_0.609), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_2"}
  %convert.305 = s64[16,76]{1,0} convert(f32[16,76]{1,0} %convert.306), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %bitcast.256 = s64[1216]{0} bitcast(s64[16,76]{1,0} %convert.305), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %broadcast.550 = s64[1216,30522]{1,0} broadcast(s64[1216]{0} %bitcast.256), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.14 = s64[1216,30522]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.62 = pred[1216,30522]{1,0} compare(s64[1216,30522]{1,0} %broadcast.550, s64[1216,30522]{1,0} %iota.14), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.548 = f32[1216,30522]{1,0} broadcast(f32[] %constant_437), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.547 = f32[1216,30522]{1,0} broadcast(f32[] %constant_111), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.59 = f32[1216,30522]{1,0} select(pred[1216,30522]{1,0} %compare.62, f32[1216,30522]{1,0} %broadcast.548, f32[1216,30522]{1,0} %broadcast.547), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_435 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.545 = s64[1216]{0} broadcast(s64[] %constant_435), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.61 = pred[1216]{0} compare(s64[1216]{0} %broadcast.545, s64[1216]{0} %bitcast.256), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_434 = s64[] constant(30522), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.544 = s64[1216]{0} broadcast(s64[] %constant_434), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.60 = pred[1216]{0} compare(s64[1216]{0} %bitcast.256, s64[1216]{0} %broadcast.544), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %and.11 = pred[1216]{0} and(pred[1216]{0} %compare.61, pred[1216]{0} %compare.60), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.543 = f32[1216]{0} broadcast(f32[] %constant_111), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_433 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.541 = f32[1216]{0} broadcast(f32[] %constant_433), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.58 = f32[1216]{0} select(pred[1216]{0} %and.11, f32[1216]{0} %broadcast.543, f32[1216]{0} %broadcast.541), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.540 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %select.58), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %add.172 = f32[1216,30522]{1,0} add(f32[1216,30522]{1,0} %select.59, f32[1216,30522]{1,0} %broadcast.540), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.41 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %divide.21, f32[1216,30522]{1,0} %add.172), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.183 = f32[1216,30522]{1,0} multiply(f32[1216,30522]{1,0} %broadcast.552, f32[1216,30522]{1,0} %subtract.41), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %convert.304 = f16[1216,30522]{1,0} convert(f32[1216,30522]{1,0} %multiply.183), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %convert.42 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %convert.304), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
  ROOT %reduce.44 = f32[30522]{0} reduce(f32[1216,30522]{1,0} %convert.42, f32[] %constant_111), dimensions={0}, to_apply=%add_float_.1019, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
}

%add_float_.793 (x.794: f32[], y.795: f32[]) -> f32[] {
  %x.794 = f32[] parameter(0)
  %y.795 = f32[] parameter(1)
  ROOT %add.796 = f32[] add(f32[] %x.794, f32[] %y.795)
}

%fused_computation.16 (param_0.372: f16[16,2]) -> f32[2] {
  %param_0.372 = f16[16,2]{1,0} parameter(0)
  %convert.43 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %param_0.372), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/BiasAddGrad"}
  %constant_112 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.45 = f32[2]{0} reduce(f32[16,2]{1,0} %convert.43, f32[] %constant_112), dimensions={0}, to_apply=%add_float_.793, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/BiasAddGrad"}
}

%fused_computation.19 (param_0.36: f16[768,768]) -> f32[12,64,768] {
  %param_0.36 = f16[768,768]{1,0} parameter(0)
  %convert.44 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.36), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.143 = f32[768,12,64]{2,1,0} bitcast(f32[768,768]{1,0} %convert.44), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.116 = f32[12,64,768]{1,0,2} transpose(f32[768,12,64]{2,1,0} %bitcast.143), dimensions={1,2,0}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.20 (param_0.39: f16[768,768]) -> f32[768,12,64] {
  %param_0.39 = f16[768,768]{1,0} parameter(0)
  %convert.45 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.39), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.144 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %convert.45), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.117 = f32[768,12,64]{0,2,1} transpose(f32[12,64,768]{2,1,0} %bitcast.144), dimensions={2,0,1}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.21 (param_0.42: f16[16,12,64,512]) -> f16[768,8192] {
  %param_0.42 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.118 = f16[12,64,16,512]{3,1,0,2} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.42), dimensions={1,2,0,3}
  %copy.66 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{3,1,0,2} %transpose.118)
  ROOT %bitcast.145 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.66)
}

%fused_computation.26 (param_0.53: f16[768,768]) -> f32[12,64,768] {
  %param_0.53 = f16[768,768]{1,0} parameter(0)
  %convert.46 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.53), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.146 = f32[768,12,64]{2,1,0} bitcast(f32[768,768]{1,0} %convert.46), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.119 = f32[12,64,768]{1,0,2} transpose(f32[768,12,64]{2,1,0} %bitcast.146), dimensions={1,2,0}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.27 (param_0.56: f16[768,768]) -> f32[768,12,64] {
  %param_0.56 = f16[768,768]{1,0} parameter(0)
  %convert.47 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.56), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.147 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %convert.47), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.120 = f32[768,12,64]{0,2,1} transpose(f32[12,64,768]{2,1,0} %bitcast.147), dimensions={2,0,1}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.28 (param_0.59: f16[16,12,64,512]) -> f16[768,8192] {
  %param_0.59 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.121 = f16[12,64,16,512]{3,1,0,2} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.59), dimensions={1,2,0,3}
  %copy.67 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{3,1,0,2} %transpose.121)
  ROOT %bitcast.148 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.67)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_position_embedding_Sum-reduction.1695 (x.1696: f32[], y.1697: f32[]) -> f32[] {
  %x.1696 = f32[] parameter(0)
  %y.1697 = f32[] parameter(1)
  ROOT %add.1698 = f32[] add(f32[] %x.1696, f32[] %y.1697)
}

%fused_computation.31 (param_0.377: f16[1,16,512,768]) -> f32[512,768] {
  %param_0.377 = f16[1,16,512,768]{3,2,1,0} parameter(0)
  %convert.50 = f32[1,16,512,768]{3,2,1,0} convert(f16[1,16,512,768]{3,2,1,0} %param_0.377), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Sum"}
  %bitcast.149 = f32[16,512,768]{2,1,0} bitcast(f32[1,16,512,768]{3,2,1,0} %convert.50)
  %constant_137 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.46 = f32[512,768]{1,0} reduce(f32[16,512,768]{2,1,0} %bitcast.149, f32[] %constant_137), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_position_embedding_Sum-reduction.1695, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Sum"}
  %convert.49 = f16[512,768]{1,0} convert(f32[512,768]{1,0} %reduce.46), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Sum"}
  ROOT %convert.48 = f32[512,768]{1,0} convert(f16[512,768]{1,0} %convert.49), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.32 (param_0.69: f32[1,768,16,512]) -> f16[1,16,512,768] {
  %param_0.69 = f32[1,768,16,512]{1,3,2,0} parameter(0)
  %convert.51 = f16[1,768,16,512]{1,3,2,0} convert(f32[1,768,16,512]{1,3,2,0} %param_0.69), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast/Cast"}
  ROOT %transpose.122 = f16[1,16,512,768]{3,2,1,0} transpose(f16[1,768,16,512]{1,3,2,0} %convert.51), dimensions={0,2,3,1}, metadata={op_type="Transpose" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast/Cast-0-0-TransposeNCHWToNHWC-LayoutOptimizer"}
}

%fused_computation.33 (param_0.487: f16[8192,768], param_1.379: f16[8192,768], param_2.279: f16[16,512,768], param_3.201: f16[8192,768], param_4.115: f16[16,512,768]) -> f32[1,768,16,512] {
  %param_4.115 = f16[16,512,768]{2,1,0} parameter(4)
  %constant_289 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.334 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_289), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.33 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_4.115, f16[16,512,768]{2,1,0} %broadcast.334), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %param_2.279 = f16[16,512,768]{2,1,0} parameter(2)
  %param_3.201 = f16[8192,768]{1,0} parameter(3)
  %bitcast.153 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_3.201), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum"}
  %add.56 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %param_2.279, f16[16,512,768]{2,1,0} %bitcast.153), metadata={op_type="AddN" op_name="AddN_9"}
  %param_1.379 = f16[8192,768]{1,0} parameter(1)
  %bitcast.152 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_1.379), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum"}
  %add.55 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.56, f16[16,512,768]{2,1,0} %bitcast.152), metadata={op_type="AddN" op_name="AddN_9"}
  %param_0.487 = f16[8192,768]{1,0} parameter(0)
  %bitcast.151 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_0.487), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum"}
  %add.54 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.55, f16[16,512,768]{2,1,0} %bitcast.151), metadata={op_type="AddN" op_name="AddN_9"}
  %constant_138 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.205 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_138), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %multiply.57 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %add.54, f16[16,512,768]{2,1,0} %broadcast.205), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %constant_139 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.204 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_139), dimensions={}
  %select.23 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.33, f16[16,512,768]{2,1,0} %multiply.57, f16[16,512,768]{2,1,0} %broadcast.204), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul_1"}
  %convert.52 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %select.23), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/ArithmeticOptimizer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.150 = f32[1,16,512,768]{3,2,1,0} bitcast(f32[16,512,768]{2,1,0} %convert.52), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/ArithmeticOptimizer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.123 = f32[1,768,16,512]{1,3,2,0} transpose(f32[1,16,512,768]{3,2,1,0} %bitcast.150), dimensions={0,3,1,2}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/ArithmeticOptimizer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.34 (param_0.382: f32[768,12,64]) -> f16[768,768] {
  %param_0.382 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.136 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.382), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast"}
  %transpose.124 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.136), dimensions={1,2,0}
  ROOT %bitcast.154 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.124)
}

%fused_computation.35 (param_0.383: f16[16,12,64,512]) -> f16[8192,768] {
  %param_0.383 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.159 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.383), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}
  %copy.68 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{1,3,2,0} %transpose.159), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}
  ROOT %bitcast.155 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.68)
}

%fused_computation.36 (param_0.380: f32[768,12,64]) -> f16[768,768] {
  %param_0.380 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.135 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.380), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast"}
  %transpose.125 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.135), dimensions={1,2,0}
  ROOT %bitcast.156 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.125)
}

%fused_computation.37 (param_0.381: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.381 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.158 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.381), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %copy.69 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.158), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  ROOT %bitcast.157 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.69)
}

%fused_computation.38 (param_0.378: f32[768,12,64]) -> f16[768,768] {
  %param_0.378 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.134 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.378), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast"}
  %transpose.126 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.134), dimensions={1,2,0}
  ROOT %bitcast.158 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.126)
}

%fused_computation.39 (param_0.379: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.379 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.157 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.379), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  %copy.70 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.157), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  ROOT %bitcast.159 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.70)
}

%fused_computation.40 (param_0.87: f32[12,64]) -> f32[12,64] {
  %param_0.87 = f32[12,64]{1,0} parameter(0)
  %convert.54 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_0.87), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  ROOT %convert.53 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.54), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.42 (param_0.385: f16[8192,768]) -> f16[16,12,64,512] {
  %param_0.385 = f16[8192,768]{1,0} parameter(0)
  %reshape.237 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.385), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum"}
  %transpose.127 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.237), dimensions={0,2,3,1}
  ROOT %copy.71 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.127)
}

%fused_computation.43 (param_0.94: f16[768,768]) -> f32[768,12,64] {
  %param_0.94 = f16[768,768]{1,0} parameter(0)
  %convert.56 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.94), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.161 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %convert.56), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.128 = f32[768,12,64]{0,2,1} transpose(f32[12,64,768]{2,1,0} %bitcast.161), dimensions={2,0,1}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.44 (param_0.97: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.97 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.129 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.97), dimensions={1,3,0,2}
  %copy.72 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.129)
  ROOT %bitcast.162 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.72)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_query_add_Sum-reduction.1612 (x.1613: f32[], y.1614: f32[]) -> f32[] {
  %x.1613 = f32[] parameter(0)
  %y.1614 = f32[] parameter(1)
  ROOT %add.1615 = f32[] add(f32[] %x.1613, f32[] %y.1614)
}

%fused_computation.45 (param_0.100: f32[16,12,64]) -> f32[12,64] {
  %param_0.100 = f32[16,12,64]{2,1,0} parameter(0)
  %constant_140 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.47 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_0.100, f32[] %constant_140), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_query_add_Sum-reduction.1612, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %convert.58 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.47), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  ROOT %convert.57 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.58), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.47 (param_0.105: f16[768,768]) -> f32[768,12,64] {
  %param_0.105 = f16[768,768]{1,0} parameter(0)
  %convert.60 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.105), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.164 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %convert.60), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.130 = f32[768,12,64]{0,2,1} transpose(f32[12,64,768]{2,1,0} %bitcast.164), dimensions={2,0,1}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.48 (param_0.108: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.108 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.131 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.108), dimensions={1,3,0,2}
  %copy.73 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.131)
  ROOT %bitcast.165 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.73)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_key_add_Sum-reduction.1582 (x.1583: f32[], y.1584: f32[]) -> f32[] {
  %x.1583 = f32[] parameter(0)
  %y.1584 = f32[] parameter(1)
  ROOT %add.1585 = f32[] add(f32[] %x.1583, f32[] %y.1584)
}

%fused_computation.49 (param_0.111: f32[16,12,64]) -> f32[12,64] {
  %param_0.111 = f32[16,12,64]{2,1,0} parameter(0)
  %constant_141 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.48 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_0.111, f32[] %constant_141), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_key_add_Sum-reduction.1582, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %convert.62 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.48), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  ROOT %convert.61 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.62), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.51 (param_0.500: f16[8192,768], param_1.396: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.396 = f32[12,64]{1,0} parameter(1)
  %convert.174 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.396), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast"}
  %broadcast.344 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.174), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %param_0.500 = f16[8192,768]{1,0} parameter(0)
  %reshape.244 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.500), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum"}
  %add.110 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.344, f16[16,512,12,64]{1,3,2,0} %reshape.244), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %constant_301 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.343 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_301), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %multiply.158 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %add.110, f16[16,512,12,64]{1,3,2,0} %broadcast.343), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %transpose.132 = f16[16,12,512,64]{2,3,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.158), dimensions={0,2,1,3}
  ROOT %copy.74 = f16[16,12,512,64]{3,2,1,0} copy(f16[16,12,512,64]{2,3,1,0} %transpose.132)
}

%fused_computation.52 (param_0.637: f32[16,12,512], param_1.562: f16[16,12,512,512], param_2.410: f32[16,12,512], param_3.309: f16[16,12,512,512], param_4.218: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_4.218 = f16[16,12,512,512]{3,2,1,0} parameter(4)
  %copy.117 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_4.218), metadata={op_name="XLA_Args"}
  %constant_502 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.618 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_502), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.79 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.117, f16[16,12,512,512]{2,3,1,0} %broadcast.618), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_3.309 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.116 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.309), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  %constant_501 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.617 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_501), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.209 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.116, f16[16,12,512,512]{2,3,1,0} %broadcast.617), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_500 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.616 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_500), dimensions={}
  %select.74 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.79, f16[16,12,512,512]{2,3,1,0} %multiply.209, f16[16,12,512,512]{2,3,1,0} %broadcast.616), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.637 = f32[16,12,512]{2,1,0} parameter(0)
  %convert.64 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_0.637), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  %broadcast.116 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.64), dimensions={0,1,2}, metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/sub"}
  %subtract.8 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %select.74, f16[16,12,512,512]{2,3,1,0} %broadcast.116), metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/sub"}
  %param_1.562 = f16[16,12,512,512]{2,3,1,0} parameter(1)
  %param_2.410 = f32[16,12,512]{2,1,0} parameter(2)
  %convert.180 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_2.410), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %broadcast.350 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.180), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %divide.12 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_1.562, f16[16,12,512,512]{2,3,1,0} %broadcast.350), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %multiply.58 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %subtract.8, f16[16,12,512,512]{2,3,1,0} %divide.12), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul_1"}
  ROOT %copy.75 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.58), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul_1"}
}

%fused_computation.53 (param_0.635: f16[16,12,512,512], param_1.560: f32[16,12,512], param_2.409: f16[16,12,512,512], param_3.307: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_3.307 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.113 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.307), metadata={op_name="XLA_Args"}
  %constant_494 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.611 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_494), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.77 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.113, f16[16,12,512,512]{2,3,1,0} %broadcast.611), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_2.409 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.112 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.409), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  %constant_493 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.610 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_493), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.207 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.112, f16[16,12,512,512]{2,3,1,0} %broadcast.610), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_491 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.608 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_491), dimensions={}
  %select.72 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.77, f16[16,12,512,512]{2,3,1,0} %multiply.207, f16[16,12,512,512]{2,3,1,0} %broadcast.608), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.635 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.560 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.178 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.560), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %broadcast.348 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.178), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %divide.10 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.635, f16[16,12,512,512]{2,3,1,0} %broadcast.348), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %multiply.59 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.72, f16[16,12,512,512]{2,3,1,0} %divide.10), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul"}
  %convert.65 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %multiply.59), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  ROOT %bitcast.167 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.65), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
}

%fused_computation.55 (param_0.516: f16[8192,768], param_1.415: f32[12,64]) -> f16[16,12,64,512] {
  %param_1.415 = f32[12,64]{1,0} parameter(1)
  %convert.184 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.415), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast"}
  %broadcast.361 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.184), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  %param_0.516 = f16[8192,768]{1,0} parameter(0)
  %reshape.248 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.516), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum"}
  %add.114 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.361, f16[16,512,12,64]{3,1,2,0} %reshape.248), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  %transpose.133 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.114), dimensions={0,2,3,1}
  ROOT %copy.77 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.133)
}

%fused_computation.56 (param_0.390: f32[12,64,768]) -> f16[768,768] {
  %param_0.390 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.137 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.390), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast"}
  %transpose.134 = f16[768,12,64]{0,2,1} transpose(f16[12,64,768]{2,1,0} %convert.137), dimensions={2,0,1}
  ROOT %bitcast.168 = f16[768,768]{0,1} bitcast(f16[768,12,64]{0,2,1} %transpose.134)
}

%fused_computation.57 (param_0.128: f16[8192,768]) -> f32[16,512,768] {
  %param_0.128 = f16[8192,768]{1,0} parameter(0)
  %convert.66 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_0.128), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_1/Cast"}
  ROOT %bitcast.169 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.66), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_1/Cast"}
}

%fused_computation.58 (param_0.130: f16[16,512,3072]) -> f16[3072,8192] {
  %param_0.130 = f16[16,512,3072]{2,1,0} parameter(0)
  %transpose.135 = f16[3072,16,512]{0,2,1} transpose(f16[16,512,3072]{2,1,0} %param_0.130), dimensions={2,0,1}
  ROOT %bitcast.170 = f16[3072,8192]{0,1} bitcast(f16[3072,16,512]{0,2,1} %transpose.135)
}

%fused_computation.59 (param_0.132: f32[3072]) -> f32[3072] {
  %param_0.132 = f32[3072]{0} parameter(0)
  %convert.68 = f16[3072]{0} convert(f32[3072]{0} %param_0.132), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  ROOT %convert.67 = f32[3072]{0} convert(f16[3072]{0} %convert.68), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_intermediate_add_Sum-reduction.1472 (x.1473: f32[], y.1474: f32[]) -> f32[] {
  %x.1473 = f32[] parameter(0)
  %y.1474 = f32[] parameter(1)
  ROOT %add.1475 = f32[] add(f32[] %x.1473, f32[] %y.1474)
}

%fused_computation.60 (param_0.391: f16[16,512,3072]) -> f32[3072] {
  %param_0.391 = f16[16,512,3072]{2,1,0} parameter(0)
  %convert.69 = f32[16,512,3072]{2,1,0} convert(f16[16,512,3072]{2,1,0} %param_0.391), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  %bitcast.171 = f32[8192,3072]{1,0} bitcast(f32[16,512,3072]{2,1,0} %convert.69)
  %constant_144 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.49 = f32[3072]{0} reduce(f32[8192,3072]{1,0} %bitcast.171, f32[] %constant_144), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_intermediate_add_Sum-reduction.1472, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
}

%fused_computation.61 (param_0.395: f16[16,512,3072], param_1.314: f16[8192,3072], param_2.295: f16[16,512,3072], param_3.211: f16[8192,3072], param_4.119: f32[3072]) -> f16[16,512,3072] {
  %param_4.119 = f32[3072]{0} parameter(4)
  %convert.186 = f16[3072]{0} convert(f32[3072]{0} %param_4.119), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast"}
  %broadcast.363 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.186), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %param_3.211 = f16[8192,3072]{1,0} parameter(3)
  %bitcast.235 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_3.211), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum"}
  %add.116 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.363, f16[16,512,3072]{2,1,0} %bitcast.235), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %multiply.136 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.116, f16[16,512,3072]{2,1,0} %add.116)
  %param_2.295 = f16[16,512,3072]{2,1,0} parameter(2)
  %constant_149 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.212 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_149), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.15 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_2.295, f16[16,512,3072]{2,1,0} %broadcast.212), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %param_1.314 = f16[8192,3072]{1,0} parameter(1)
  %bitcast.172 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_1.314), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum"}
  %select.25 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.15, f16[16,512,3072]{2,1,0} %bitcast.172, f16[16,512,3072]{2,1,0} %broadcast.212), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/Mul"}
  %constant_146 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.209 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_146), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.135 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.116, f16[16,512,3072]{2,1,0} %broadcast.209), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.68 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.25, f16[16,512,3072]{2,1,0} %multiply.135), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_3/Mul_1"}
  %constant_148 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.211 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_148), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.395 = f16[16,512,3072]{2,1,0} parameter(0)
  %multiply.67 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %param_0.395, f16[16,512,3072]{2,1,0} %param_0.395), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/TanhGrad"}
  %subtract.9 = f16[16,512,3072]{2,1,0} subtract(f16[16,512,3072]{2,1,0} %broadcast.211, f16[16,512,3072]{2,1,0} %multiply.67), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/TanhGrad"}
  %multiply.66 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.68, f16[16,512,3072]{2,1,0} %subtract.9), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/TanhGrad"}
  %constant_147 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.210 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_147), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.65 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.66, f16[16,512,3072]{2,1,0} %broadcast.210), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2/Mul"}
  %multiply.64 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.136, f16[16,512,3072]{2,1,0} %multiply.65), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Pow/mul"}
  %constant_145 = f16[] constant(0.13416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %broadcast.208 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_145), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul_1"}
  %multiply.63 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.64, f16[16,512,3072]{2,1,0} %broadcast.208), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Pow/mul_1"}
  %multiply.62 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.25, f16[16,512,3072]{2,1,0} %broadcast.209), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_3/Mul"}
  %add.93 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.211, f16[16,512,3072]{2,1,0} %param_0.395), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/add_1"}
  %multiply.61 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.62, f16[16,512,3072]{2,1,0} %add.93), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul/Mul"}
  %add.58 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.63, f16[16,512,3072]{2,1,0} %multiply.61), metadata={op_type="AddN" op_name="AddN_7"}
  ROOT %add.57 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %add.58, f16[16,512,3072]{2,1,0} %multiply.65), metadata={op_type="AddN" op_name="AddN_7"}
}

%fused_computation.62 (param_0.396: f32[768,12,64]) -> f16[768,768] {
  %param_0.396 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.138 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.396), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast"}
  %transpose.136 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.138), dimensions={1,2,0}
  ROOT %bitcast.173 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.136)
}

%fused_computation.63 (param_0.397: f16[16,12,64,512]) -> f16[8192,768] {
  %param_0.397 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.164 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.397), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}
  %copy.78 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{1,3,2,0} %transpose.164), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}
  ROOT %bitcast.174 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.78)
}

%fused_computation.64 (param_0.398: f32[768,12,64]) -> f16[768,768] {
  %param_0.398 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.139 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.398), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast"}
  %transpose.137 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.139), dimensions={1,2,0}
  ROOT %bitcast.175 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.137)
}

%fused_computation.65 (param_0.399: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.399 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.165 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.399), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  %copy.79 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.165), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  ROOT %bitcast.176 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.79)
}

%fused_computation.66 (param_0.400: f32[768,12,64]) -> f16[768,768] {
  %param_0.400 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.140 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.400), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast"}
  %transpose.138 = f16[12,64,768]{1,0,2} transpose(f16[768,12,64]{2,1,0} %convert.140), dimensions={1,2,0}
  ROOT %bitcast.177 = f16[768,768]{0,1} bitcast(f16[12,64,768]{1,0,2} %transpose.138)
}

%fused_computation.67 (param_0.401: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.401 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.166 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.401), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  %copy.80 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.166), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  ROOT %bitcast.178 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.80)
}

%fused_computation.68 (param_0.151: f32[12,64]) -> f32[12,64] {
  %param_0.151 = f32[12,64]{1,0} parameter(0)
  %convert.71 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_0.151), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  ROOT %convert.70 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.71), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.70 (param_0.403: f16[8192,768]) -> f16[16,12,64,512] {
  %param_0.403 = f16[8192,768]{1,0} parameter(0)
  %reshape.239 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.403), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum"}
  %transpose.139 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.239), dimensions={0,2,3,1}
  ROOT %copy.81 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.139)
}

%fused_computation.71 (param_0.158: f16[768,768]) -> f32[768,12,64] {
  %param_0.158 = f16[768,768]{1,0} parameter(0)
  %convert.73 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.158), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.180 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %convert.73), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.140 = f32[768,12,64]{0,2,1} transpose(f32[12,64,768]{2,1,0} %bitcast.180), dimensions={2,0,1}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.72 (param_0.161: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.161 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.141 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.161), dimensions={1,3,0,2}
  %copy.82 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.141)
  ROOT %bitcast.181 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.82)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_query_add_Sum-reduction.1352 (x.1353: f32[], y.1354: f32[]) -> f32[] {
  %x.1353 = f32[] parameter(0)
  %y.1354 = f32[] parameter(1)
  ROOT %add.1355 = f32[] add(f32[] %x.1353, f32[] %y.1354)
}

%fused_computation.73 (param_0.164: f32[16,12,64]) -> f32[12,64] {
  %param_0.164 = f32[16,12,64]{2,1,0} parameter(0)
  %constant_150 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.50 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_0.164, f32[] %constant_150), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_query_add_Sum-reduction.1352, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %convert.75 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.50), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  ROOT %convert.74 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.75), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.75 (param_0.169: f16[768,768]) -> f32[768,12,64] {
  %param_0.169 = f16[768,768]{1,0} parameter(0)
  %convert.77 = f32[768,768]{1,0} convert(f16[768,768]{1,0} %param_0.169), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %bitcast.183 = f32[12,64,768]{2,1,0} bitcast(f32[768,768]{1,0} %convert.77), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  ROOT %transpose.142 = f32[768,12,64]{0,2,1} transpose(f32[12,64,768]{2,1,0} %bitcast.183), dimensions={2,0,1}, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.76 (param_0.172: f16[16,12,512,64]) -> f16[768,8192] {
  %param_0.172 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.143 = f16[12,64,16,512]{1,3,0,2} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.172), dimensions={1,3,0,2}
  %copy.83 = f16[12,64,16,512]{3,2,1,0} copy(f16[12,64,16,512]{1,3,0,2} %transpose.143)
  ROOT %bitcast.184 = f16[768,8192]{1,0} bitcast(f16[12,64,16,512]{3,2,1,0} %copy.83)
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_key_add_Sum-reduction.1322 (x.1323: f32[], y.1324: f32[]) -> f32[] {
  %x.1323 = f32[] parameter(0)
  %y.1324 = f32[] parameter(1)
  ROOT %add.1325 = f32[] add(f32[] %x.1323, f32[] %y.1324)
}

%fused_computation.77 (param_0.175: f32[16,12,64]) -> f32[12,64] {
  %param_0.175 = f32[16,12,64]{2,1,0} parameter(0)
  %constant_151 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.51 = f32[12,64]{1,0} reduce(f32[16,12,64]{2,1,0} %param_0.175, f32[] %constant_151), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_key_add_Sum-reduction.1322, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %convert.79 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %reduce.51), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  ROOT %convert.78 = f32[12,64]{1,0} convert(f16[12,64]{1,0} %convert.79), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%fused_computation.79 (param_0.529: f16[8192,768], param_1.433: f32[12,64]) -> f16[16,12,512,64] {
  %constant_327 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.376 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_327), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %param_1.433 = f32[12,64]{1,0} parameter(1)
  %convert.195 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.433), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast"}
  %broadcast.375 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.195), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %param_0.529 = f16[8192,768]{1,0} parameter(0)
  %reshape.255 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.529), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum"}
  %add.124 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.375, f16[16,512,12,64]{1,3,2,0} %reshape.255), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %multiply.162 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %broadcast.376, f16[16,512,12,64]{1,3,2,0} %add.124), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  %transpose.144 = f16[16,12,512,64]{2,3,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.162), dimensions={0,2,1,3}
  ROOT %copy.84 = f16[16,12,512,64]{3,2,1,0} copy(f16[16,12,512,64]{2,3,1,0} %transpose.144)
}

%fused_computation.80 (param_0.624: f32[16,12,512], param_1.546: f16[16,12,512,512], param_2.407: f32[16,12,512], param_3.302: f16[16,12,512,512], param_4.207: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_4.207 = f16[16,12,512,512]{3,2,1,0} parameter(4)
  %copy.109 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_4.207), metadata={op_name="XLA_Args"}
  %constant_480 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.601 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_480), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.75 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.109, f16[16,12,512,512]{2,3,1,0} %broadcast.601), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_3.302 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.108 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.302), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  %constant_479 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.600 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_479), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.205 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.108, f16[16,12,512,512]{2,3,1,0} %broadcast.600), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul"}
  %constant_478 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.599 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_478), dimensions={}
  %select.70 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.75, f16[16,12,512,512]{2,3,1,0} %multiply.205, f16[16,12,512,512]{2,3,1,0} %broadcast.599), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.624 = f32[16,12,512]{2,1,0} parameter(0)
  %convert.81 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_0.624), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  %broadcast.117 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.81), dimensions={0,1,2}, metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %subtract.10 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %select.70, f16[16,12,512,512]{2,3,1,0} %broadcast.117), metadata={op_type="Sub" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %param_1.546 = f16[16,12,512,512]{2,3,1,0} parameter(1)
  %param_2.407 = f32[16,12,512]{2,1,0} parameter(2)
  %convert.202 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_2.407), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %broadcast.386 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.202), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %divide.18 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_1.546, f16[16,12,512,512]{2,3,1,0} %broadcast.386), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %multiply.69 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %subtract.10, f16[16,12,512,512]{2,3,1,0} %divide.18), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul_1"}
  ROOT %copy.85 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.69), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul_1"}
}

%fused_computation.81 (param_0.622: f16[16,12,512,512], param_1.544: f32[16,12,512], param_2.406: f16[16,12,512,512], param_3.300: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_3.300 = f16[16,12,512,512]{3,2,1,0} parameter(3)
  %copy.105 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_3.300), metadata={op_name="XLA_Args"}
  %constant_473 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.594 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_473), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.73 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.105, f16[16,12,512,512]{2,3,1,0} %broadcast.594), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/GreaterEqual"}
  %param_2.406 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.104 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.406), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  %constant_472 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.593 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_472), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %multiply.203 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %copy.104, f16[16,12,512,512]{2,3,1,0} %broadcast.593), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul"}
  %constant_471 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.592 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_471), dimensions={}
  %select.68 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.73, f16[16,12,512,512]{2,3,1,0} %multiply.203, f16[16,12,512,512]{2,3,1,0} %broadcast.592), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  %param_0.622 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.544 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.200 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.544), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %broadcast.383 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.200), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %divide.16 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.622, f16[16,12,512,512]{2,3,1,0} %broadcast.383), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %multiply.70 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.68, f16[16,12,512,512]{2,3,1,0} %divide.16), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %convert.82 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %multiply.70), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  ROOT %bitcast.186 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.82), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
}

%fused_computation.83 (param_0.545: f16[8192,768], param_1.452: f32[12,64]) -> f16[16,12,64,512] {
  %param_1.452 = f32[12,64]{1,0} parameter(1)
  %convert.212 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.452), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast"}
  %broadcast.395 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.212), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  %param_0.545 = f16[8192,768]{1,0} parameter(0)
  %reshape.259 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.545), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum"}
  %add.128 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.395, f16[16,512,12,64]{3,1,2,0} %reshape.259), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  %transpose.145 = f16[16,12,64,512]{2,3,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.128), dimensions={0,2,3,1}
  ROOT %copy.87 = f16[16,12,64,512]{3,2,1,0} copy(f16[16,12,64,512]{2,3,1,0} %transpose.145)
}

%fused_computation.84 (param_0.408: f32[12,64,768]) -> f16[768,768] {
  %param_0.408 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.141 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.408), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast"}
  %transpose.146 = f16[768,12,64]{0,2,1} transpose(f16[12,64,768]{2,1,0} %convert.141), dimensions={2,0,1}
  ROOT %bitcast.187 = f16[768,768]{0,1} bitcast(f16[768,12,64]{0,2,1} %transpose.146)
}

%fused_computation.85 (param_0.192: f16[8192,768]) -> f32[16,512,768] {
  %param_0.192 = f16[8192,768]{1,0} parameter(0)
  %convert.83 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %param_0.192), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_1/Cast"}
  ROOT %bitcast.188 = f32[16,512,768]{2,1,0} bitcast(f32[8192,768]{1,0} %convert.83), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_1/Cast"}
}

%fused_computation.88 (param_0.198: f16[16,512,3072]) -> f16[3072,8192] {
  %param_0.198 = f16[16,512,3072]{2,1,0} parameter(0)
  %transpose.147 = f16[3072,16,512]{0,2,1} transpose(f16[16,512,3072]{2,1,0} %param_0.198), dimensions={2,0,1}
  ROOT %bitcast.189 = f16[3072,8192]{0,1} bitcast(f16[3072,16,512]{0,2,1} %transpose.147)
}

%fused_computation.89 (param_0.200: f32[3072]) -> f32[3072] {
  %param_0.200 = f32[3072]{0} parameter(0)
  %convert.85 = f16[3072]{0} convert(f32[3072]{0} %param_0.200), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
  ROOT %convert.84 = f32[3072]{0} convert(f16[3072]{0} %convert.85), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_intermediate_add_Sum-reduction.1212 (x.1213: f32[], y.1214: f32[]) -> f32[] {
  %x.1213 = f32[] parameter(0)
  %y.1214 = f32[] parameter(1)
  ROOT %add.1215 = f32[] add(f32[] %x.1213, f32[] %y.1214)
}

%fused_computation.90 (param_0.410: f16[16,512,3072]) -> f32[3072] {
  %param_0.410 = f16[16,512,3072]{2,1,0} parameter(0)
  %convert.86 = f32[16,512,3072]{2,1,0} convert(f16[16,512,3072]{2,1,0} %param_0.410), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
  %bitcast.190 = f32[8192,3072]{1,0} bitcast(f32[16,512,3072]{2,1,0} %convert.86)
  %constant_160 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.52 = f32[3072]{0} reduce(f32[8192,3072]{1,0} %bitcast.190, f32[] %constant_160), dimensions={0}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_intermediate_add_Sum-reduction.1212, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
}

%fused_computation.91 (param_0.414: f16[16,512,3072], param_1.326: f16[8192,3072], param_2.319: f16[16,512,3072], param_3.222: f16[8192,3072], param_4.123: f32[3072]) -> f16[16,512,3072] {
  %constant_161 = f16[] constant(0.13416), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/mul_1"}
  %broadcast.222 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_161), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul_1"}
  %param_4.123 = f32[3072]{0} parameter(4)
  %convert.214 = f16[3072]{0} convert(f32[3072]{0} %param_4.123), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast"}
  %broadcast.397 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.214), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %param_3.222 = f16[8192,3072]{1,0} parameter(3)
  %bitcast.241 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_3.222), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum"}
  %add.130 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.397, f16[16,512,3072]{2,1,0} %bitcast.241), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %multiply.138 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.130, f16[16,512,3072]{2,1,0} %add.130)
  %param_2.319 = f16[16,512,3072]{2,1,0} parameter(2)
  %constant_165 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.228 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_165), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.16 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_2.319, f16[16,512,3072]{2,1,0} %broadcast.228), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %param_1.326 = f16[8192,3072]{1,0} parameter(1)
  %bitcast.191 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_1.326), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum"}
  %select.27 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.16, f16[16,512,3072]{2,1,0} %bitcast.191, f16[16,512,3072]{2,1,0} %broadcast.228), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/Mul"}
  %constant_162 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.223 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_162), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.137 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.130, f16[16,512,3072]{2,1,0} %broadcast.223), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul"}
  %multiply.83 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.27, f16[16,512,3072]{2,1,0} %multiply.137), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_3/Mul_1"}
  %constant_164 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.227 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_164), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.414 = f16[16,512,3072]{2,1,0} parameter(0)
  %multiply.82 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %param_0.414, f16[16,512,3072]{2,1,0} %param_0.414), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/TanhGrad"}
  %subtract.11 = f16[16,512,3072]{2,1,0} subtract(f16[16,512,3072]{2,1,0} %broadcast.227, f16[16,512,3072]{2,1,0} %multiply.82), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/TanhGrad"}
  %multiply.81 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.83, f16[16,512,3072]{2,1,0} %subtract.11), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/TanhGrad"}
  %constant_163 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.225 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_163), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.80 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.81, f16[16,512,3072]{2,1,0} %broadcast.225), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_2/Mul"}
  %multiply.79 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.138, f16[16,512,3072]{2,1,0} %multiply.80), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul"}
  %multiply.78 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %broadcast.222, f16[16,512,3072]{2,1,0} %multiply.79), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow/mul_1"}
  %multiply.77 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %select.27, f16[16,512,3072]{2,1,0} %broadcast.223), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_3/Mul"}
  %add.94 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.227, f16[16,512,3072]{2,1,0} %param_0.414), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %multiply.76 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.77, f16[16,512,3072]{2,1,0} %add.94), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul/Mul"}
  %add.63 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.78, f16[16,512,3072]{2,1,0} %multiply.76), metadata={op_type="AddN" op_name="AddN_4"}
  ROOT %add.62 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %add.63, f16[16,512,3072]{2,1,0} %multiply.80), metadata={op_type="AddN" op_name="AddN_4"}
}

%fused_computation.92 (param_0.415: f16[16,768]) -> f32[16,512,768] {
  %param_0.415 = f16[16,768]{1,0} parameter(0)
  %convert.87 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_0.415), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_3/Cast"}
  %bitcast.192 = f32[16,1,768]{2,1,0} bitcast(f32[16,768]{1,0} %convert.87), metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %constant_166 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %pad.16 = f32[16,512,768]{2,1,0} pad(f32[16,1,768]{2,1,0} %bitcast.192, f32[] %constant_166), padding=0_0x0_511x0_0, metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
}

%scatter-combiner.1131 (p0.1132: f16[], p1.1133: f16[]) -> f16[] {
  %p0.1132 = f16[] parameter(0)
  %p1.1133 = f16[] parameter(1)
  ROOT %add.1134 = f16[] add(f16[] %p0.1132, f16[] %p1.1133)
}

%fused_computation.93 (param_0.568: f16[1216,768], param_1.486: s32[16,76], param_2.341: s32[16]) -> f16[8192,768] {
  %constant_167 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.118 = f16[8192,768]{1,0} broadcast(f16[] %constant_167), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/UnsortedSegmentSum"}
  %param_1.486 = s32[16,76]{1,0} parameter(1)
  %param_2.341 = s32[16]{0} parameter(2)
  %broadcast.424 = s32[16,76]{1,0} broadcast(s32[16]{0} %param_2.341), dimensions={0}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %add.148 = s32[16,76]{1,0} add(s32[16,76]{1,0} %param_1.486, s32[16,76]{1,0} %broadcast.424), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %bitcast.249 = s32[1216]{0} bitcast(s32[16,76]{1,0} %add.148), metadata={op_type="Reshape" op_name="model/bert_pretrainer/cls/predictions/Reshape_1"}
  %param_0.568 = f16[1216,768]{1,0} parameter(0)
  ROOT %scatter.1 = f16[8192,768]{1,0} scatter(f16[8192,768]{1,0} %broadcast.118, s32[1216]{0} %bitcast.249, f16[1216,768]{1,0} %param_0.568), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1131, metadata={op_type="UnsortedSegmentSum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/UnsortedSegmentSum"}
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_2_Sum-reduction.1041 (x.1042: f32[], y.1043: f32[]) -> f32[] {
  %x.1042 = f32[] parameter(0)
  %y.1043 = f32[] parameter(1)
  ROOT %add.1044 = f32[] add(f32[] %x.1042, f32[] %y.1043)
}

%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum-reduction.1063 (x.1064: f32[], y.1065: f32[]) -> f32[] {
  %x.1064 = f32[] parameter(0)
  %y.1065 = f32[] parameter(1)
  ROOT %add.1066 = f32[] add(f32[] %x.1064, f32[] %y.1065)
}

%fused_computation.95 (param_0.614: f32[768], param_1.535: f32[1216], param_2.393: f16[1216,768], param_3.329: f32[1216], param_4.241: f16[1216,768]) -> (f32[1216], f32[1216]) {
  %param_2.393 = f16[1216,768]{1,0} parameter(2)
  %convert.328 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_2.393), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1/Cast"}
  %negate.14 = f32[1216,768]{1,0} negate(f32[1216,768]{1,0} %convert.328), metadata={op_type="Neg" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub/Neg"}
  %constant_414 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %broadcast.508 = f32[1216]{0} broadcast(f32[] %constant_414), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %param_1.535 = f32[1216]{0} parameter(1)
  %constant_413 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.507 = f32[1216]{0} broadcast(f32[] %constant_413), dimensions={}
  %multiply.180 = f32[1216]{0} multiply(f32[1216]{0} %param_1.535, f32[1216]{0} %broadcast.507), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %add.160 = f32[1216]{0} add(f32[1216]{0} %broadcast.508, f32[1216]{0} %multiply.180), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %rsqrt.14 = f32[1216]{0} rsqrt(f32[1216]{0} %add.160), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %broadcast.238 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %rsqrt.14), dimensions={0}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %param_0.614 = f32[768]{0} parameter(0)
  %broadcast.237 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_0.614), dimensions={1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.142 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.238, f32[1216,768]{1,0} %broadcast.237), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.101 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %negate.14, f32[1216,768]{1,0} %multiply.142), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Mul"}
  %constant_174 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.53 = f32[1216]{0} reduce(f32[1216,768]{1,0} %multiply.101, f32[] %constant_174), dimensions={1}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_2_Sum-reduction.1041, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Sum"}
  %param_4.241 = f16[1216,768]{1,0} parameter(4)
  %convert.348.clone.1 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_4.241), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %multiply.194.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.328, f32[1216,768]{1,0} %convert.348.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1/Mul_1"}
  %param_3.329 = f32[1216]{0} parameter(3)
  %multiply.193.clone.1 = f32[1216]{0} multiply(f32[1216]{0} %param_3.329, f32[1216]{0} %broadcast.507), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.582.clone.1 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.193.clone.1), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.192.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %negate.14, f32[1216,768]{1,0} %broadcast.582.clone.1), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Mul_1"}
  %add.176.clone.1 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.194.clone.1, f32[1216,768]{1,0} %multiply.192.clone.1), metadata={op_type="AddN" op_name="AddN"}
  %multiply.102.clone.1 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %add.176.clone.1, f32[1216,768]{1,0} %broadcast.237), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Mul"}
  %reduce.54.clone.1 = f32[1216]{0} reduce(f32[1216,768]{1,0} %multiply.102.clone.1, f32[] %constant_174), dimensions={1}, to_apply=%gradient_tape_model_bert_pretrainer_cls_predictions_transform_LayerNorm_batchnorm_mul_Sum-reduction.1063, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul/Sum"}
  ROOT %tuple.15 = (f32[1216]{0}, f32[1216]{0}) tuple(f32[1216]{0} %reduce.53, f32[1216]{0} %reduce.54.clone.1)
}

%max_float_.981 (x.982: f32[], y.983: f32[]) -> f32[] {
  %x.982 = f32[] parameter(0)
  %y.983 = f32[] parameter(1)
  ROOT %maximum.984 = f32[] maximum(f32[] %x.982, f32[] %y.983)
}

%fused_computation.102 (param_0.587: f32[30522], param_1.505: f16[1216,30528]) -> f32[1216] {
  %param_1.505 = f16[1216,30528]{1,0} parameter(1)
  %slice.23 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_1.505), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_0.587 = f32[30522]{0} parameter(0)
  %convert.252 = f16[30522]{0} convert(f32[30522]{0} %param_0.587), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.510 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.252), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.162 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.23, f16[1216,30522]{1,0} %broadcast.510), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.97 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.162), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %constant_178 = f32[] constant(-inf), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  ROOT %reduce.55 = f32[1216]{0} reduce(f32[1216,30522]{1,0} %convert.97, f32[] %constant_178), dimensions={1}, to_apply=%max_float_.981, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
}

%fused_computation.104 (param_0.575: f32[768], param_1.490: f16[1216,768], param_2.351: f32[768], param_3.245: f32[1216], param_4.150: f32[1216]) -> f16[1216,768] {
  %param_1.490 = f16[1216,768]{1,0} parameter(1)
  %convert.100 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_1.490), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %constant_402 = f32[] constant(1e-12), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %broadcast.499 = f32[1216]{0} broadcast(f32[] %constant_402), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %param_4.150 = f32[1216]{0} parameter(4)
  %constant_382 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.498 = f32[1216]{0} broadcast(f32[] %constant_382), dimensions={}
  %multiply.176 = f32[1216]{0} multiply(f32[1216]{0} %param_4.150, f32[1216]{0} %broadcast.498), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %add.156 = f32[1216]{0} add(f32[1216]{0} %broadcast.499, f32[1216]{0} %multiply.176), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add"}
  %rsqrt.10 = f32[1216]{0} rsqrt(f32[1216]{0} %add.156), metadata={op_type="Rsqrt" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/Rsqrt"}
  %broadcast.241 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %rsqrt.10), dimensions={0}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %param_2.351 = f32[768]{0} parameter(2)
  %broadcast.240 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_2.351), dimensions={1}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.143 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %broadcast.241, f32[1216,768]{1,0} %broadcast.240), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul"}
  %multiply.108 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %convert.100, f32[1216,768]{1,0} %multiply.143), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_1"}
  %param_0.575 = f32[768]{0} parameter(0)
  %broadcast.138 = f32[1216,768]{1,0} broadcast(f32[768]{0} %param_0.575), dimensions={1}, metadata={op_type="Sub" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub"}
  %param_3.245 = f32[1216]{0} parameter(3)
  %multiply.166 = f32[1216]{0} multiply(f32[1216]{0} %param_3.245, f32[1216]{0} %broadcast.498), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.472 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.166), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.107 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %multiply.143, f32[1216,768]{1,0} %broadcast.472), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2"}
  %subtract.15 = f32[1216,768]{1,0} subtract(f32[1216,768]{1,0} %broadcast.138, f32[1216,768]{1,0} %multiply.107), metadata={op_type="Sub" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/sub"}
  %add.71 = f32[1216,768]{1,0} add(f32[1216,768]{1,0} %multiply.108, f32[1216,768]{1,0} %subtract.15), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/add_1"}
  ROOT %convert.99 = f16[1216,768]{1,0} convert(f32[1216,768]{1,0} %add.71), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1"}
}

%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_variance-reduction.926 (x.927: f32[], y.928: f32[]) -> f32[] {
  %x.927 = f32[] parameter(0)
  %y.928 = f32[] parameter(1)
  ROOT %add.929 = f32[] add(f32[] %x.927, f32[] %y.928)
}

%fused_computation.106 (param_0.579: f32[1216], param_1.496: f16[1216,768]) -> f32[1216] {
  %param_1.496 = f16[1216,768]{1,0} parameter(1)
  %convert.248 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %param_1.496), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %param_0.579 = f32[1216]{0} parameter(0)
  %constant_389 = f32[] constant(0.00130208337), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/truediv_1"}
  %broadcast.483 = f32[1216]{0} broadcast(f32[] %constant_389), dimensions={}
  %multiply.170 = f32[1216]{0} multiply(f32[1216]{0} %param_0.579, f32[1216]{0} %broadcast.483), metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %broadcast.481 = f32[1216,768]{1,0} broadcast(f32[1216]{0} %multiply.170), dimensions={0}, metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %subtract.33 = f32[1216,768]{1,0} subtract(f32[1216,768]{1,0} %convert.248, f32[1216,768]{1,0} %broadcast.481), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %multiply.110 = f32[1216,768]{1,0} multiply(f32[1216,768]{1,0} %subtract.33, f32[1216,768]{1,0} %subtract.33), metadata={op_type="SquaredDifference" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/SquaredDifference"}
  %constant_181 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.56 = f32[1216]{0} reduce(f32[1216,768]{1,0} %multiply.110, f32[] %constant_181), dimensions={1}, to_apply=%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_variance-reduction.926, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
}

%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_mean-reduction.908 (x.909: f32[], y.910: f32[]) -> f32[] {
  %x.909 = f32[] parameter(0)
  %y.910 = f32[] parameter(1)
  ROOT %add.911 = f32[] add(f32[] %x.909, f32[] %y.910)
}

%fused_computation.109 (param_0.662: f16[1216,768]) -> (f32[1216], f16[1216,768], f16[1216,768]) {
  %constant_185_clone_1 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.246.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_185_clone_1), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %param_0.662 = f16[1216,768]{1,0} parameter(0)
  %constant_191_clone_1 = f16[] constant(0.044708), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %broadcast.141.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_191_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_1"}
  %multiply.146.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.662, f16[1216,768]{1,0} %param_0.662), metadata={op_type="Square" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow/Pow"}
  %multiply.114.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.662, f16[1216,768]{1,0} %multiply.146.clone.1), metadata={op_type="Pow" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Pow"}
  %multiply.113.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %broadcast.141.clone.1, f16[1216,768]{1,0} %multiply.114.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_1"}
  %add.73.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %param_0.662, f16[1216,768]{1,0} %multiply.113.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add"}
  %constant_193_clone_1 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.247.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_193_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2"}
  %multiply.112.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %add.73.clone.1, f16[1216,768]{1,0} %broadcast.247.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_2"}
  %tanh.0.clone.1 = f16[1216,768]{1,0} tanh(f16[1216,768]{1,0} %multiply.112.clone.1), metadata={op_type="Tanh" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/Tanh"}
  %add.96.clone.1 = f16[1216,768]{1,0} add(f16[1216,768]{1,0} %broadcast.246.clone.1, f16[1216,768]{1,0} %tanh.0.clone.1), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/add_1"}
  %constant_184_clone_1 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.245.clone.1 = f16[1216,768]{1,0} broadcast(f16[] %constant_184_clone_1), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.145.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %param_0.662, f16[1216,768]{1,0} %broadcast.245.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul"}
  %multiply.144.clone.1 = f16[1216,768]{1,0} multiply(f16[1216,768]{1,0} %add.96.clone.1, f16[1216,768]{1,0} %multiply.145.clone.1), metadata={op_type="Mul" op_name="model/bert_pretrainer/cls/predictions/transform/dense/Gelu/mul_3"}
  %convert.102 = f32[1216,768]{1,0} convert(f16[1216,768]{1,0} %multiply.144.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast"}
  %constant_183 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.57 = f32[1216]{0} reduce(f32[1216,768]{1,0} %convert.102, f32[] %constant_183), dimensions={1}, to_apply=%model_bert_pretrainer_cls_predictions_transform_LayerNorm_moments_mean-reduction.908, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  ROOT %tuple.21 = (f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) tuple(f32[1216]{0} %reduce.57, f16[1216,768]{1,0} %multiply.144.clone.1, f16[1216,768]{1,0} %tanh.0.clone.1)
}

%fused_computation.113 (param_0.254: f32[768]) -> f16[1216,768] {
  %param_0.254 = f32[768]{0} parameter(0)
  %convert.105 = f16[768]{0} convert(f32[768]{0} %param_0.254), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/Cast"}
  ROOT %broadcast.142 = f16[1216,768]{1,0} broadcast(f16[768]{0} %convert.105), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd"}
}

%add_float_.816 (x.817: f32[], y.818: f32[]) -> f32[] {
  %x.817 = f32[] parameter(0)
  %y.818 = f32[] parameter(1)
  ROOT %add.819 = f32[] add(f32[] %x.817, f32[] %y.818)
}

%fused_computation.115 (param_0.436: f16[16,768]) -> f32[768] {
  %param_0.436 = f16[16,768]{1,0} parameter(0)
  %convert.106 = f32[16,768]{1,0} convert(f16[16,768]{1,0} %param_0.436), metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/BiasAddGrad"}
  %constant_195 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.59 = f32[768]{0} reduce(f32[16,768]{1,0} %convert.106, f32[] %constant_195), dimensions={0}, to_apply=%add_float_.816, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/BiasAddGrad"}
}

%fused_computation.116 (param_0.259: f16[16,768], param_1.171: f16[16,768]) -> f16[16,768] {
  %param_0.259 = f16[16,768]{1,0} parameter(0)
  %constant_197 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.144 = f16[16,768]{1,0} broadcast(f16[] %constant_197), dimensions={}, metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  %param_1.171 = f16[16,768]{1,0} parameter(1)
  %multiply.116 = f16[16,768]{1,0} multiply(f16[16,768]{1,0} %param_1.171, f16[16,768]{1,0} %param_1.171), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  %subtract.17 = f16[16,768]{1,0} subtract(f16[16,768]{1,0} %broadcast.144, f16[16,768]{1,0} %multiply.116), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  ROOT %multiply.115 = f16[16,768]{1,0} multiply(f16[16,768]{1,0} %param_0.259, f16[16,768]{1,0} %subtract.17), metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
}

%fused_computation.117 (param_0.481: f32[16,2], param_1.372: f32[16], param_2.270: s32[16,1]) -> f16[16,2] {
  %constant_198 = f32[] constant(0.0625), metadata={op_type="Const" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/ExpandDims"}
  %broadcast.146 = f32[16,2]{1,0} broadcast(f32[] %constant_198), dimensions={}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/mul"}
  %param_0.481 = f32[16,2]{1,0} parameter(0)
  %param_1.372 = f32[16]{0} parameter(1)
  %broadcast.145 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_1.372), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %divide.4 = f32[16,2]{1,0} divide(f32[16,2]{1,0} %param_0.481, f32[16,2]{1,0} %broadcast.145), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %param_2.270 = s32[16,1]{1,0} parameter(2)
  %convert.164 = f32[16,1]{1,0} convert(s32[16,1]{1,0} %param_2.270), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_5"}
  %convert.163 = s64[16,1]{1,0} convert(f32[16,1]{1,0} %convert.164), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %bitcast.231 = s64[16]{0} bitcast(s64[16,1]{1,0} %convert.163), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_6"}
  %broadcast.316 = s64[16,2]{1,0} broadcast(s64[16]{0} %bitcast.231), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.8 = s64[16,2]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.31 = pred[16,2]{1,0} compare(s64[16,2]{1,0} %broadcast.316, s64[16,2]{1,0} %iota.8), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_274 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %broadcast.315 = f32[16,2]{1,0} broadcast(f32[] %constant_274), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_273 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %broadcast.314 = f32[16,2]{1,0} broadcast(f32[] %constant_273), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.46 = f32[16,2]{1,0} select(pred[16,2]{1,0} %compare.31, f32[16,2]{1,0} %broadcast.315, f32[16,2]{1,0} %broadcast.314), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_271 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.313 = s64[16]{0} broadcast(s64[] %constant_271), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.30 = pred[16]{0} compare(s64[16]{0} %broadcast.313, s64[16]{0} %bitcast.231), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_270 = s64[] constant(2), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.312 = s64[16]{0} broadcast(s64[] %constant_270), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.29 = pred[16]{0} compare(s64[16]{0} %bitcast.231, s64[16]{0} %broadcast.312), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %and.5 = pred[16]{0} and(pred[16]{0} %compare.30, pred[16]{0} %compare.29), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.311 = f32[16]{0} broadcast(f32[] %constant_273), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_269 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.309 = f32[16]{0} broadcast(f32[] %constant_269), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %select.45 = f32[16]{0} select(pred[16]{0} %and.5, f32[16]{0} %broadcast.311, f32[16]{0} %broadcast.309), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.308 = f32[16,2]{1,0} broadcast(f32[16]{0} %select.45), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %add.106 = f32[16,2]{1,0} add(f32[16,2]{1,0} %select.46, f32[16,2]{1,0} %broadcast.308), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.18 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %divide.4, f32[16,2]{1,0} %add.106), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.117 = f32[16,2]{1,0} multiply(f32[16,2]{1,0} %broadcast.146, f32[16,2]{1,0} %subtract.18), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/mul"}
  ROOT %convert.107 = f16[16,2]{1,0} convert(f32[16,2]{1,0} %multiply.117), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_4/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
}

%max_float_.755 (x.756: f32[], y.757: f32[]) -> f32[] {
  %x.756 = f32[] parameter(0)
  %y.757 = f32[] parameter(1)
  ROOT %maximum.758 = f32[] maximum(f32[] %x.756, f32[] %y.757)
}

%fused_computation.120 (param_0.554: f32[2], param_1.465: f16[16,8]) -> f32[16] {
  %param_1.465 = f16[16,8]{1,0} parameter(1)
  %slice.13 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_1.465), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_0.554 = f32[2]{0} parameter(0)
  %convert.221 = f16[2]{0} convert(f32[2]{0} %param_0.554), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.406 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.221), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.136 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.13, f16[16,2]{1,0} %broadcast.406), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.111 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.136), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %constant_199 = f32[] constant(-inf), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  ROOT %reduce.60 = f32[16]{0} reduce(f32[16,2]{1,0} %convert.111, f32[] %constant_199), dimensions={1}, to_apply=%max_float_.755, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
}

%fused_computation.122 (param_0.439: f32[768,2]) -> f16[768,8] {
  %param_0.439 = f32[768,2]{1,0} parameter(0)
  %convert.113 = f16[768,2]{1,0} convert(f32[768,2]{1,0} %param_0.439), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul/Cast"}
  %constant_200 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.17 = f16[768,8]{1,0} pad(f16[768,2]{1,0} %convert.113, f16[] %constant_200), padding=0_0x0_6, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
}

%fused_computation.123 (param_0.274: f32[768]) -> f16[16,768] {
  %param_0.274 = f32[768]{0} parameter(0)
  %convert.114 = f16[768]{0} convert(f32[768]{0} %param_0.274), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/Cast"}
  ROOT %broadcast.156 = f16[16,768]{1,0} broadcast(f16[768]{0} %convert.114), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd"}
}

%fused_computation.124 (param_0.279: f32[1,768,16,512]) -> f16[16,768] {
  %param_0.279 = f32[1,768,16,512]{1,3,2,0} parameter(0)
  %transpose.148 = f32[1,16,512,768]{3,2,1,0} transpose(f32[1,768,16,512]{1,3,2,0} %param_0.279), dimensions={0,2,3,1}, metadata={op_type="Transpose" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3-0-1-TransposeNCHWToNHWC-LayoutOptimizer"}
  %bitcast.202 = f32[16,512,768]{2,1,0} bitcast(f32[1,16,512,768]{3,2,1,0} %transpose.148), metadata={op_type="Squeeze" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Squeeze_1"}
  %slice.11 = f32[16,1,768]{2,1,0} slice(f32[16,512,768]{2,1,0} %bitcast.202), slice={[0:16], [0:1], [0:768]}, metadata={op_type="StridedSlice" op_name="model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice"}
  %convert.115 = f16[16,1,768]{2,1,0} convert(f32[16,1,768]{2,1,0} %slice.11), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_3"}
  ROOT %bitcast.201 = f16[16,768]{1,0} bitcast(f16[16,1,768]{2,1,0} %convert.115), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_3"}
}

%fused_computation.125 (param_0.548: f16[16,512,3072], param_1.455: f16[16,512,3072], param_2.321: f16[8192,3072], param_3.226: f32[3072]) -> f16[8192,3072] {
  %param_1.455 = f16[16,512,3072]{2,1,0} parameter(1)
  %constant_204 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.254 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_204), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.18 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_1.455, f16[16,512,3072]{2,1,0} %broadcast.254), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %constant_203 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.252 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_203), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.548 = f16[16,512,3072]{2,1,0} parameter(0)
  %add.97 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.252, f16[16,512,3072]{2,1,0} %param_0.548), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_3.226 = f32[3072]{0} parameter(3)
  %convert.217 = f16[3072]{0} convert(f32[3072]{0} %param_3.226), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast"}
  %broadcast.400 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.217), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %param_2.321 = f16[8192,3072]{1,0} parameter(2)
  %bitcast.243 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_2.321), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum"}
  %add.132 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.400, f16[16,512,3072]{2,1,0} %bitcast.243), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %constant_202 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.250 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_202), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.147 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.132, f16[16,512,3072]{2,1,0} %broadcast.250), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul"}
  %multiply.118 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.97, f16[16,512,3072]{2,1,0} %multiply.147), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_3"}
  %select.33 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.18, f16[16,512,3072]{2,1,0} %multiply.118, f16[16,512,3072]{2,1,0} %broadcast.254), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/Mul_1"}
  ROOT %bitcast.203 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %select.33)
}

%fused_computation.126 (param_0.551: f16[8192,3072], param_1.460: f32[3072]) -> f16[16,512,3072] {
  %param_1.460 = f32[3072]{0} parameter(1)
  %convert.219 = f16[3072]{0} convert(f32[3072]{0} %param_1.460), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast"}
  %broadcast.403 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.219), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %param_0.551 = f16[8192,3072]{1,0} parameter(0)
  %bitcast.245 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_0.551), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum"}
  %add.134 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.403, f16[16,512,3072]{2,1,0} %bitcast.245), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add"}
  %multiply.148 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.134, f16[16,512,3072]{2,1,0} %add.134)
  %multiply.121 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.134, f16[16,512,3072]{2,1,0} %multiply.148), metadata={op_type="Pow" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Pow"}
  %constant_205 = f16[] constant(0.044708), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %broadcast.256 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_205), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %multiply.120 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.121, f16[16,512,3072]{2,1,0} %broadcast.256), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_1"}
  %add.77 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.120, f16[16,512,3072]{2,1,0} %add.134), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add"}
  %constant_206 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.257 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_206), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.119 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.77, f16[16,512,3072]{2,1,0} %broadcast.257), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/mul_2"}
  ROOT %tanh.1 = f16[16,512,3072]{2,1,0} tanh(f16[16,512,3072]{2,1,0} %multiply.119), metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Tanh"}
}

%fused_computation.128 (param_0.289: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.289 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.149 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.289), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  %copy.88 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.149), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}
  ROOT %bitcast.205 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.88)
}

%fused_computation.130 (param_0.538: f16[16,12,512,512], param_1.442: f32[16,12,512], param_2.318: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_2.318 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.101 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.318), metadata={op_name="XLA_Args"}
  %constant_337 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.391 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_337), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.43 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.101, f16[16,12,512,512]{2,3,1,0} %broadcast.391), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/GreaterEqual"}
  %constant_207 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.260 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_207), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_209 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.259 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_209), dimensions={}
  %select.34 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.43, f16[16,12,512,512]{2,3,1,0} %broadcast.260, f16[16,12,512,512]{2,3,1,0} %broadcast.259), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul"}
  %param_0.538 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.442 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.197 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.442), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %broadcast.380 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.197), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %divide.14 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.538, f16[16,12,512,512]{2,3,1,0} %broadcast.380), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %multiply.122 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.34, f16[16,12,512,512]{2,3,1,0} %divide.14), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  ROOT %copy.89 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.122), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
}

%fused_computation.132 (param_0.297: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_0.297 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %convert.119 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %param_0.297), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  ROOT %bitcast.206 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.119), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
}

%fused_computation.133 (param_0.299: f16[16,12,512,512], param_1.210: f16[16,12,512]) -> f16[16,12,512,512] {
  %param_0.299 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.210 = f16[16,12,512]{2,1,0} parameter(1)
  %broadcast.160 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %param_1.210), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %subtract.20 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %param_0.299, f16[16,12,512,512]{2,3,1,0} %broadcast.160), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  ROOT %exponential.0 = f16[16,12,512,512]{2,3,1,0} exponential(f16[16,12,512,512]{2,3,1,0} %subtract.20), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
}

%fused_computation.134 (param_0.485: f16[16,12,512,512], param_1.378: s32[16,512]) -> f16[16,12,512,512] {
  %param_0.485 = f16[16,12,512,512]{3,2,1,0} parameter(0)
  %transpose.150 = f16[16,12,512,512]{2,3,1,0} transpose(f16[16,12,512,512]{3,2,1,0} %param_0.485), dimensions={0,1,3,2}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  %constant_286 = f16[] constant(-65504), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.332 = f16[16,512]{1,0} broadcast(f16[] %constant_286), dimensions={}
  %constant_284 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.331 = f16[16,512]{1,0} broadcast(f16[] %constant_284), dimensions={}
  %param_1.378 = s32[16,512]{1,0} parameter(1)
  %convert.168 = f16[16,512]{1,0} convert(s32[16,512]{1,0} %param_1.378), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/self_attention_mask/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int32_Cast"}
  %subtract.27 = f16[16,512]{1,0} subtract(f16[16,512]{1,0} %broadcast.331, f16[16,512]{1,0} %convert.168), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %multiply.154 = f16[16,512]{1,0} multiply(f16[16,512]{1,0} %broadcast.332, f16[16,512]{1,0} %subtract.27), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.329 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,512]{1,0} %multiply.154), dimensions={0,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
  ROOT %add.80 = f16[16,12,512,512]{2,3,1,0} add(f16[16,12,512,512]{2,3,1,0} %transpose.150, f16[16,12,512,512]{2,3,1,0} %broadcast.329), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/add"}
}

%fused_computation.136 (param_0.305: f16[8192,768], param_1.219: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.219 = f32[12,64]{1,0} parameter(1)
  %convert.121 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.219), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Cast"}
  %broadcast.162 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.121), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add"}
  %param_0.305 = f16[8192,768]{1,0} parameter(0)
  %reshape.232 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.305), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum"}
  %add.82 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.162, f16[16,512,12,64]{3,1,2,0} %reshape.232), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add"}
  ROOT %transpose.151 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.82), dimensions={0,2,1,3}
}

%fused_computation.137 (param_0.519: f16[16,512,3072], param_1.418: f16[16,512,3072], param_2.297: f16[8192,3072], param_3.215: f32[3072]) -> f16[8192,3072] {
  %param_1.418 = f16[16,512,3072]{2,1,0} parameter(1)
  %constant_214 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.267 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_214), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_1/dropout/GreaterEqual"}
  %compare.19 = pred[16,512,3072]{2,1,0} compare(f16[16,512,3072]{2,1,0} %param_1.418, f16[16,512,3072]{2,1,0} %broadcast.267), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %constant_213 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.265 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_213), dimensions={}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/add_1"}
  %param_0.519 = f16[16,512,3072]{2,1,0} parameter(0)
  %add.98 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.265, f16[16,512,3072]{2,1,0} %param_0.519), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/add_1"}
  %param_3.215 = f32[3072]{0} parameter(3)
  %convert.188 = f16[3072]{0} convert(f32[3072]{0} %param_3.215), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast"}
  %broadcast.365 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.188), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %param_2.297 = f16[8192,3072]{1,0} parameter(2)
  %bitcast.237 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_2.297), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum"}
  %add.118 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.365, f16[16,512,3072]{2,1,0} %bitcast.237), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %constant_212 = f16[] constant(0.5), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %broadcast.264 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_212), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.149 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.118, f16[16,512,3072]{2,1,0} %broadcast.264), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul"}
  %multiply.124 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.98, f16[16,512,3072]{2,1,0} %multiply.149), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_3"}
  %select.35 = f16[16,512,3072]{2,1,0} select(pred[16,512,3072]{2,1,0} %compare.19, f16[16,512,3072]{2,1,0} %multiply.124, f16[16,512,3072]{2,1,0} %broadcast.267), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/Mul_1"}
  ROOT %bitcast.207 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %select.35)
}

%fused_computation.138 (param_0.522: f16[8192,3072], param_1.423: f32[3072]) -> f16[16,512,3072] {
  %param_1.423 = f32[3072]{0} parameter(1)
  %convert.191 = f16[3072]{0} convert(f32[3072]{0} %param_1.423), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast"}
  %broadcast.367 = f16[16,512,3072]{2,1,0} broadcast(f16[3072]{0} %convert.191), dimensions={2}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %param_0.522 = f16[8192,3072]{1,0} parameter(0)
  %bitcast.239 = f16[16,512,3072]{2,1,0} bitcast(f16[8192,3072]{1,0} %param_0.522), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum"}
  %add.120 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %broadcast.367, f16[16,512,3072]{2,1,0} %bitcast.239), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add"}
  %multiply.150 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.120, f16[16,512,3072]{2,1,0} %add.120)
  %multiply.127 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.120, f16[16,512,3072]{2,1,0} %multiply.150), metadata={op_type="Pow" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Pow"}
  %constant_215 = f16[] constant(0.044708), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %broadcast.268 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_215), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %multiply.126 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %multiply.127, f16[16,512,3072]{2,1,0} %broadcast.268), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_1"}
  %add.83 = f16[16,512,3072]{2,1,0} add(f16[16,512,3072]{2,1,0} %multiply.126, f16[16,512,3072]{2,1,0} %add.120), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/add"}
  %constant_218 = f16[] constant(0.79785), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %broadcast.270 = f16[16,512,3072]{2,1,0} broadcast(f16[] %constant_218), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  %multiply.125 = f16[16,512,3072]{2,1,0} multiply(f16[16,512,3072]{2,1,0} %add.83, f16[16,512,3072]{2,1,0} %broadcast.270), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/mul_2"}
  ROOT %tanh.2 = f16[16,512,3072]{2,1,0} tanh(f16[16,512,3072]{2,1,0} %multiply.125), metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Tanh"}
}

%fused_computation.140 (param_0.315: f16[16,12,512,64]) -> f16[8192,768] {
  %param_0.315 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.152 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.315), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  %copy.90 = f16[16,512,12,64]{3,2,1,0} copy(f16[16,512,12,64]{3,1,2,0} %transpose.152), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}
  ROOT %bitcast.209 = f16[8192,768]{1,0} bitcast(f16[16,512,12,64]{3,2,1,0} %copy.90)
}

%fused_computation.142 (param_0.509: f16[16,12,512,512], param_1.405: f32[16,12,512], param_2.294: f16[16,12,512,512]) -> f16[16,12,512,512] {
  %param_2.294 = f16[16,12,512,512]{3,2,1,0} parameter(2)
  %copy.97 = f16[16,12,512,512]{2,3,1,0} copy(f16[16,12,512,512]{3,2,1,0} %param_2.294), metadata={op_name="XLA_Args"}
  %constant_313 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.357 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_313), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %compare.39 = pred[16,12,512,512]{2,3,1,0} compare(f16[16,12,512,512]{2,3,1,0} %copy.97, f16[16,12,512,512]{2,3,1,0} %broadcast.357), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/GreaterEqual"}
  %constant_220 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.272 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_220), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %constant_221 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.271 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[] %constant_221), dimensions={}
  %select.36 = f16[16,12,512,512]{2,3,1,0} select(pred[16,12,512,512]{2,3,1,0} %compare.39, f16[16,12,512,512]{2,3,1,0} %broadcast.272, f16[16,12,512,512]{2,3,1,0} %broadcast.271), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul"}
  %param_0.509 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.405 = f32[16,12,512]{2,1,0} parameter(1)
  %convert.176 = f16[16,12,512]{2,1,0} convert(f32[16,12,512]{2,1,0} %param_1.405), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %broadcast.346 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %convert.176), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %divide.8 = f16[16,12,512,512]{2,3,1,0} divide(f16[16,12,512,512]{2,3,1,0} %param_0.509, f16[16,12,512,512]{2,3,1,0} %broadcast.346), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %multiply.128 = f16[16,12,512,512]{2,3,1,0} multiply(f16[16,12,512,512]{2,3,1,0} %select.36, f16[16,12,512,512]{2,3,1,0} %divide.8), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  ROOT %copy.91 = f16[16,12,512,512]{3,2,1,0} copy(f16[16,12,512,512]{2,3,1,0} %multiply.128), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
}

%fused_computation.144 (param_0.323: f16[16,12,512,512]) -> f32[16,12,512,512] {
  %param_0.323 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %convert.125 = f32[16,12,512,512]{2,3,1,0} convert(f16[16,12,512,512]{2,3,1,0} %param_0.323), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  ROOT %bitcast.210 = f32[16,12,512,512]{3,2,1,0} bitcast(f32[16,12,512,512]{2,3,1,0} %convert.125), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
}

%fused_computation.145 (param_0.325: f16[16,12,512,512], param_1.239: f16[16,12,512]) -> f16[16,12,512,512] {
  %param_0.325 = f16[16,12,512,512]{2,3,1,0} parameter(0)
  %param_1.239 = f16[16,12,512]{2,1,0} parameter(1)
  %broadcast.166 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,12,512]{2,1,0} %param_1.239), dimensions={0,1,2}, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %subtract.21 = f16[16,12,512,512]{2,3,1,0} subtract(f16[16,12,512,512]{2,3,1,0} %param_0.325, f16[16,12,512,512]{2,3,1,0} %broadcast.166), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  ROOT %exponential.1 = f16[16,12,512,512]{2,3,1,0} exponential(f16[16,12,512,512]{2,3,1,0} %subtract.21), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
}

%fused_computation.146 (param_0.483: f16[16,12,512,512], param_1.375: s32[16,512]) -> f16[16,12,512,512] {
  %param_0.483 = f16[16,12,512,512]{3,2,1,0} parameter(0)
  %transpose.153 = f16[16,12,512,512]{2,3,1,0} transpose(f16[16,12,512,512]{3,2,1,0} %param_0.483), dimensions={0,1,3,2}, metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  %constant_279 = f16[] constant(-65504), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.324 = f16[16,512]{1,0} broadcast(f16[] %constant_279), dimensions={}
  %constant_278 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.322 = f16[16,512]{1,0} broadcast(f16[] %constant_278), dimensions={}
  %param_1.375 = s32[16,512]{1,0} parameter(1)
  %convert.166 = f16[16,512]{1,0} convert(s32[16,512]{1,0} %param_1.375), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/self_attention_mask/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int32_Cast"}
  %subtract.25 = f16[16,512]{1,0} subtract(f16[16,512]{1,0} %broadcast.322, f16[16,512]{1,0} %convert.166), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %multiply.152 = f16[16,512]{1,0} multiply(f16[16,512]{1,0} %broadcast.324, f16[16,512]{1,0} %subtract.25), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul"}
  %broadcast.321 = f16[16,12,512,512]{2,3,1,0} broadcast(f16[16,512]{1,0} %multiply.152), dimensions={0,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
  ROOT %add.86 = f16[16,12,512,512]{2,3,1,0} add(f16[16,12,512,512]{2,3,1,0} %transpose.153, f16[16,12,512,512]{2,3,1,0} %broadcast.321), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
}

%fused_computation.149 (param_0.335: f16[8192,768], param_1.252: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.252 = f32[12,64]{1,0} parameter(1)
  %convert.128 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.252), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Cast"}
  %broadcast.171 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.128), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add"}
  %param_0.335 = f16[8192,768]{1,0} parameter(0)
  %reshape.236 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.335), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum"}
  %add.88 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.171, f16[16,512,12,64]{3,1,2,0} %reshape.236), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add"}
  ROOT %transpose.154 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.88), dimensions={0,2,1,3}
}

%fused_computation.150 (param_0.489: f32[1,768,16,512], param_1.380: f16[16,512,768]) -> f16[8192,768] {
  %param_1.380 = f16[16,512,768]{2,1,0} parameter(1)
  %constant_292 = f16[] constant(0.099976), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %broadcast.336 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_292), dimensions={}, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/GreaterEqual"}
  %compare.35 = pred[16,512,768]{2,1,0} compare(f16[16,512,768]{2,1,0} %param_1.380, f16[16,512,768]{2,1,0} %broadcast.336), direction=GE, metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/GreaterEqual"}
  %constant_224 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %broadcast.277 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_224), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/dropout_2/dropout/Mul"}
  %constant_225 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.276 = f16[16,512,768]{2,1,0} broadcast(f16[] %constant_225), dimensions={}
  %select.37 = f16[16,512,768]{2,1,0} select(pred[16,512,768]{2,1,0} %compare.35, f16[16,512,768]{2,1,0} %broadcast.277, f16[16,512,768]{2,1,0} %broadcast.276), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %param_0.489 = f32[1,768,16,512]{1,3,2,0} parameter(0)
  %convert.151 = f16[1,768,16,512]{1,3,2,0} convert(f32[1,768,16,512]{1,3,2,0} %param_0.489), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %transpose.155 = f16[1,16,512,768]{3,2,1,0} transpose(f16[1,768,16,512]{1,3,2,0} %convert.151), dimensions={0,2,3,1}, metadata={op_type="Transpose" op_name="model/bert_pretrainer/bert_encoder_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1-0-0-TransposeNCHWToNHWC-LayoutOptimizer"}
  %bitcast.212 = f16[16,512,768]{2,1,0} bitcast(f16[1,16,512,768]{3,2,1,0} %transpose.155), metadata={op_type="Squeeze" op_name="model/bert_pretrainer/bert_encoder_1/tf.compat.v1.squeeze/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_half_Squeeze"}
  %multiply.131 = f16[16,512,768]{2,1,0} multiply(f16[16,512,768]{2,1,0} %select.37, f16[16,512,768]{2,1,0} %bitcast.212), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul_1"}
  ROOT %bitcast.211 = f16[8192,768]{1,0} bitcast(f16[16,512,768]{2,1,0} %multiply.131)
}

%fused_computation.151 (param_0.491: f16[8192,768], param_1.382: f32[512,768], param_2.283: s32[16,512], param_3.204: f32[30522,768]) -> f32[1,768,16,512] {
  %param_1.382 = f32[512,768]{1,0} parameter(1)
  %convert.130 = f16[512,768]{1,0} convert(f32[512,768]{1,0} %param_1.382), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/position_embedding/Cast"}
  %broadcast.172 = f16[16,512,768]{2,1,0} broadcast(f16[512,768]{1,0} %convert.130), dimensions={1,2}, metadata={op_type="BroadcastTo" op_name="model/bert_pretrainer/bert_encoder_1/position_embedding/BroadcastTo"}
  %param_0.491 = f16[8192,768]{1,0} parameter(0)
  %bitcast.215 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %param_0.491), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/Reshape_1"}
  %add.90 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %broadcast.172, f16[16,512,768]{2,1,0} %bitcast.215), metadata={op_type="AddN" op_name="model/bert_pretrainer/bert_encoder_1/add/ArithmeticOptimizer/AddOpsRewrite_add_1"}
  %param_3.204 = f32[30522,768]{1,0} parameter(3)
  %convert.170 = f16[30522,768]{1,0} convert(f32[30522,768]{1,0} %param_3.204), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2/Cast"}
  %param_2.283 = s32[16,512]{1,0} parameter(2)
  %bitcast.233 = s32[8192]{0} bitcast(s32[16,512]{1,0} %param_2.283), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %gather.2 = f16[8192,768]{1,0} gather(f16[30522,768]{1,0} %convert.170, s32[8192]{0} %bitcast.233), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="GatherV2" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2"}
  %bitcast.214 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %gather.2), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape_1"}
  %add.89 = f16[16,512,768]{2,1,0} add(f16[16,512,768]{2,1,0} %add.90, f16[16,512,768]{2,1,0} %bitcast.214), metadata={op_type="AddN" op_name="model/bert_pretrainer/bert_encoder_1/add/ArithmeticOptimizer/AddOpsRewrite_add_1"}
  %convert.129 = f32[16,512,768]{2,1,0} convert(f16[16,512,768]{2,1,0} %add.89), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  %bitcast.213 = f32[1,16,512,768]{3,2,1,0} bitcast(f32[16,512,768]{2,1,0} %convert.129), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast"}
  ROOT %transpose.156 = f32[1,768,16,512]{1,3,2,0} transpose(f32[1,16,512,768]{3,2,1,0} %bitcast.213), dimensions={0,3,1,2}, metadata={op_type="Transpose" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormGradV3-1-TransposeNHWCToNCHW-LayoutOptimizer"}
}

%fused_computation.152 (param_0.467: f32[2,768]) -> f16[8,768] {
  %param_0.467 = f32[2,768]{1,0} parameter(0)
  %convert.131 = f16[2,768]{1,0} convert(f32[2,768]{1,0} %param_0.467), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul/Cast"}
  %constant_227 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.18 = f16[8,768]{1,0} pad(f16[2,768]{1,0} %convert.131, f16[] %constant_227), padding=0_6x0_0, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
}

%fused_computation.153 (param_0.468: s32[16,512]) -> f16[8192,8] {
  %param_0.468 = s32[16,512]{1,0} parameter(0)
  %bitcast.216 = s32[8192]{0} bitcast(s32[16,512]{1,0} %param_0.468), metadata={op_type="Reshape" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/Reshape"}
  %broadcast.175 = s32[8192,2]{1,0} broadcast(s32[8192]{0} %bitcast.216), dimensions={0}, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %iota.4 = s32[8192,2]{1,0} iota(), iota_dimension=1, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %compare.10 = pred[8192,2]{1,0} compare(s32[8192,2]{1,0} %broadcast.175, s32[8192,2]{1,0} %iota.4), direction=EQ, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %constant_228 = f16[] constant(1), metadata={op_type="Sub" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/sub"}
  %broadcast.174 = f16[8192,2]{1,0} broadcast(f16[] %constant_228), dimensions={}, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %constant_229 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %broadcast.173 = f16[8192,2]{1,0} broadcast(f16[] %constant_229), dimensions={}, metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  %select.38 = f16[8192,2]{1,0} select(pred[8192,2]{1,0} %compare.10, f16[8192,2]{1,0} %broadcast.174, f16[8192,2]{1,0} %broadcast.173), metadata={op_type="OneHot" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/one_hot"}
  ROOT %pad.19 = f16[8192,8]{1,0} pad(f16[8192,2]{1,0} %select.38, f16[] %constant_229), padding=0_0x0_6, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
}

%fused_computation.157 (param_0.474: s64[], param_1.614: f32[], param_2.451: f32[]) -> (f32[], f32[], f32[]) {
  %constant_96 = f32[] constant(5e-05), metadata={op_type="Mul" op_name="Adam/PolynomialDecay/Mul"}
  %constant_237 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %param_0.474 = s64[] parameter(0)
  %convert.132 = f32[] convert(s64[] %param_0.474), metadata={op_type="Cast" op_name="Adam/Cast"}
  %minimum.0 = f32[] minimum(f32[] %convert.132, f32[] %constant_237), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %subtract.23 = f32[] subtract(f32[] %constant_237, f32[] %minimum.0), metadata={op_type="Sub" op_name="Adam/PolynomialDecay/sub_1"}
  %multiply.132 = f32[] multiply(f32[] %constant_96, f32[] %subtract.23), metadata={op_type="Mul" op_name="Adam/PolynomialDecay/Mul"}
  %param_1.614 = f32[] parameter(1)
  %constant_246_clone_1 = s64[] constant(1), metadata={op_type="AddV2" op_name="Adam/add"}
  %add.102.clone.1 = s64[] add(s64[] %param_0.474, s64[] %constant_246_clone_1), metadata={op_type="AddV2" op_name="Adam/add"}
  %convert.156.clone.1 = f32[] convert(s64[] %add.102.clone.1), metadata={op_type="Cast" op_name="Adam/Cast_1"}
  %power.11.clone.1 = f32[] power(f32[] %param_1.614, f32[] %convert.156.clone.1), metadata={op_type="Pow" op_name="Adam/Pow_1"}
  %param_2.451 = f32[] parameter(2)
  %power.10.clone.1 = f32[] power(f32[] %param_2.451, f32[] %convert.156.clone.1), metadata={op_type="Pow" op_name="Adam/Pow"}
  ROOT %tuple.24 = (f32[], f32[], f32[]) tuple(f32[] %multiply.132, f32[] %power.11.clone.1, f32[] %power.10.clone.1)
}

%fused_computation.160 (param_0.389: f16[8192,768]) -> f16[16,12,512,64] {
  %param_0.389 = f16[8192,768]{1,0} parameter(0)
  %reshape.238 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.389), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum"}
  ROOT %transpose.163 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.238), dimensions={0,2,1,3}
}

%fused_computation.161 (param_0.407: f16[8192,768]) -> f16[16,12,512,64] {
  %param_0.407 = f16[8192,768]{1,0} parameter(0)
  %reshape.240 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.407), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum"}
  ROOT %transpose.170 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %reshape.240), dimensions={0,2,1,3}
}

%fused_computation.162 (param_0.426: f32[30522,768]) -> f16[30528,768] {
  %param_0.426 = f32[30522,768]{1,0} parameter(0)
  %convert.142 = f16[30522,768]{1,0} convert(f32[30522,768]{1,0} %param_0.426), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/word_embeddings/GatherV2/Cast"}
  %constant_179 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.20 = f16[30528,768]{1,0} pad(f16[30522,768]{1,0} %convert.142, f16[] %constant_179), padding=0_6x0_0, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
}

%fused_computation.164 (param_0.444: f32[12,64,768]) -> f16[768,768] {
  %param_0.444 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.143 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.444), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast"}
  ROOT %bitcast.219 = f16[768,768]{1,0} bitcast(f16[12,64,768]{2,1,0} %convert.143)
}

%fused_computation.165 (param_0.446: f32[768,12,64]) -> f16[768,768] {
  %param_0.446 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.144 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.446), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast"}
  ROOT %bitcast.220 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.144)
}

%fused_computation.166 (param_0.450: f32[768,12,64]) -> f16[768,768] {
  %param_0.450 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.145 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.450), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast"}
  ROOT %bitcast.221 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.145)
}

%fused_computation.167 (param_0.452: f32[768,12,64]) -> f16[768,768] {
  %param_0.452 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.146 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.452), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast"}
  ROOT %bitcast.222 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.146)
}

%fused_computation.168 (param_0.457: f32[12,64,768]) -> f16[768,768] {
  %param_0.457 = f32[12,64,768]{2,1,0} parameter(0)
  %convert.147 = f16[12,64,768]{2,1,0} convert(f32[12,64,768]{2,1,0} %param_0.457), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast"}
  ROOT %bitcast.223 = f16[768,768]{1,0} bitcast(f16[12,64,768]{2,1,0} %convert.147)
}

%fused_computation.169 (param_0.459: f32[768,12,64]) -> f16[768,768] {
  %param_0.459 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.148 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.459), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast"}
  ROOT %bitcast.224 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.148)
}

%fused_computation.170 (param_0.463: f32[768,12,64]) -> f16[768,768] {
  %param_0.463 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.149 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.463), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast"}
  ROOT %bitcast.225 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.149)
}

%fused_computation.171 (param_0.465: f32[768,12,64]) -> f16[768,768] {
  %param_0.465 = f32[768,12,64]{2,1,0} parameter(0)
  %convert.150 = f16[768,12,64]{2,1,0} convert(f32[768,12,64]{2,1,0} %param_0.465), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast"}
  ROOT %bitcast.226 = f16[768,768]{1,0} bitcast(f16[768,12,64]{2,1,0} %convert.150)
}

%fused_computation.175 (param_0.496: f16[8192,768], param_1.389: f32[12,64]) -> f16[16,12,64,512] {
  %param_1.389 = f32[12,64]{1,0} parameter(1)
  %convert.172 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.389), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast"}
  %broadcast.340 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.172), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %param_0.496 = f16[8192,768]{1,0} parameter(0)
  %reshape.242 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.496), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum"}
  %add.108 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.340, f16[16,512,12,64]{1,3,2,0} %reshape.242), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add"}
  %constant_297 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.339 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_297), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %multiply.156 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %add.108, f16[16,512,12,64]{1,3,2,0} %broadcast.339), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  ROOT %transpose.171 = f16[16,12,64,512]{3,2,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.156), dimensions={0,2,3,1}
}

%fused_computation.176 (param_0.513: f16[8192,768], param_1.410: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.410 = f32[12,64]{1,0} parameter(1)
  %convert.182 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.410), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast"}
  %broadcast.359 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.182), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  %param_0.513 = f16[8192,768]{1,0} parameter(0)
  %reshape.246 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.513), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum"}
  %add.112 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.359, f16[16,512,12,64]{3,1,2,0} %reshape.246), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add"}
  ROOT %transpose.172 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.112), dimensions={0,2,1,3}
}

%fused_computation.177 (param_0.526: f16[8192,768], param_1.428: f32[12,64]) -> f16[16,12,64,512] {
  %constant_323 = f16[] constant(0.125), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %broadcast.371 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[] %constant_323), dimensions={}, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %param_1.428 = f32[12,64]{1,0} parameter(1)
  %convert.193 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.428), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast"}
  %broadcast.370 = f16[16,512,12,64]{1,3,2,0} broadcast(f16[12,64]{1,0} %convert.193), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %param_0.526 = f16[8192,768]{1,0} parameter(0)
  %reshape.251 = f16[16,512,12,64]{1,3,2,0} reshape(f16[8192,768]{1,0} %param_0.526), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum"}
  %add.122 = f16[16,512,12,64]{1,3,2,0} add(f16[16,512,12,64]{1,3,2,0} %broadcast.370, f16[16,512,12,64]{1,3,2,0} %reshape.251), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add"}
  %multiply.160 = f16[16,512,12,64]{1,3,2,0} multiply(f16[16,512,12,64]{1,3,2,0} %broadcast.371, f16[16,512,12,64]{1,3,2,0} %add.122), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  ROOT %transpose.173 = f16[16,12,64,512]{3,2,1,0} transpose(f16[16,512,12,64]{1,3,2,0} %multiply.160), dimensions={0,2,3,1}
}

%fused_computation.178 (param_0.542: f16[8192,768], param_1.447: f32[12,64]) -> f16[16,12,512,64] {
  %param_1.447 = f32[12,64]{1,0} parameter(1)
  %convert.210 = f16[12,64]{1,0} convert(f32[12,64]{1,0} %param_1.447), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast"}
  %broadcast.393 = f16[16,512,12,64]{3,1,2,0} broadcast(f16[12,64]{1,0} %convert.210), dimensions={2,3}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  %param_0.542 = f16[8192,768]{1,0} parameter(0)
  %reshape.257 = f16[16,512,12,64]{3,1,2,0} reshape(f16[8192,768]{1,0} %param_0.542), metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum"}
  %add.126 = f16[16,512,12,64]{3,1,2,0} add(f16[16,512,12,64]{3,1,2,0} %broadcast.393, f16[16,512,12,64]{3,1,2,0} %reshape.257), metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add"}
  ROOT %transpose.174 = f16[16,12,512,64]{3,2,1,0} transpose(f16[16,512,12,64]{3,1,2,0} %add.126), dimensions={0,2,1,3}
}

%fused_computation.179 (param_0.562: f32[16], param_1.477: f32[2], param_2.334: f16[16,8]) -> f32[16,2] {
  %param_2.334 = f16[16,8]{1,0} parameter(2)
  %slice.19 = f16[16,2]{1,0} slice(f16[16,8]{1,0} %param_2.334), slice={[0:16], [0:2]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %param_1.477 = f32[2]{0} parameter(1)
  %convert.229 = f16[2]{0} convert(f32[2]{0} %param_1.477), metadata={op_type="Cast" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/Cast"}
  %broadcast.415 = f16[16,2]{1,0} broadcast(f16[2]{0} %convert.229), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %add.142 = f16[16,2]{1,0} add(f16[16,2]{1,0} %slice.19, f16[16,2]{1,0} %broadcast.415), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd"}
  %convert.228 = f32[16,2]{1,0} convert(f16[16,2]{1,0} %add.142), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_4"}
  %param_0.562 = f32[16]{0} parameter(0)
  %broadcast.414 = f32[16,2]{1,0} broadcast(f32[16]{0} %param_0.562), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.29 = f32[16,2]{1,0} subtract(f32[16,2]{1,0} %convert.228, f32[16,2]{1,0} %broadcast.414), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %exponential.2 = f32[16,2]{1,0} exponential(f32[16,2]{1,0} %subtract.29), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
}

%fused_computation.180 (param_0.565: f16[8192,768], param_1.483: s32[16,76], param_2.339: s32[16]) -> f16[1216,768] {
  %param_0.565 = f16[8192,768]{1,0} parameter(0)
  %param_1.483 = s32[16,76]{1,0} parameter(1)
  %param_2.339 = s32[16]{0} parameter(2)
  %broadcast.422 = s32[16,76]{1,0} broadcast(s32[16]{0} %param_2.339), dimensions={0}, metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %add.146 = s32[16,76]{1,0} add(s32[16,76]{1,0} %param_1.483, s32[16,76]{1,0} %broadcast.422), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %bitcast.247 = s32[1216]{0} bitcast(s32[16,76]{1,0} %add.146), metadata={op_type="Reshape" op_name="model/bert_pretrainer/cls/predictions/Reshape_1"}
  ROOT %gather.3 = f16[1216,768]{1,0} gather(f16[8192,768]{1,0} %param_0.565, s32[1216]{0} %bitcast.247), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,768}, metadata={op_type="GatherV2" op_name="model/bert_pretrainer/cls/predictions/GatherV2"}
}

%fused_computation.182 (param_0.672: s32[16,76], param_1.616: f32[1216,30522], param_2.452: f32[1216], param_3.344: f32[16,76], param_4.247: f32[]) -> f16[1216,30528] {
  %param_4.247 = f32[] parameter(4)
  %constant_451 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %compare.71 = pred[] compare(f32[] %param_4.247, f32[] %constant_451), direction=EQ, metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %constant_453 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %divide.26 = f32[] divide(f32[] %constant_453, f32[] %param_4.247), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %select.66 = f32[] select(pred[] %compare.71, f32[] %constant_451, f32[] %divide.26), metadata={op_type="DivNoNan" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/div_no_nan/div_no_nan"}
  %broadcast.579 = f32[16,76]{1,0} broadcast(f32[] %select.66), dimensions={}, metadata={op_type="Tile" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Tile_1"}
  %param_3.344 = f32[16,76]{1,0} parameter(3)
  %convert.326 = s32[16,76]{1,0} convert(f32[16,76]{1,0} %param_3.344), metadata={op_type="Cast" op_name="model/Cast"}
  %convert.325 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %convert.326), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast"}
  %multiply.188 = f32[16,76]{1,0} multiply(f32[16,76]{1,0} %broadcast.579, f32[16,76]{1,0} %convert.325), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/mul/Mul"}
  %bitcast.261 = f32[1216]{0} bitcast(f32[16,76]{1,0} %multiply.188), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %broadcast.577 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %bitcast.261), dimensions={0}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %param_1.616 = f32[1216,30522]{1,0} parameter(1)
  %param_2.452 = f32[1216]{0} parameter(2)
  %broadcast.576 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_2.452), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %divide.25 = f32[1216,30522]{1,0} divide(f32[1216,30522]{1,0} %param_1.616, f32[1216,30522]{1,0} %broadcast.576), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %param_0.672 = s32[16,76]{1,0} parameter(0)
  %convert.324 = f32[16,76]{1,0} convert(s32[16,76]{1,0} %param_0.672), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_2"}
  %convert.323 = s64[16,76]{1,0} convert(f32[16,76]{1,0} %convert.324), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %bitcast.260 = s64[1216]{0} bitcast(s64[16,76]{1,0} %convert.323), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3"}
  %broadcast.575 = s64[1216,30522]{1,0} broadcast(s64[1216]{0} %bitcast.260), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %iota.16 = s64[1216,30522]{1,0} iota(), iota_dimension=1, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.70 = pred[1216,30522]{1,0} compare(s64[1216,30522]{1,0} %broadcast.575, s64[1216,30522]{1,0} %iota.16), direction=EQ, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.574 = f32[1216,30522]{1,0} broadcast(f32[] %constant_453), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.572 = f32[1216,30522]{1,0} broadcast(f32[] %constant_451), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.65 = f32[1216,30522]{1,0} select(pred[1216,30522]{1,0} %compare.70, f32[1216,30522]{1,0} %broadcast.574, f32[1216,30522]{1,0} %broadcast.572), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_450 = s64[] constant(0), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.571 = s64[1216]{0} broadcast(s64[] %constant_450), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.69 = pred[1216]{0} compare(s64[1216]{0} %broadcast.571, s64[1216]{0} %bitcast.260), direction=LE, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_449 = s64[] constant(30522), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.570 = s64[1216]{0} broadcast(s64[] %constant_449), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %compare.68 = pred[1216]{0} compare(s64[1216]{0} %bitcast.260, s64[1216]{0} %broadcast.570), direction=LT, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %and.13 = pred[1216]{0} and(pred[1216]{0} %compare.69, pred[1216]{0} %compare.68), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.569 = f32[1216]{0} broadcast(f32[] %constant_451), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_448 = f32[] constant(nan), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.568 = f32[1216]{0} broadcast(f32[] %constant_448), dimensions={}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %select.64 = f32[1216]{0} select(pred[1216]{0} %and.13, f32[1216]{0} %broadcast.569, f32[1216]{0} %broadcast.568), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %broadcast.567 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %select.64), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %add.174 = f32[1216,30522]{1,0} add(f32[1216,30522]{1,0} %select.65, f32[1216,30522]{1,0} %broadcast.567), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.43 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %divide.25, f32[1216,30522]{1,0} %add.174), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %multiply.187 = f32[1216,30522]{1,0} multiply(f32[1216,30522]{1,0} %broadcast.577, f32[1216,30522]{1,0} %subtract.43), metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/mul"}
  %convert.322 = f16[1216,30522]{1,0} convert(f32[1216,30522]{1,0} %multiply.187), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %constant_524 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  ROOT %pad.21 = f16[1216,30528]{1,0} pad(f16[1216,30522]{1,0} %convert.322, f16[] %constant_524), padding=0_0x0_6, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul"}
}

%fused_computation.183 (param_0.671: f16[16,12,512,64]) -> f32[16,12,64] {
  %param_0.671 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.176 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.671), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}
  %convert.357 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.176), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %bitcast.263 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.357), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %constant_523 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.61 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.263, f32[] %constant_523), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_key_add_Sum-reduction.1322, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
}

%fused_computation.184 (param_0.670: f16[16,12,512,64]) -> f32[16,12,64] {
  %param_0.670 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.178 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.670), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/Mul"}
  %convert.359 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.178), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %bitcast.265 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.359), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %constant_522 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.62 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.265, f32[] %constant_522), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_query_add_Sum-reduction.1352, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_value_add_Sum-reduction.1379 (x.1380: f32[], y.1381: f32[]) -> f32[] {
  %x.1380 = f32[] parameter(0)
  %y.1381 = f32[] parameter(1)
  ROOT %add.1382 = f32[] add(f32[] %x.1380, f32[] %y.1381)
}

%fused_computation.185 (param_0.669: f16[16,12,64,512]) -> f32[12,64] {
  %param_0.669 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.180 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.669), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}
  %convert.361 = f32[16,512,12,64]{1,3,2,0} convert(f16[16,512,12,64]{1,3,2,0} %transpose.180), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  %bitcast.267 = f32[16,12,64,512]{3,2,1,0} bitcast(f32[16,512,12,64]{1,3,2,0} %convert.361), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  %constant_521 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.63 = f32[12,64]{1,0} reduce(f32[16,12,64,512]{3,2,1,0} %bitcast.267, f32[] %constant_521), dimensions={0,3}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_value_add_Sum-reduction.1379, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
}

%fused_computation.186 (param_0.668: f16[16,12,512,64]) -> f32[16,12,64] {
  %param_0.668 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.182 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.668), dimensions={0,2,1,3}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}
  %convert.363 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.182), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %bitcast.269 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.363), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %constant_518 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.64 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.269, f32[] %constant_518), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_key_add_Sum-reduction.1582, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
}

%fused_computation.187 (param_0.667: f16[16,12,512,64]) -> f32[16,12,64] {
  %param_0.667 = f16[16,12,512,64]{3,2,1,0} parameter(0)
  %transpose.184 = f16[16,512,12,64]{3,1,2,0} transpose(f16[16,12,512,64]{3,2,1,0} %param_0.667), dimensions={0,2,1,3}, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/Mul"}
  %convert.367 = f32[16,512,12,64]{3,1,2,0} convert(f16[16,512,12,64]{3,1,2,0} %transpose.184), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %bitcast.271 = f32[16,12,512,64]{3,2,1,0} bitcast(f32[16,512,12,64]{3,1,2,0} %convert.367), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %constant_517 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.65 = f32[16,12,64]{2,1,0} reduce(f32[16,12,512,64]{3,2,1,0} %bitcast.271, f32[] %constant_517), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_query_add_Sum-reduction.1612, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
}

%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_value_add_Sum-reduction.1639 (x.1640: f32[], y.1641: f32[]) -> f32[] {
  %x.1640 = f32[] parameter(0)
  %y.1641 = f32[] parameter(1)
  ROOT %add.1642 = f32[] add(f32[] %x.1640, f32[] %y.1641)
}

%fused_computation.188 (param_0.666: f16[16,12,64,512]) -> f32[12,64] {
  %param_0.666 = f16[16,12,64,512]{3,2,1,0} parameter(0)
  %transpose.186 = f16[16,512,12,64]{1,3,2,0} transpose(f16[16,12,64,512]{3,2,1,0} %param_0.666), dimensions={0,3,1,2}, metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}
  %convert.370 = f32[16,512,12,64]{1,3,2,0} convert(f16[16,512,12,64]{1,3,2,0} %transpose.186), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %bitcast.273 = f32[16,12,64,512]{3,2,1,0} bitcast(f32[16,512,12,64]{1,3,2,0} %convert.370), metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %constant_515 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  ROOT %reduce.66 = f32[12,64]{1,0} reduce(f32[16,12,64,512]{3,2,1,0} %bitcast.273, f32[] %constant_515), dimensions={0,3}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_value_add_Sum-reduction.1639, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
}

%max_F32.1850 (lhs.1851: f32[], rhs.1852: f32[]) -> f32[] {
  %lhs.1851 = f32[] parameter(0)
  %rhs.1852 = f32[] parameter(1)
  ROOT %maximum.1853 = f32[] maximum(f32[] %lhs.1851, f32[] %rhs.1852)
}

%fused_computation.190 (param_0.665: f32[30522], param_1.615: f16[1216,30528]) -> f32[16,76] {
  %param_1.615 = f16[1216,30528]{1,0} parameter(1)
  %slice.33 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_1.615), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_0.665 = f32[30522]{0} parameter(0)
  %convert.374 = f16[30522]{0} convert(f32[30522]{0} %param_0.665), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.620 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.374), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.181 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.33, f16[1216,30522]{1,0} %broadcast.620), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.373 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.181), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %bitcast.275 = f32[16,76,30522]{2,1,0} bitcast(f32[1216,30522]{1,0} %convert.373), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/Cast_1"}
  %constant_514 = f32[] constant(-inf), metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  ROOT %reduce.67 = f32[16,76]{1,0} reduce(f32[16,76,30522]{2,1,0} %bitcast.275, f32[] %constant_514), dimensions={2}, to_apply=%max_F32.1850, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
}

%add_float_.991 (x.992: f32[], y.993: f32[]) -> f32[] {
  %x.992 = f32[] parameter(0)
  %y.993 = f32[] parameter(1)
  ROOT %add.994 = f32[] add(f32[] %x.992, f32[] %y.993)
}

%fused_computation.191 (param_0.673: f32[1216], param_1.617: f32[30522], param_2.453: f16[1216,30528]) -> (f32[1216], f32[1216,30522]) {
  %param_2.453 = f16[1216,30528]{1,0} parameter(2)
  %slice.29.clone.1 = f16[1216,30522]{1,0} slice(f16[1216,30528]{1,0} %param_2.453), slice={[0:1216], [0:30522]}, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %param_1.617 = f32[30522]{0} parameter(1)
  %convert.261.clone.1 = f16[30522]{0} convert(f32[30522]{0} %param_1.617), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/BiasAdd/Cast"}
  %broadcast.519.clone.1 = f16[1216,30522]{1,0} broadcast(f16[30522]{0} %convert.261.clone.1), dimensions={1}, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %add.168.clone.1 = f16[1216,30522]{1,0} add(f16[1216,30522]{1,0} %slice.29.clone.1, f16[1216,30522]{1,0} %broadcast.519.clone.1), metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/BiasAdd"}
  %convert.259.clone.1 = f32[1216,30522]{1,0} convert(f16[1216,30522]{1,0} %add.168.clone.1), metadata={op_type="Cast" op_name="model/bert_pretrain_loss_and_metric_layer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %param_0.673 = f32[1216]{0} parameter(0)
  %broadcast.518.clone.1 = f32[1216,30522]{1,0} broadcast(f32[1216]{0} %param_0.673), dimensions={0}, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %subtract.37.clone.1 = f32[1216,30522]{1,0} subtract(f32[1216,30522]{1,0} %convert.259.clone.1, f32[1216,30522]{1,0} %broadcast.518.clone.1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %exponential.3.clone.1 = f32[1216,30522]{1,0} exponential(f32[1216,30522]{1,0} %subtract.37.clone.1), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %constant_525 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.68 = f32[1216]{0} reduce(f32[1216,30522]{1,0} %exponential.3.clone.1, f32[] %constant_525), dimensions={1}, to_apply=%add_float_.991, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %tuple.19 = (f32[1216]{0}, f32[1216,30522]{1,0}) tuple(f32[1216]{0} %reduce.68, f32[1216,30522]{1,0} %exponential.3.clone.1)
}

%add_float_.765 (x.766: f32[], y.767: f32[]) -> f32[] {
  %x.766 = f32[] parameter(0)
  %y.767 = f32[] parameter(1)
  ROOT %add.768 = f32[] add(f32[] %x.766, f32[] %y.767)
}

%fused_computation.192 (param_0.664: f32[16,2]) -> (f32[16], f32[16]) {
  %param_0.664 = f32[16,2]{1,0} parameter(0)
  %constant_526 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.69 = f32[16]{0} reduce(f32[16,2]{1,0} %param_0.664, f32[] %constant_526), dimensions={1}, to_apply=%add_float_.765, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %log.0 = f32[16]{0} log(f32[16]{0} %reduce.69), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  ROOT %tuple.22 = (f32[16]{0}, f32[16]{0}) tuple(f32[16]{0} %log.0, f32[16]{0} %reduce.69)
}

%horizontally_fused_computation (param_0_0: f32[768], param_0_1: f32[768], param_1_0: f32[768], param_1_1: f32[768], param_2_0: f32[768], param_2_1: f32[768], param_3_0: f32[768], param_3_1: f32[768], param_4_0: f32[768], param_4_1: f32[768]) -> (f32[768], f32[768], f32[768], f32[768], f32[768]) {
  %param_0_1 = f32[768]{0} parameter(1)
  %constant_527 = f32[] constant(0.99), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %broadcast.624 = f32[768]{0} broadcast(f32[] %constant_527), dimensions={}, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %multiply.210 = f32[768]{0} multiply(f32[768]{0} %param_0_1, f32[768]{0} %broadcast.624), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %param_0_0 = f32[768]{0} parameter(0)
  %constant_528 = f32[] constant(0.01), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %broadcast.625 = f32[768]{0} broadcast(f32[] %constant_528), dimensions={}, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %multiply.211 = f32[768]{0} multiply(f32[768]{0} %param_0_0, f32[768]{0} %broadcast.625), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %add.184 = f32[768]{0} add(f32[768]{0} %multiply.210, f32[768]{0} %multiply.211), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %reshape.260 = f32[768]{0} reshape(f32[768]{0} %add.184)
  %param_1_1 = f32[768]{0} parameter(3)
  %multiply.212 = f32[768]{0} multiply(f32[768]{0} %param_1_1, f32[768]{0} %broadcast.624), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %param_1_0 = f32[768]{0} parameter(2)
  %multiply.213 = f32[768]{0} multiply(f32[768]{0} %param_1_0, f32[768]{0} %broadcast.625), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %add.185 = f32[768]{0} add(f32[768]{0} %multiply.212, f32[768]{0} %multiply.213), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %reshape.261 = f32[768]{0} reshape(f32[768]{0} %add.185)
  %param_2_1 = f32[768]{0} parameter(5)
  %multiply.214 = f32[768]{0} multiply(f32[768]{0} %param_2_1, f32[768]{0} %broadcast.624), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %param_2_0 = f32[768]{0} parameter(4)
  %multiply.215 = f32[768]{0} multiply(f32[768]{0} %param_2_0, f32[768]{0} %broadcast.625), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %add.186 = f32[768]{0} add(f32[768]{0} %multiply.214, f32[768]{0} %multiply.215), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %reshape.262 = f32[768]{0} reshape(f32[768]{0} %add.186)
  %param_3_1 = f32[768]{0} parameter(7)
  %multiply.216 = f32[768]{0} multiply(f32[768]{0} %param_3_1, f32[768]{0} %broadcast.624), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %param_3_0 = f32[768]{0} parameter(6)
  %multiply.217 = f32[768]{0} multiply(f32[768]{0} %param_3_0, f32[768]{0} %broadcast.625), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %add.187 = f32[768]{0} add(f32[768]{0} %multiply.216, f32[768]{0} %multiply.217), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %reshape.263 = f32[768]{0} reshape(f32[768]{0} %add.187)
  %param_4_1 = f32[768]{0} parameter(9)
  %multiply.218 = f32[768]{0} multiply(f32[768]{0} %param_4_1, f32[768]{0} %broadcast.624), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %param_4_0 = f32[768]{0} parameter(8)
  %multiply.219 = f32[768]{0} multiply(f32[768]{0} %param_4_0, f32[768]{0} %broadcast.625), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %add.188 = f32[768]{0} add(f32[768]{0} %multiply.218, f32[768]{0} %multiply.219), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %reshape.264 = f32[768]{0} reshape(f32[768]{0} %add.188)
  %concatenate = f32[3840]{0} concatenate(f32[768]{0} %reshape.260, f32[768]{0} %reshape.261, f32[768]{0} %reshape.262, f32[768]{0} %reshape.263, f32[768]{0} %reshape.264), dimensions={0}
  %slice.36 = f32[768]{0} slice(f32[3840]{0} %concatenate), slice={[0:768]}
  %slice.37 = f32[768]{0} slice(f32[3840]{0} %concatenate), slice={[768:1536]}
  %slice.38 = f32[768]{0} slice(f32[3840]{0} %concatenate), slice={[1536:2304]}
  %slice.39 = f32[768]{0} slice(f32[3840]{0} %concatenate), slice={[2304:3072]}
  %slice.40 = f32[768]{0} slice(f32[3840]{0} %concatenate), slice={[3072:3840]}
  ROOT %tuple.25 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %slice.36, f32[768]{0} %slice.37, f32[768]{0} %slice.38, f32[768]{0} %slice.39, f32[768]{0} %slice.40)
}

%horizontally_fused_computation.1 (param_0_0.1: f32[768], param_0_1.1: f32[768], param_1_0.1: f32[768], param_1_1.1: f32[768], param_2_0.1: f32[768], param_2_1.1: f32[768], param_3_0.1: f32[768], param_3_1.1: f32[768], param_4_0.1: f32[768], param_4_1.1: f32[768]) -> (f32[768], f32[768], f32[768], f32[768], f32[768]) {
  %param_0_1.1 = f32[768]{0} parameter(1)
  %constant_538 = f32[] constant(0.99), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %broadcast.638 = f32[768]{0} broadcast(f32[] %constant_538), dimensions={}, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %multiply.220 = f32[768]{0} multiply(f32[768]{0} %param_0_1.1, f32[768]{0} %broadcast.638), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %param_0_0.1 = f32[768]{0} parameter(0)
  %constant_539 = f32[] constant(-2)
  %broadcast.639 = f32[768]{0} broadcast(f32[] %constant_539), dimensions={}
  %power.12 = f32[768]{0} power(f32[768]{0} %param_0_0.1, f32[768]{0} %broadcast.639), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %constant_540 = f32[] constant(-1.001e-05)
  %broadcast.641 = f32[768]{0} broadcast(f32[] %constant_540), dimensions={}
  %add.189 = f32[768]{0} add(f32[768]{0} %power.12, f32[768]{0} %broadcast.641), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %constant_541 = f32[] constant(0.0100012207)
  %broadcast.642 = f32[768]{0} broadcast(f32[] %constant_541), dimensions={}
  %multiply.221 = f32[768]{0} multiply(f32[768]{0} %add.189, f32[768]{0} %broadcast.642), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %add.190 = f32[768]{0} add(f32[768]{0} %multiply.220, f32[768]{0} %multiply.221), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %reshape.265 = f32[768]{0} reshape(f32[768]{0} %add.190)
  %param_1_1.1 = f32[768]{0} parameter(3)
  %multiply.222 = f32[768]{0} multiply(f32[768]{0} %param_1_1.1, f32[768]{0} %broadcast.638), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %param_1_0.1 = f32[768]{0} parameter(2)
  %power.13 = f32[768]{0} power(f32[768]{0} %param_1_0.1, f32[768]{0} %broadcast.639), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %add.191 = f32[768]{0} add(f32[768]{0} %power.13, f32[768]{0} %broadcast.641), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %multiply.223 = f32[768]{0} multiply(f32[768]{0} %add.191, f32[768]{0} %broadcast.642), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %add.192 = f32[768]{0} add(f32[768]{0} %multiply.222, f32[768]{0} %multiply.223), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %reshape.266 = f32[768]{0} reshape(f32[768]{0} %add.192)
  %param_2_1.1 = f32[768]{0} parameter(5)
  %multiply.224 = f32[768]{0} multiply(f32[768]{0} %param_2_1.1, f32[768]{0} %broadcast.638), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %param_2_0.1 = f32[768]{0} parameter(4)
  %power.14 = f32[768]{0} power(f32[768]{0} %param_2_0.1, f32[768]{0} %broadcast.639), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %add.193 = f32[768]{0} add(f32[768]{0} %power.14, f32[768]{0} %broadcast.641), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %multiply.225 = f32[768]{0} multiply(f32[768]{0} %add.193, f32[768]{0} %broadcast.642), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %add.194 = f32[768]{0} add(f32[768]{0} %multiply.224, f32[768]{0} %multiply.225), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %reshape.267 = f32[768]{0} reshape(f32[768]{0} %add.194)
  %param_3_1.1 = f32[768]{0} parameter(7)
  %multiply.226 = f32[768]{0} multiply(f32[768]{0} %param_3_1.1, f32[768]{0} %broadcast.638), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %param_3_0.1 = f32[768]{0} parameter(6)
  %power.15 = f32[768]{0} power(f32[768]{0} %param_3_0.1, f32[768]{0} %broadcast.639), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %add.195 = f32[768]{0} add(f32[768]{0} %power.15, f32[768]{0} %broadcast.641), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %multiply.228 = f32[768]{0} multiply(f32[768]{0} %add.195, f32[768]{0} %broadcast.642), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %add.196 = f32[768]{0} add(f32[768]{0} %multiply.226, f32[768]{0} %multiply.228), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %reshape.268 = f32[768]{0} reshape(f32[768]{0} %add.196)
  %param_4_1.1 = f32[768]{0} parameter(9)
  %multiply.229 = f32[768]{0} multiply(f32[768]{0} %param_4_1.1, f32[768]{0} %broadcast.638), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %param_4_0.1 = f32[768]{0} parameter(8)
  %power.16 = f32[768]{0} power(f32[768]{0} %param_4_0.1, f32[768]{0} %broadcast.639), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %add.198 = f32[768]{0} add(f32[768]{0} %power.16, f32[768]{0} %broadcast.641), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %multiply.230 = f32[768]{0} multiply(f32[768]{0} %add.198, f32[768]{0} %broadcast.642), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %add.199 = f32[768]{0} add(f32[768]{0} %multiply.229, f32[768]{0} %multiply.230), metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %reshape.269 = f32[768]{0} reshape(f32[768]{0} %add.199)
  %concatenate.1 = f32[3840]{0} concatenate(f32[768]{0} %reshape.265, f32[768]{0} %reshape.266, f32[768]{0} %reshape.267, f32[768]{0} %reshape.268, f32[768]{0} %reshape.269), dimensions={0}
  %slice.41 = f32[768]{0} slice(f32[3840]{0} %concatenate.1), slice={[0:768]}
  %slice.42 = f32[768]{0} slice(f32[3840]{0} %concatenate.1), slice={[768:1536]}
  %slice.43 = f32[768]{0} slice(f32[3840]{0} %concatenate.1), slice={[1536:2304]}
  %slice.44 = f32[768]{0} slice(f32[3840]{0} %concatenate.1), slice={[2304:3072]}
  %slice.45 = f32[768]{0} slice(f32[3840]{0} %concatenate.1), slice={[3072:3840]}
  ROOT %tuple.26 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) tuple(f32[768]{0} %slice.41, f32[768]{0} %slice.42, f32[768]{0} %slice.43, f32[768]{0} %slice.44, f32[768]{0} %slice.45)
}

ENTRY %cluster_0__XlaCompiledKernel_true__XlaHasReferenceVars_false__XlaNumConstantArgs_123__XlaNumResourceArgs_156_.2342 (arg0.1: f16[16,512,3072], arg1.2: f16[16,512,3072], arg2.3: f16[16,12,512,512], arg3.4: f16[16,12,512,512], arg4.5: f16[16,512,768], arg5.6: f16[16,512,768], arg6.7: f16[16,512,768], arg7.8: f16[16,512,768], arg8.9: f16[16,512,768], arg9.10: s32[16,512], arg10.11: s32[16,76], arg11.12: s32[16,76], arg12.13: f32[16,76], arg13.14: s32[16,1], arg14.15: s32[16,512], arg15.16: s32[16,512], arg16.17: f32[], arg17.18: f32[], arg18.19: s64[], arg19.20: f32[768], arg20.21: f32[768,768], arg21.22: f32[768], arg22.23: f32[3072], arg23.24: f32[768,3072], arg24.25: f32[3072,768], arg25.26: f32[768], arg26.27: f32[768], arg27.28: f32[768], arg28.29: f32[768], arg29.30: f32[768], arg30.31: f32[12,64], arg31.32: f32[768,12,64], arg32.33: f32[12,64], arg33.34: f32[768,12,64], arg34.35: f32[12,64], arg35.36: f32[768], arg36.37: f32[3072], arg37.38: f32[768,3072], arg38.39: f32[3072,768], arg39.40: f32[768], arg40.41: f32[12,64], arg41.42: f32[768,12,64], arg42.43: f32[12,64], arg43.44: f32[768,12,64], arg44.45: f32[12,64], arg45.46: f32[512,768], arg46.47: f32[768], arg47.48: f32[768], arg48.49: f32[768], arg49.50: f32[768], arg50.51: f32[768,12,64], arg51.52: f32[12,64,768], arg52.53: f32[768], arg53.54: f32[768], arg54.55: f32[768], arg55.56: f32[768], arg56.57: f32[768], arg57.58: f32[768], arg58.59: f32[768], arg59.60: f32[768], arg60.61: f32[768,12,64], arg61.62: f32[12,64,768], arg62.63: f32[768], arg63.64: f32[768], arg64.65: f32[768], arg65.66: f32[768], arg66.67: f32[2,768], arg67.68: f32[2], arg68.69: f32[768,2], arg69.70: f32[30522], arg70.71: f32[30522,768], arg71.72: f32[768], arg72.73: f32[768], arg73.74: f32[768], arg74.75: f32[768,768], arg75.76: f32[], arg76.77: f32[], arg77.78: f32[2], arg78.79: f32[2], arg79.80: f32[768,2], arg80.81: f32[768,2], arg81.82: f32[768], arg82.83: f32[768], arg83.84: f32[768,768], arg84.85: f32[768,768], arg85.86: f32[], arg86.87: f32[30522], arg87.88: f32[30522], arg88.89: f32[], arg89.90: f32[768], arg90.91: f32[768], arg91.92: f32[], arg92.93: f32[768], arg93.94: f32[768], arg94.95: f32[768,768], arg95.96: f32[768,768], arg96.97: f32[768], arg97.98: f32[768], arg98.99: f32[768], arg99.100: f32[768], arg100.101: f32[768], arg101.102: f32[768], arg102.103: f32[3072,768], arg103.104: f32[3072,768], arg104.105: f32[768], arg105.106: f32[768], arg106.107: f32[768,3072], arg107.108: f32[768,3072], arg108.109: f32[3072], arg109.110: f32[3072], arg110.111: f32[768], arg111.112: f32[768], arg112.113: f32[768], arg113.114: f32[768], arg114.115: f32[12,64,768], arg115.116: f32[12,64,768], arg116.117: f32[768], arg117.118: f32[768], arg118.119: f32[768,12,64], arg119.120: f32[768,12,64], arg120.121: f32[12,64], arg121.122: f32[12,64], arg122.123: f32[768,12,64], arg123.124: f32[768,12,64], arg124.125: f32[12,64], arg125.126: f32[12,64], arg126.127: f32[768,12,64], arg127.128: f32[768,12,64], arg128.129: f32[12,64], arg129.130: f32[12,64], arg130.131: f32[768], arg131.132: f32[768], arg132.133: f32[768], arg133.134: f32[768], arg134.135: f32[3072,768], arg135.136: f32[3072,768], arg136.137: f32[768], arg137.138: f32[768], arg138.139: f32[768,3072], arg139.140: f32[768,3072], arg140.141: f32[3072], arg141.142: f32[3072], arg142.143: f32[768], arg143.144: f32[768], arg144.145: f32[768], arg145.146: f32[768], arg146.147: f32[12,64,768], arg147.148: f32[12,64,768], arg148.149: f32[768], arg149.150: f32[768], arg150.151: f32[12,64], arg151.152: f32[12,64], arg152.153: f32[768,12,64], arg153.154: f32[768,12,64], arg154.155: f32[768,12,64], arg155.156: f32[768,12,64], arg156.157: f32[12,64], arg157.158: f32[12,64], arg158.159: f32[768,12,64], arg159.160: f32[768,12,64], arg160.161: f32[12,64], arg161.162: f32[12,64], arg162.163: f32[768], arg163.164: f32[768], arg164.165: f32[768], arg165.166: f32[768], arg166.167: f32[2,768], arg167.168: f32[2,768], arg168.169: f32[30522,768], arg169.170: f32[30522,768], arg170.171: f32[512,768], arg171.172: f32[512,768]) -> (f32[], f32[768], f32[768,768], f32[768], f32[3072], f32[768,3072], f32[3072,768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[12,64], f32[768,12,64], f32[12,64], f32[768,12,64], f32[12,64], f32[768], f32[3072], f32[768,3072], f32[3072,768], f32[768], f32[12,64], f32[768,12,64], f32[12,64], f32[768,12,64], f32[12,64], f32[512,768], f32[768], f32[768], f32[768], f32[768], f32[768,12,64], f32[12,64,768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768,12,64], f32[12,64,768], f32[768], f32[768], f32[768], f32[768], f32[2,768], f32[2], f32[768,2], f32[30522], f32[30522,768], f32[768], f32[768], f32[768], f32[768,768], f32[], f32[], f32[2], f32[2], f32[768,2], f32[768,2], f32[768], f32[768], f32[768,768], f32[768,768], f32[], f32[30522], f32[30522], f32[], f32[768], f32[768], f32[], f32[768], f32[768], f32[768,768], f32[768,768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[768], f32[3072,768], f32[3072,768], f32[768], f32[768], f32[768,3072], f32[768,3072], f32[3072], f32[3072], f32[768], f32[768], f32[768], f32[768], f32[12,64,768], f32[12,64,768], f32[768], f32[768], f32[768,12,64], f32[768,12,64], f32[12,64], f32[12,64], f32[768,12,64], f32[768,12,64], f32[12,64], f32[12,64], f32[768,12,64], f32[768,12,64], f32[12,64], f32[12,64], f32[768], f32[768], f32[768], f32[768], f32[3072,768], f32[3072,768], f32[768], f32[768], f32[768,3072], f32[768,3072], f32[3072], f32[3072], f32[768], f32[768], f32[768], f32[768], f32[12,64,768], f32[12,64,768], f32[768], f32[768], f32[12,64], f32[12,64], f32[768,12,64], f32[768,12,64], f32[768,12,64], f32[768,12,64], f32[12,64], f32[12,64], f32[768,12,64], f32[768,12,64], f32[12,64], f32[12,64], f32[768], f32[768], f32[768], f32[768], f32[2,768], f32[2,768], f32[30522,768], f32[30522,768], f32[512,768], f32[512,768]) {
  %constant_190 = f32[] constant(1), metadata={op_type="Minimum" op_name="Adam/PolynomialDecay/Minimum"}
  %copy.118 = f32[] copy(f32[] %constant_190)
  %arg19.20 = f32[768]{0} parameter(19), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg81.82 = f32[768]{0} parameter(81), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg82.83 = f32[768]{0} parameter(82), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg18.19 = s64[] parameter(18), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg17.18 = f32[] parameter(17), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg16.17 = f32[] parameter(16), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.157 = (f32[], f32[], f32[]) fusion(s64[] %arg18.19, f32[] %arg17.18, f32[] %arg16.17), kind=kLoop, calls=%fused_computation.157, metadata={op_type="Mul" op_name="Adam/PolynomialDecay/Mul"}
  %get-tuple-element.98 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.157), index=2
  %get-tuple-element.97 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.157), index=1
  %get-tuple-element.96 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.157), index=0
  %constant_800 = f32[] constant(1e-06), metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_40/update_0/ResourceApplyAdam"}
  %arg67.68 = f32[2]{0} parameter(67), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg15.16 = s32[16,512]{1,0} parameter(15), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.153 = f16[8192,8]{1,0} fusion(s32[16,512]{1,0} %arg15.16), kind=kLoop, calls=%fused_computation.153, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
  %arg66.67 = f32[2,768]{1,0} parameter(66), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.152 = f16[8,768]{1,0} fusion(f32[2,768]{1,0} %arg66.67), kind=kLoop, calls=%fused_computation.152, metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}
  %custom-call.10 = f16[8192,768]{1,0} custom-call(f16[8192,8]{1,0} %fusion.153, f16[8,768]{1,0} %fusion.152), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %arg45.46 = f32[512,768]{1,0} parameter(45), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg14.15 = s32[16,512]{1,0} parameter(14), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg70.71 = f32[30522,768]{1,0} parameter(70), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.151 = f32[1,768,16,512]{1,3,2,0} fusion(f16[8192,768]{1,0} %custom-call.10, f32[512,768]{1,0} %arg45.46, s32[16,512]{1,0} %arg14.15, f32[30522,768]{1,0} %arg70.71), kind=kLoop, calls=%fused_computation.151, metadata={op_type="Transpose" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormGradV3-1-TransposeNHWCToNCHW-LayoutOptimizer"}
  %arg46.47 = f32[768]{0} parameter(46), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg47.48 = f32[768]{0} parameter(47), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant = f32[] constant(1.001e-05)
  %constant_196 = s64[] constant(1), metadata={op_type="AddV2" op_name="Adam/add"}
  %custom-call = (f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) custom-call(f32[1,768,16,512]{1,3,2,0} %fusion.151, f32[768]{0} %arg46.47, f32[768]{0} %arg47.48, f32[] %constant, s64[] %constant_196), custom_call_target="__cudnn$batchNormalizationForwardTraining"
  %get-tuple-element.1 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call), index=0, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %arg8.9 = f16[16,512,768]{2,1,0} parameter(8), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.150 = f16[8192,768]{1,0} fusion(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.1, f16[16,512,768]{2,1,0} %arg8.9), kind=kLoop, calls=%fused_computation.150
  %arg41.42 = f32[768,12,64]{2,1,0} parameter(41), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.171 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg41.42), kind=kLoop, calls=%fused_computation.171
  %custom-call.11 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.150, f16[768,768]{1,0} %fusion.171), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg40.41 = f32[12,64]{1,0} parameter(40), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.149 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.11, f32[12,64]{1,0} %arg40.41), kind=kLoop, calls=%fused_computation.149
  %arg43.44 = f32[768,12,64]{2,1,0} parameter(43), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.170 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg43.44), kind=kLoop, calls=%fused_computation.170
  %custom-call.12 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.150, f16[768,768]{1,0} %fusion.170), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg42.43 = f32[12,64]{1,0} parameter(42), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.175 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.12, f32[12,64]{1,0} %arg42.43), kind=kLoop, calls=%fused_computation.175
  %custom-call.13 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.149, f16[16,12,64,512]{3,2,1,0} %fusion.175), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %arg9.10 = s32[16,512]{1,0} parameter(9), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.146 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{3,2,1,0} %custom-call.13, s32[16,512]{1,0} %arg9.10), kind=kLoop, calls=%fused_computation.146, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/add"}
  %bitcast.1 = f16[16,12,512,512]{3,2,1,0} bitcast(f16[16,12,512,512]{2,3,1,0} %fusion.146), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %constant_412 = f16[] constant(-inf), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %reduce.3 = f16[16,12,512]{2,1,0} reduce(f16[16,12,512,512]{3,2,1,0} %bitcast.1, f16[] %constant_412), dimensions={2}, to_apply=%max_half_.413, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %fusion.145 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.146, f16[16,12,512]{2,1,0} %reduce.3), kind=kLoop, calls=%fused_computation.145, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %fusion.144 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.145), kind=kLoop, calls=%fused_computation.144, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %constant_208 = f32[] constant(0), metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_3"}
  %reduce.4 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.144, f32[] %constant_208), dimensions={2}, to_apply=%add_float_.423, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Softmax"}
  %arg3.4 = f16[16,12,512,512]{3,2,1,0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.142 = f16[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.145, f32[16,12,512]{2,1,0} %reduce.4, f16[16,12,512,512]{3,2,1,0} %arg3.4), kind=kLoop, calls=%fused_computation.142, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/dropout_3/dropout/Mul_1"}
  %arg50.51 = f32[768,12,64]{2,1,0} parameter(50), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.169 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg50.51), kind=kLoop, calls=%fused_computation.169
  %custom-call.14 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.150, f16[768,768]{1,0} %fusion.169), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg44.45 = f32[12,64]{1,0} parameter(44), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.176 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.14, f32[12,64]{1,0} %arg44.45), kind=kLoop, calls=%fused_computation.176
  %custom-call.15 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.142, f16[16,12,512,64]{3,2,1,0} %fusion.176), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.140 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.15), kind=kLoop, calls=%fused_computation.140
  %arg51.52 = f32[12,64,768]{2,1,0} parameter(51), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.168 = f16[768,768]{1,0} fusion(f32[12,64,768]{2,1,0} %arg51.52), kind=kLoop, calls=%fused_computation.168
  %custom-call.16 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.140, f16[768,768]{1,0} %fusion.168), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg39.40 = f32[768]{0} parameter(39), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg7.8 = f16[16,512,768]{2,1,0} parameter(7), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.392 = f16[1,768,16,512]{1,3,2,0} convert(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.1), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast_1"}
  %arg52.53 = f32[768]{0} parameter(52), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg53.54 = f32[768]{0} parameter(53), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.74 = (f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) custom-call(f16[8192,768]{1,0} %custom-call.16, f32[768]{0} %arg39.40, f16[16,512,768]{2,1,0} %arg7.8, f16[1,768,16,512]{1,3,2,0} %convert.392, f32[768]{0} %arg52.53, f32[768]{0} %arg53.54, f32[] %constant, s64[] %constant_196), custom_call_target="NdpFusedForwardBertOutput"
  %get-tuple-element.30 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.74), index=0
  %arg37.38 = f32[768,3072]{1,0} parameter(37), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.276 = f16[768,3072]{1,0} convert(f32[768,3072]{1,0} %arg37.38), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum/Cast"}
  %custom-call.17 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.30, f16[768,3072]{1,0} %convert.276), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg36.37 = f32[3072]{0} parameter(36), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.138 = f16[16,512,3072]{2,1,0} fusion(f16[8192,3072]{1,0} %custom-call.17, f32[3072]{0} %arg36.37), kind=kLoop, calls=%fused_computation.138, metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/activation/Gelu/Tanh"}
  %arg1.2 = f16[16,512,3072]{2,1,0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.137 = f16[8192,3072]{1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.138, f16[16,512,3072]{2,1,0} %arg1.2, f16[8192,3072]{1,0} %custom-call.17, f32[3072]{0} %arg36.37), kind=kLoop, calls=%fused_computation.137
  %arg38.39 = f32[3072,768]{1,0} parameter(38), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.278 = f16[3072,768]{1,0} convert(f32[3072,768]{1,0} %arg38.39), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum/Cast"}
  %custom-call.18 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.137, f16[3072,768]{1,0} %convert.278), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %arg35.36 = f32[768]{0} parameter(35), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg6.7 = f16[16,512,768]{2,1,0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.34 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.74), index=4
  %arg56.57 = f32[768]{0} parameter(56), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg57.58 = f32[768]{0} parameter(57), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.75 = (f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f16[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) custom-call(f16[8192,768]{1,0} %custom-call.18, f32[768]{0} %arg35.36, f16[16,512,768]{2,1,0} %arg6.7, f32[1,768,16,512]{1,3,2,0} %get-tuple-element.34, f32[768]{0} %arg56.57, f32[768]{0} %arg57.58, f32[] %constant, s64[] %constant_196), custom_call_target="NdpFusedForwardBertOutput"
  %get-tuple-element.35 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f16[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.75), index=0
  %arg31.32 = f32[768,12,64]{2,1,0} parameter(31), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.167 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg31.32), kind=kLoop, calls=%fused_computation.167
  %custom-call.19 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.35, f16[768,768]{1,0} %fusion.167), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg30.31 = f32[12,64]{1,0} parameter(30), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.136 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.19, f32[12,64]{1,0} %arg30.31), kind=kLoop, calls=%fused_computation.136
  %arg33.34 = f32[768,12,64]{2,1,0} parameter(33), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.166 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg33.34), kind=kLoop, calls=%fused_computation.166
  %custom-call.20 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.35, f16[768,768]{1,0} %fusion.166), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg32.33 = f32[12,64]{1,0} parameter(32), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.177 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.20, f32[12,64]{1,0} %arg32.33), kind=kLoop, calls=%fused_computation.177
  %custom-call.21 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.136, f16[16,12,64,512]{3,2,1,0} %fusion.177), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.134 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{3,2,1,0} %custom-call.21, s32[16,512]{1,0} %arg9.10), kind=kLoop, calls=%fused_computation.134, metadata={op_type="AddV2" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/add"}
  %bitcast.3 = f16[16,12,512,512]{3,2,1,0} bitcast(f16[16,12,512,512]{2,3,1,0} %fusion.134), metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %reduce.5 = f16[16,12,512]{2,1,0} reduce(f16[16,12,512,512]{3,2,1,0} %bitcast.3, f16[] %constant_412), dimensions={2}, to_apply=%max_half_.557, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %fusion.133 = f16[16,12,512,512]{2,3,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.134, f16[16,12,512]{2,1,0} %reduce.5), kind=kLoop, calls=%fused_computation.133, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %fusion.132 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.133), kind=kLoop, calls=%fused_computation.132, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %reduce.6 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.132, f32[] %constant_208), dimensions={2}, to_apply=%add_float_.567, metadata={op_type="Softmax" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Softmax"}
  %arg2.3 = f16[16,12,512,512]{3,2,1,0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.130 = f16[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.133, f32[16,12,512]{2,1,0} %reduce.6, f16[16,12,512,512]{3,2,1,0} %arg2.3), kind=kLoop, calls=%fused_computation.130, metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/dropout_3/dropout/Mul_1"}
  %arg60.61 = f32[768,12,64]{2,1,0} parameter(60), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.165 = f16[768,768]{1,0} fusion(f32[768,12,64]{2,1,0} %arg60.61), kind=kLoop, calls=%fused_computation.165
  %custom-call.22 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.35, f16[768,768]{1,0} %fusion.165), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg34.35 = f32[12,64]{1,0} parameter(34), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.178 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.22, f32[12,64]{1,0} %arg34.35), kind=kLoop, calls=%fused_computation.178
  %custom-call.23 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.130, f16[16,12,512,64]{3,2,1,0} %fusion.178), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.128 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.23), kind=kLoop, calls=%fused_computation.128
  %arg61.62 = f32[12,64,768]{2,1,0} parameter(61), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.164 = f16[768,768]{1,0} fusion(f32[12,64,768]{2,1,0} %arg61.62), kind=kLoop, calls=%fused_computation.164
  %custom-call.24 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.128, f16[768,768]{1,0} %fusion.164), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg29.30 = f32[768]{0} parameter(29), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg5.6 = f16[16,512,768]{2,1,0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.39 = f16[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f16[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.75), index=4
  %arg64.65 = f32[768]{0} parameter(64), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg65.66 = f32[768]{0} parameter(65), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.76 = (f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) custom-call(f16[8192,768]{1,0} %custom-call.24, f32[768]{0} %arg29.30, f16[16,512,768]{2,1,0} %arg5.6, f16[1,768,16,512]{1,3,2,0} %get-tuple-element.39, f32[768]{0} %arg64.65, f32[768]{0} %arg65.66, f32[] %constant, s64[] %constant_196), custom_call_target="NdpFusedForwardBertOutput"
  %get-tuple-element.40 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.76), index=0
  %arg23.24 = f32[768,3072]{1,0} parameter(23), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.313 = f16[768,3072]{1,0} convert(f32[768,3072]{1,0} %arg23.24), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum/Cast"}
  %custom-call.25 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.40, f16[768,3072]{1,0} %convert.313), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"104\"}"
  %arg22.23 = f32[3072]{0} parameter(22), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.126 = f16[16,512,3072]{2,1,0} fusion(f16[8192,3072]{1,0} %custom-call.25, f32[3072]{0} %arg22.23), kind=kLoop, calls=%fused_computation.126, metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/activation/Gelu/Tanh"}
  %arg0.1 = f16[16,512,3072]{2,1,0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.125 = f16[8192,3072]{1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.126, f16[16,512,3072]{2,1,0} %arg0.1, f16[8192,3072]{1,0} %custom-call.25, f32[3072]{0} %arg22.23), kind=kLoop, calls=%fused_computation.125
  %arg24.25 = f32[3072,768]{1,0} parameter(24), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.315 = f16[3072,768]{1,0} convert(f32[3072,768]{1,0} %arg24.25), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum/Cast"}
  %custom-call.26 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.125, f16[3072,768]{1,0} %convert.315), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %arg21.22 = f32[768]{0} parameter(21), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg4.5 = f16[16,512,768]{2,1,0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.44 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.76), index=4
  %arg27.28 = f32[768]{0} parameter(27), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg28.29 = f32[768]{0} parameter(28), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.77 = (f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) custom-call(f16[8192,768]{1,0} %custom-call.26, f32[768]{0} %arg21.22, f16[16,512,768]{2,1,0} %arg4.5, f32[1,768,16,512]{1,3,2,0} %get-tuple-element.44, f32[768]{0} %arg27.28, f32[768]{0} %arg28.29, f32[] %constant, s64[] %constant_196), custom_call_target="NdpFusedForwardBertOutput"
  %get-tuple-element.49 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.77), index=4
  %fusion.124 = f16[16,768]{1,0} fusion(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.49), kind=kLoop, calls=%fused_computation.124, metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/Cast_3"}
  %arg20.21 = f32[768,768]{1,0} parameter(20), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.246 = f16[768,768]{1,0} convert(f32[768,768]{1,0} %arg20.21), metadata={op_type="Cast" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul/Cast"}
  %fusion.123 = f16[16,768]{1,0} fusion(f32[768]{0} %arg19.20), kind=kLoop, calls=%fused_computation.123, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd"}
  %custom-call.28 = f16[16,768]{1,0} custom-call(f16[16,768]{1,0} %fusion.124, f16[768,768]{1,0} %convert.246, f16[16,768]{1,0} %fusion.123), custom_call_target="__cublas$gemm", metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"102\"}"
  %tanh.694 = f16[16,768]{1,0} tanh(f16[16,768]{1,0} %custom-call.28), metadata={op_type="Tanh" op_name="model/bert_pretrainer/bert_encoder_1/pooler_transform/Tanh"}
  %arg68.69 = f32[768,2]{1,0} parameter(68), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.122 = f16[768,8]{1,0} fusion(f32[768,2]{1,0} %arg68.69), kind=kLoop, calls=%fused_computation.122, metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %custom-call.29 = f16[16,8]{1,0} custom-call(f16[16,768]{1,0} %tanh.694, f16[768,8]{1,0} %fusion.122), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"112\"}"
  %fusion.120 = f32[16]{0} fusion(f32[2]{0} %arg67.68, f16[16,8]{1,0} %custom-call.29), kind=kLoop, calls=%fused_computation.120, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.179 = f32[16,2]{1,0} fusion(f32[16]{0} %fusion.120, f32[2]{0} %arg67.68, f16[16,8]{1,0} %custom-call.29), kind=kLoop, calls=%fused_computation.179, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.192 = (f32[16]{0}, f32[16]{0}) fusion(f32[16,2]{1,0} %fusion.179), kind=kLoop, calls=%fused_computation.192, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits_1/SparseSoftmaxCrossEntropyWithLogits"}
  %get-tuple-element.95 = f32[16]{0} get-tuple-element((f32[16]{0}, f32[16]{0}) %fusion.192), index=1
  %arg13.14 = s32[16,1]{1,0} parameter(13), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.117 = f16[16,2]{1,0} fusion(f32[16,2]{1,0} %fusion.179, f32[16]{0} %get-tuple-element.95, s32[16,1]{1,0} %arg13.14), kind=kLoop, calls=%fused_computation.117, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrain_loss_and_metric_layer/Cast_4/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %constant_257 = f16[] constant(0), metadata={op_type="GreaterEqual" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/dropout_1/dropout/GreaterEqual"}
  %pad.4 = f16[16,8]{1,0} pad(f16[16,2]{1,0} %fusion.117, f16[] %constant_257), padding=0_0x0_6, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}
  %custom-call.30 = f16[16,768]{1,0} custom-call(f16[16,8]{1,0} %pad.4, f16[768,8]{1,0} %fusion.122), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.116 = f16[16,768]{1,0} fusion(f16[16,768]{1,0} %custom-call.30, f16[16,768]{1,0} %tanh.694), kind=kLoop, calls=%fused_computation.116, metadata={op_type="TanhGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/TanhGrad"}
  %fusion.115 = f32[768]{0} fusion(f16[16,768]{1,0} %fusion.116), kind=kLoop, calls=%fused_computation.115, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/BiasAdd/BiasAddGrad"}
  %custom-call.82 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg19.20, f32[768]{0} %arg81.82, f32[768]{0} %arg82.83, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %fusion.115), custom_call_target="NdpAdam$GemmBias$reduce.820", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_38/update_0/ResourceApplyAdam"}
  %get-tuple-element.825 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.82), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_38/update_0/ResourceApplyAdam"}
  %arg83.84 = f32[768,768]{1,0} parameter(83), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg84.85 = f32[768,768]{1,0} parameter(84), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.31 = f16[768,768]{1,0} custom-call(f16[16,768]{1,0} %fusion.124, f16[16,768]{1,0} %fusion.116), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"115\"}"
  %custom-call.83 = (f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) custom-call(f32[768,768]{1,0} %arg20.21, f32[768,768]{1,0} %arg83.84, f32[768,768]{1,0} %arg84.85, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[768,768]{1,0} %custom-call.31), custom_call_target="NdpAdam$Gemm$custom-call.31", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_37/update_0/ResourceApplyAdam"}
  %get-tuple-element.840 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) %custom-call.83), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_37/update_0/ResourceApplyAdam"}
  %arg104.105 = f32[768]{0} parameter(104), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg105.106 = f32[768]{0} parameter(105), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.48 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.77), index=3
  %get-tuple-element.46 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.77), index=1
  %get-tuple-element.47 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.77), index=2
  %constant_242 = f16[] constant(1.1113), metadata={op_type="Mul" op_name="model/bert_pretrainer/bert_encoder_1/dropout/dropout/Mul"}
  %get-tuple-element.45 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.77), index=0
  %arg11.12 = s32[16,76]{1,0} parameter(11), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant_47 = s32[16]{0} constant({...}), metadata={op_type="AddV2" op_name="model/bert_pretrainer/cls/predictions/add"}
  %fusion.180 = f16[1216,768]{1,0} fusion(f16[8192,768]{1,0} %get-tuple-element.45, s32[16,76]{1,0} %arg11.12, s32[16]{0} %constant_47), kind=kLoop, calls=%fused_computation.180, metadata={op_type="GatherV2" op_name="model/bert_pretrainer/cls/predictions/GatherV2"}
  %arg74.75 = f32[768,768]{1,0} parameter(74), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %convert.880 = f16[768,768]{1,0} convert(f32[768,768]{1,0} %arg74.75), metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/dense/MatMul/Cast"}
  %arg73.74 = f32[768]{0} parameter(73), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.113 = f16[1216,768]{1,0} fusion(f32[768]{0} %arg73.74), kind=kLoop, calls=%fused_computation.113, metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd"}
  %custom-call.33 = f16[1216,768]{1,0} custom-call(f16[1216,768]{1,0} %fusion.180, f16[768,768]{1,0} %convert.880, f16[1216,768]{1,0} %fusion.113), custom_call_target="__cublas$gemm", metadata={op_type="BiasAdd" op_name="model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"115\"}"
  %fusion.109 = (f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) fusion(f16[1216,768]{1,0} %custom-call.33), kind=kInput, calls=%fused_computation.109, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/mean"}
  %get-tuple-element.93 = f16[1216,768]{1,0} get-tuple-element((f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) %fusion.109), index=2
  %arg72.73 = f32[768]{0} parameter(72), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.91 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) %fusion.109), index=0
  %get-tuple-element.92 = f16[1216,768]{1,0} get-tuple-element((f32[1216]{0}, f16[1216,768]{1,0}, f16[1216,768]{1,0}) %fusion.109), index=1
  %fusion.106 = f32[1216]{0} fusion(f32[1216]{0} %get-tuple-element.91, f16[1216,768]{1,0} %get-tuple-element.92), kind=kInput, calls=%fused_computation.106, metadata={op_type="Mean" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/moments/variance"}
  %arg10.11 = s32[16,76]{1,0} parameter(10), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg69.70 = f32[30522]{0} parameter(69), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg71.72 = f32[768]{0} parameter(71), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.104 = f16[1216,768]{1,0} fusion(f32[768]{0} %arg71.72, f16[1216,768]{1,0} %get-tuple-element.92, f32[768]{0} %arg72.73, f32[1216]{0} %get-tuple-element.91, f32[1216]{0} %fusion.106), kind=kLoop, calls=%fused_computation.104, metadata={op_type="Cast" op_name="model/bert_pretrainer/cls/predictions/transform/LayerNorm/Cast_1"}
  %fusion.162 = f16[30528,768]{1,0} fusion(f32[30522,768]{1,0} %arg70.71), kind=kLoop, calls=%fused_computation.162, metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}
  %custom-call.34 = f16[1216,30528]{1,0} custom-call(f16[1216,768]{1,0} %fusion.104, f16[30528,768]{1,0} %fusion.162), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="model/bert_pretrainer/cls/predictions/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.102 = f32[1216]{0} fusion(f32[30522]{0} %arg69.70, f16[1216,30528]{1,0} %custom-call.34), kind=kInput, calls=%fused_computation.102, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.191 = (f32[1216]{0}, f32[1216,30522]{1,0}) fusion(f32[1216]{0} %fusion.102, f32[30522]{0} %arg69.70, f16[1216,30528]{1,0} %custom-call.34), kind=kInput, calls=%fused_computation.191, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %get-tuple-element.90 = f32[1216,30522]{1,0} get-tuple-element((f32[1216]{0}, f32[1216,30522]{1,0}) %fusion.191), index=1
  %get-tuple-element.89 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f32[1216,30522]{1,0}) %fusion.191), index=0
  %arg12.13 = f32[16,76]{1,0} parameter(12), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.190 = f32[16,76]{1,0} fusion(f32[30522]{0} %arg69.70, f16[1216,30528]{1,0} %custom-call.34), kind=kInput, calls=%fused_computation.190, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %fusion.3 = s64[16,76]{1,0} fusion(f32[16,76]{1,0} %fusion.190, f32[30522]{0} %arg69.70, f16[1216,30528]{1,0} %custom-call.34), kind=kInput, calls=%fused_computation.3, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax"}
  %log.997 = f32[1216]{0} log(f32[1216]{0} %get-tuple-element.89), metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.7 = f32[1216]{0} fusion(f32[1216]{0} %log.997, s32[16,76]{1,0} %arg10.11, f32[1216]{0} %fusion.102, f32[30522]{0} %arg69.70, f16[1216,30528]{1,0} %custom-call.34), kind=kInput, calls=%fused_computation.7, metadata={op_type="SparseSoftmaxCrossEntropyWithLogits" op_name="model/bert_pretrain_loss_and_metric_layer/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits"}
  %fusion.2 = (f32[], f32[], f32[]) fusion(s32[16,76]{1,0} %arg10.11, s64[16,76]{1,0} %fusion.3, f32[16,76]{1,0} %arg12.13, f32[1216]{0} %fusion.7), kind=kInput, calls=%fused_computation.2, metadata={op_type="Sum" op_name="model/bert_pretrain_loss_and_metric_layer/Sum_2"}
  %get-tuple-element.87 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.2), index=1
  %fusion.182 = f16[1216,30528]{1,0} fusion(s32[16,76]{1,0} %arg10.11, f32[1216,30522]{1,0} %get-tuple-element.90, f32[1216]{0} %get-tuple-element.89, f32[16,76]{1,0} %arg12.13, f32[] %get-tuple-element.87), kind=kLoop, calls=%fused_computation.182, metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul"}
  %custom-call.35 = f16[1216,768]{1,0} custom-call(f16[1216,30528]{1,0} %fusion.182, f16[30528,768]{1,0} %fusion.162), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.95 = (f32[1216]{0}, f32[1216]{0}) fusion(f32[768]{0} %arg72.73, f32[1216]{0} %fusion.106, f16[1216,768]{1,0} %custom-call.35, f32[1216]{0} %get-tuple-element.91, f16[1216,768]{1,0} %get-tuple-element.92), kind=kInput, calls=%fused_computation.95, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/LayerNorm/batchnorm/mul_2/Sum"}
  %get-tuple-element.83 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f32[1216]{0}) %fusion.95), index=0
  %get-tuple-element.84 = f32[1216]{0} get-tuple-element((f32[1216]{0}, f32[1216]{0}) %fusion.95), index=1
  %fusion.11 = (f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) fusion(f16[1216,768]{1,0} %get-tuple-element.93, f16[1216,768]{1,0} %custom-call.33, f32[1216]{0} %get-tuple-element.83, f32[1216]{0} %get-tuple-element.91, f16[1216,768]{1,0} %get-tuple-element.92, f32[1216]{0} %get-tuple-element.84, f32[768]{0} %arg72.73, f32[1216]{0} %fusion.106, f16[1216,768]{1,0} %custom-call.35), kind=kInput, calls=%fused_computation.11, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/BiasAdd/BiasAddGrad"}
  %get-tuple-element.80 = f16[1216,768]{1,0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.11), index=1
  %custom-call.36 = f16[1216,768]{1,0} custom-call(f16[1216,768]{1,0} %get-tuple-element.80, f16[768,768]{1,0} %convert.880), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.93 = f16[8192,768]{1,0} fusion(f16[1216,768]{1,0} %custom-call.36, s32[16,76]{1,0} %arg11.12, s32[16]{0} %constant_47), kind=kInput, calls=%fused_computation.93, metadata={op_type="UnsortedSegmentSum" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/UnsortedSegmentSum"}
  %convert.7 = f32[8192,768]{1,0} convert(f16[8192,768]{1,0} %fusion.93), metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/Cast/Cast"}
  %custom-call.37 = f16[16,768]{1,0} custom-call(f16[16,768]{1,0} %fusion.116, f16[768,768]{1,0} %convert.246), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/pooler_transform/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"115\"}"
  %fusion.92 = f32[16,512,768]{2,1,0} fusion(f16[16,768]{1,0} %custom-call.37), kind=kLoop, calls=%fused_computation.92, metadata={op_type="StridedSliceGrad" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/tf.__operators__.getitem/strided_slice/StridedSliceGrad"}
  %custom-call.80 = (f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) custom-call(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.48, f32[768]{0} %arg27.28, f32[768]{0} %get-tuple-element.46, f32[768]{0} %get-tuple-element.47, f16[16,512,768]{2,1,0} %arg4.5, f16[] %constant_242, f32[8192,768]{1,0} %convert.7, f32[16,512,768]{2,1,0} %fusion.92), custom_call_target="NdpFusedBackwardBertOutput$Normal"
  %get-tuple-element.66 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.80), index=2
  %custom-call.84 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg21.22, f32[768]{0} %arg104.105, f32[768]{0} %arg105.106, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.66), custom_call_target="NdpAdam$Batchnorm$custom-call.80", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_34/update_0/ResourceApplyAdam"}
  %get-tuple-element.1184 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.84), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_34/update_0/ResourceApplyAdam"}
  %arg108.109 = f32[3072]{0} parameter(108), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg109.110 = f32[3072]{0} parameter(109), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.64 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.80), index=0
  %custom-call.38 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.64, f16[3072,768]{1,0} %convert.315), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.91 = f16[16,512,3072]{2,1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.126, f16[8192,3072]{1,0} %custom-call.38, f16[16,512,3072]{2,1,0} %arg0.1, f16[8192,3072]{1,0} %custom-call.25, f32[3072]{0} %arg22.23), kind=kLoop, calls=%fused_computation.91, metadata={op_type="AddN" op_name="AddN_4"}
  %fusion.90 = f32[3072]{0} fusion(f16[16,512,3072]{2,1,0} %fusion.91), kind=kInput, calls=%fused_computation.90, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Sum"}
  %fusion.89 = f32[3072]{0} fusion(f32[3072]{0} %fusion.90), kind=kLoop, calls=%fused_computation.89, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1220 = (f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) custom-call(f32[3072]{0} %arg22.23, f32[3072]{0} %arg108.109, f32[3072]{0} %arg109.110, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[3072]{0} %fusion.89), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_32/update_0/ResourceApplyAdam"}
  %get-tuple-element.1221 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) %custom-call.1220), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_32/update_0/ResourceApplyAdam"}
  %arg106.107 = f32[768,3072]{1,0} parameter(106), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg107.108 = f32[768,3072]{1,0} parameter(107), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.88 = f16[3072,8192]{0,1} fusion(f16[16,512,3072]{2,1,0} %fusion.91), kind=kLoop, calls=%fused_computation.88
  %custom-call.39 = f16[768,3072]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.40, f16[3072,8192]{0,1} %fusion.88), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/intermediate/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %custom-call.85 = (f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) custom-call(f32[768,3072]{1,0} %arg23.24, f32[768,3072]{1,0} %arg106.107, f32[768,3072]{1,0} %arg107.108, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[768,3072]{1,0} %custom-call.39), custom_call_target="NdpAdam$Gemm$custom-call.39", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_31/update_0/ResourceApplyAdam"}
  %get-tuple-element.1232 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) %custom-call.85), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_31/update_0/ResourceApplyAdam"}
  %arg102.103 = f32[3072,768]{1,0} parameter(102), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg103.104 = f32[3072,768]{1,0} parameter(103), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.65 = f16[768,8192]{0,1} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.80), index=1
  %custom-call.40 = f16[3072,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.125, f16[768,8192]{0,1} %get-tuple-element.65), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"113\"}"
  %custom-call.86 = (f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) custom-call(f32[3072,768]{1,0} %arg24.25, f32[3072,768]{1,0} %arg102.103, f32[3072,768]{1,0} %arg103.104, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[3072,768]{1,0} %custom-call.40), custom_call_target="NdpAdam$Gemm$custom-call.40", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_33/update_0/ResourceApplyAdam"}
  %get-tuple-element.1240 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) %custom-call.86), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_33/update_0/ResourceApplyAdam"}
  %get-tuple-element.36 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f16[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.75), index=1
  %arg58.59 = f32[768]{0} parameter(58), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.41 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.76), index=1
  %arg62.63 = f32[768]{0} parameter(62), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg25.26 = f32[768]{0} parameter(25), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.2 = f32[768]{0} get-tuple-element((f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call), index=1, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %arg48.49 = f32[768]{0} parameter(48), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.31 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.74), index=1
  %arg54.55 = f32[768]{0} parameter(54), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.193 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) fusion(f32[768]{0} %get-tuple-element.36, f32[768]{0} %arg58.59, f32[768]{0} %get-tuple-element.41, f32[768]{0} %arg62.63, f32[768]{0} %get-tuple-element.46, f32[768]{0} %arg25.26, f32[768]{0} %get-tuple-element.2, f32[768]{0} %arg48.49, f32[768]{0} %get-tuple-element.31, f32[768]{0} %arg54.55), kind=kInput, calls=%horizontally_fused_computation
  %get-tuple-element.101 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.193), index=2, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %get-tuple-element.32 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.74), index=2
  %arg55.56 = f32[768]{0} parameter(55), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg26.27 = f32[768]{0} parameter(26), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element = f32[768]{0} get-tuple-element((f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call), index=2, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %arg49.50 = f32[768]{0} parameter(49), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.42 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.76), index=2
  %arg63.64 = f32[768]{0} parameter(63), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.37 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f16[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.75), index=2
  %arg59.60 = f32[768]{0} parameter(59), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.194 = (f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) fusion(f32[768]{0} %get-tuple-element.32, f32[768]{0} %arg55.56, f32[768]{0} %get-tuple-element.47, f32[768]{0} %arg26.27, f32[768]{0} %get-tuple-element, f32[768]{0} %arg49.50, f32[768]{0} %get-tuple-element.42, f32[768]{0} %arg63.64, f32[768]{0} %get-tuple-element.37, f32[768]{0} %arg59.60), kind=kInput, calls=%horizontally_fused_computation.1
  %get-tuple-element.105 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.194), index=1, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormV3"}
  %arg98.99 = f32[768]{0} parameter(98), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg99.100 = f32[768]{0} parameter(99), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.68 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.80), index=4
  %custom-call.87 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg27.28, f32[768]{0} %arg98.99, f32[768]{0} %arg99.100, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.68), custom_call_target="NdpAdam$Batchnorm$custom-call.80", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_35/update_0/ResourceApplyAdam"}
  %get-tuple-element.1157 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.87), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_35/update_0/ResourceApplyAdam"}
  %arg100.101 = f32[768]{0} parameter(100), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg101.102 = f32[768]{0} parameter(101), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.69 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.80), index=5
  %custom-call.88 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg28.29, f32[768]{0} %arg100.101, f32[768]{0} %arg101.102, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.69), custom_call_target="NdpAdam$Batchnorm$custom-call.80", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_36/update_0/ResourceApplyAdam"}
  %get-tuple-element.1162 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.88), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_36/update_0/ResourceApplyAdam"}
  %arg116.117 = f32[768]{0} parameter(116), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg117.118 = f32[768]{0} parameter(117), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.43 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.76), index=3
  %get-tuple-element.67 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.80), index=3
  %bitcast.288 = f32[1,16,512,768]{3,2,1,0} bitcast(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.67), metadata={op_type="Transpose" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/output_layer_norm/FusedBatchNormGradV3-0-1-TransposeNCHWToNHWC-LayoutOptimizer"}
  %bitcast.78 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %fusion.91)
  %custom-call.41 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %bitcast.78, f16[768,3072]{1,0} %convert.313), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.85 = f32[16,512,768]{2,1,0} fusion(f16[8192,768]{1,0} %custom-call.41), kind=kLoop, calls=%fused_computation.85, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/Cast_1/Cast"}
  %custom-call.78 = (f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) custom-call(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.43, f32[768]{0} %arg64.65, f32[768]{0} %get-tuple-element.41, f32[768]{0} %get-tuple-element.42, f16[16,512,768]{2,1,0} %arg5.6, f16[] %constant_242, f32[1,16,512,768]{3,2,1,0} %bitcast.288, f32[16,512,768]{2,1,0} %fusion.85), custom_call_target="NdpFusedBackwardBertOutput$DropoutTransfer"
  %get-tuple-element.52 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.78), index=2
  %custom-call.89 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg29.30, f32[768]{0} %arg116.117, f32[768]{0} %arg117.118, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.52), custom_call_target="NdpAdam$Batchnorm$custom-call.78", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_28/update_0/ResourceApplyAdam"}
  %get-tuple-element.1291 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.89), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_28/update_0/ResourceApplyAdam"}
  %arg124.125 = f32[12,64]{1,0} parameter(124), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg125.126 = f32[12,64]{1,0} parameter(125), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.50 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.78), index=0
  %fusion.84 = f16[768,768]{0,1} fusion(f32[12,64,768]{2,1,0} %arg61.62), kind=kLoop, calls=%fused_computation.84
  %custom-call.42 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.50, f16[768,768]{0,1} %fusion.84), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.161 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.42), kind=kLoop, calls=%fused_computation.161
  %fusion.83 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.22, f32[12,64]{1,0} %arg34.35), kind=kLoop, calls=%fused_computation.83
  %custom-call.43 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.161, f16[16,12,64,512]{3,2,1,0} %fusion.83), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.81 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.133, f32[16,12,512]{2,1,0} %reduce.6, f16[16,12,512,512]{3,2,1,0} %custom-call.43, f16[16,12,512,512]{3,2,1,0} %arg2.3), kind=kLoop, calls=%fused_computation.81, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  %reduce.7 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.81, f32[] %constant_208), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_1_self_attention_softmax_Sum-reduction.1306, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/Sum"}
  %fusion.80 = f16[16,12,512,512]{3,2,1,0} fusion(f32[16,12,512]{2,1,0} %reduce.7, f16[16,12,512,512]{2,3,1,0} %fusion.133, f32[16,12,512]{2,1,0} %reduce.6, f16[16,12,512,512]{3,2,1,0} %custom-call.43, f16[16,12,512,512]{3,2,1,0} %arg2.3), kind=kLoop, calls=%fused_computation.80, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/softmax/mul_1"}
  %fusion.79 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.20, f32[12,64]{1,0} %arg32.33), kind=kLoop, calls=%fused_computation.79
  %custom-call.44 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.80, f16[16,12,512,64]{3,2,1,0} %fusion.79), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.183 = f32[16,12,64]{2,1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.44), kind=kInput, calls=%fused_computation.183, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Sum"}
  %fusion.77 = f32[12,64]{1,0} fusion(f32[16,12,64]{2,1,0} %fusion.183), kind=kLoop, calls=%fused_computation.77, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1330 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) custom-call(f32[12,64]{1,0} %arg30.31, f32[12,64]{1,0} %arg124.125, f32[12,64]{1,0} %arg125.126, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64]{1,0} %fusion.77), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_24/update_0/ResourceApplyAdam"}
  %get-tuple-element.1331 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1330), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_24/update_0/ResourceApplyAdam"}
  %arg122.123 = f32[768,12,64]{2,1,0} parameter(122), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg123.124 = f32[768,12,64]{2,1,0} parameter(123), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.76 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.44), kind=kLoop, calls=%fused_computation.76
  %custom-call.45 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.76, f16[8192,768]{1,0} %get-tuple-element.35), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"99\"}"
  %fusion.75 = f32[768,12,64]{0,2,1} fusion(f16[768,768]{1,0} %custom-call.45), kind=kLoop, calls=%fused_computation.75, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1340 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) custom-call(f32[768,12,64]{2,1,0} %arg31.32, f32[768,12,64]{2,1,0} %arg122.123, f32[768,12,64]{2,1,0} %arg123.124, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768,12,64]{0,2,1} %fusion.75), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_23/update_0/ResourceApplyAdam"}
  %get-tuple-element.1341 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1340), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_23/update_0/ResourceApplyAdam"}
  %arg128.129 = f32[12,64]{1,0} parameter(128), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg129.130 = f32[12,64]{1,0} parameter(129), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.46 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.80, f16[16,12,512,64]{3,2,1,0} %fusion.136), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum/Einsum_1"}, backend_config="{\"alpha_real\":0.125,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.184 = f32[16,12,64]{2,1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.46), kind=kInput, calls=%fused_computation.184, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Sum"}
  %fusion.73 = f32[12,64]{1,0} fusion(f32[16,12,64]{2,1,0} %fusion.184), kind=kLoop, calls=%fused_computation.73, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1360 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) custom-call(f32[12,64]{1,0} %arg32.33, f32[12,64]{1,0} %arg128.129, f32[12,64]{1,0} %arg129.130, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64]{1,0} %fusion.73), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_22/update_0/ResourceApplyAdam"}
  %get-tuple-element.1361 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1360), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_22/update_0/ResourceApplyAdam"}
  %arg126.127 = f32[768,12,64]{2,1,0} parameter(126), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg127.128 = f32[768,12,64]{2,1,0} parameter(127), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.72 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.46), kind=kLoop, calls=%fused_computation.72
  %custom-call.47 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.72, f16[8192,768]{1,0} %get-tuple-element.35), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"99\"}"
  %fusion.71 = f32[768,12,64]{0,2,1} fusion(f16[768,768]{1,0} %custom-call.47), kind=kLoop, calls=%fused_computation.71, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1370 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) custom-call(f32[768,12,64]{2,1,0} %arg33.34, f32[768,12,64]{2,1,0} %arg126.127, f32[768,12,64]{2,1,0} %arg127.128, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768,12,64]{0,2,1} %fusion.71), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_21/update_0/ResourceApplyAdam"}
  %get-tuple-element.1371 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1370), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_21/update_0/ResourceApplyAdam"}
  %arg120.121 = f32[12,64]{1,0} parameter(120), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg121.122 = f32[12,64]{1,0} parameter(121), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.70 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.42), kind=kLoop, calls=%fused_computation.70
  %custom-call.48 = f16[16,12,64,512]{3,2,1,0} custom-call(f16[16,12,64,512]{3,2,1,0} %fusion.70, f16[16,12,512,512]{3,2,1,0} %fusion.130), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/einsum_1/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.185 = f32[12,64]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.48), kind=kInput, calls=%fused_computation.185, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Sum"}
  %fusion.68 = f32[12,64]{1,0} fusion(f32[12,64]{1,0} %fusion.185), kind=kLoop, calls=%fused_computation.68, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1387 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) custom-call(f32[12,64]{1,0} %arg34.35, f32[12,64]{1,0} %arg120.121, f32[12,64]{1,0} %arg121.122, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64]{1,0} %fusion.68), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_26/update_0/ResourceApplyAdam"}
  %get-tuple-element.1388 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1387), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_26/update_0/ResourceApplyAdam"}
  %arg136.137 = f32[768]{0} parameter(136), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg137.138 = f32[768]{0} parameter(137), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.38 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f16[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.75), index=3
  %get-tuple-element.56 = f16[16,512,768]{2,1,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.78), index=6
  %fusion.67 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.44), kind=kLoop, calls=%fused_computation.67
  %fusion.66 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg31.32), kind=kLoop, calls=%fused_computation.66
  %custom-call.49 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.67, f16[768,768]{0,1} %fusion.66), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %bitcast.91 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %custom-call.49), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/key/einsum/Einsum"}
  %fusion.65 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.46), kind=kLoop, calls=%fused_computation.65
  %fusion.64 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg33.34), kind=kLoop, calls=%fused_computation.64
  %custom-call.50 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.65, f16[768,768]{0,1} %fusion.64), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %bitcast.94 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %custom-call.50), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/query/einsum/Einsum"}
  %fusion.63 = f16[8192,768]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.48), kind=kLoop, calls=%fused_computation.63
  %fusion.62 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg60.61), kind=kLoop, calls=%fused_computation.62
  %custom-call.51 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.63, f16[768,768]{0,1} %fusion.62), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %bitcast.97 = f16[16,512,768]{2,1,0} bitcast(f16[8192,768]{1,0} %custom-call.51), metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum"}
  %custom-call.81 = (f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) custom-call(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.38, f32[768]{0} %arg56.57, f32[768]{0} %get-tuple-element.36, f32[768]{0} %get-tuple-element.37, f16[16,512,768]{2,1,0} %arg6.7, f16[] %constant_242, f16[16,512,768]{2,1,0} %get-tuple-element.56, f16[16,512,768]{2,1,0} %bitcast.91, f16[16,512,768]{2,1,0} %bitcast.94, f16[16,512,768]{2,1,0} %bitcast.97), custom_call_target="NdpFusedBackwardBertOutput$SelfAttention"
  %get-tuple-element.72 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.81), index=2
  %custom-call.90 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg35.36, f32[768]{0} %arg136.137, f32[768]{0} %arg137.138, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.72), custom_call_target="NdpAdam$Batchnorm$custom-call.81", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_18/update_0/ResourceApplyAdam"}
  %get-tuple-element.1444 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.90), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_18/update_0/ResourceApplyAdam"}
  %arg140.141 = f32[3072]{0} parameter(140), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg141.142 = f32[3072]{0} parameter(141), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.70 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.81), index=0
  %custom-call.52 = f16[8192,3072]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.70, f16[3072,768]{1,0} %convert.278), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.61 = f16[16,512,3072]{2,1,0} fusion(f16[16,512,3072]{2,1,0} %fusion.138, f16[8192,3072]{1,0} %custom-call.52, f16[16,512,3072]{2,1,0} %arg1.2, f16[8192,3072]{1,0} %custom-call.17, f32[3072]{0} %arg36.37), kind=kLoop, calls=%fused_computation.61, metadata={op_type="AddN" op_name="AddN_7"}
  %fusion.60 = f32[3072]{0} fusion(f16[16,512,3072]{2,1,0} %fusion.61), kind=kInput, calls=%fused_computation.60, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Sum"}
  %fusion.59 = f32[3072]{0} fusion(f32[3072]{0} %fusion.60), kind=kLoop, calls=%fused_computation.59, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1480 = (f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) custom-call(f32[3072]{0} %arg36.37, f32[3072]{0} %arg140.141, f32[3072]{0} %arg141.142, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[3072]{0} %fusion.59), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_16/update_0/ResourceApplyAdam"}
  %get-tuple-element.1481 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) %custom-call.1480), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_16/update_0/ResourceApplyAdam"}
  %arg138.139 = f32[768,3072]{1,0} parameter(138), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg139.140 = f32[768,3072]{1,0} parameter(139), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.58 = f16[3072,8192]{0,1} fusion(f16[16,512,3072]{2,1,0} %fusion.61), kind=kLoop, calls=%fused_computation.58
  %custom-call.53 = f16[768,3072]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.30, f16[3072,8192]{0,1} %fusion.58), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/intermediate/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %custom-call.91 = (f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) custom-call(f32[768,3072]{1,0} %arg37.38, f32[768,3072]{1,0} %arg138.139, f32[768,3072]{1,0} %arg139.140, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[768,3072]{1,0} %custom-call.53), custom_call_target="NdpAdam$Gemm$custom-call.53", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_15/update_0/ResourceApplyAdam"}
  %get-tuple-element.1492 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) %custom-call.91), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_15/update_0/ResourceApplyAdam"}
  %arg134.135 = f32[3072,768]{1,0} parameter(134), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg135.136 = f32[3072,768]{1,0} parameter(135), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.71 = f16[768,8192]{0,1} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.81), index=1
  %custom-call.54 = f16[3072,768]{1,0} custom-call(f16[8192,3072]{1,0} %fusion.137, f16[768,8192]{0,1} %get-tuple-element.71), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output/einsum/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"113\"}"
  %custom-call.92 = (f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) custom-call(f32[3072,768]{1,0} %arg38.39, f32[3072,768]{1,0} %arg134.135, f32[3072,768]{1,0} %arg135.136, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[3072,768]{1,0} %custom-call.54), custom_call_target="NdpAdam$Gemm$custom-call.54", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_17/update_0/ResourceApplyAdam"}
  %get-tuple-element.1500 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) %custom-call.92), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_17/update_0/ResourceApplyAdam"}
  %arg148.149 = f32[768]{0} parameter(148), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg149.150 = f32[768]{0} parameter(149), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.33 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[1,768,16,512]{1,3,2,0}, f32[16,512,768]{2,1,0}) %custom-call.74), index=3
  %get-tuple-element.73 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.81), index=3
  %bitcast.289 = f32[1,16,512,768]{3,2,1,0} bitcast(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.73), metadata={op_type="Transpose" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormGradV3-0-1-TransposeNCHWToNHWC-LayoutOptimizer"}
  %bitcast.104 = f16[8192,3072]{1,0} bitcast(f16[16,512,3072]{2,1,0} %fusion.61)
  %custom-call.55 = f16[8192,768]{1,0} custom-call(f16[8192,3072]{1,0} %bitcast.104, f16[768,3072]{1,0} %convert.276), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"110\"}"
  %fusion.57 = f32[16,512,768]{2,1,0} fusion(f16[8192,768]{1,0} %custom-call.55), kind=kLoop, calls=%fused_computation.57, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/Cast_1/Cast"}
  %custom-call.79 = (f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) custom-call(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.33, f32[768]{0} %arg52.53, f32[768]{0} %get-tuple-element.31, f32[768]{0} %get-tuple-element.32, f16[16,512,768]{2,1,0} %arg7.8, f16[] %constant_242, f32[1,16,512,768]{3,2,1,0} %bitcast.289, f32[16,512,768]{2,1,0} %fusion.57), custom_call_target="NdpFusedBackwardBertOutput$DropoutTransfer"
  %get-tuple-element.59 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.79), index=2
  %custom-call.93 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg39.40, f32[768]{0} %arg148.149, f32[768]{0} %arg149.150, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.59), custom_call_target="NdpAdam$Batchnorm$custom-call.79", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_12/update_0/ResourceApplyAdam"}
  %get-tuple-element.1551 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.93), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_12/update_0/ResourceApplyAdam"}
  %arg156.157 = f32[12,64]{1,0} parameter(156), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg157.158 = f32[12,64]{1,0} parameter(157), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.57 = f16[8192,768]{1,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.79), index=0
  %fusion.56 = f16[768,768]{0,1} fusion(f32[12,64,768]{2,1,0} %arg51.52), kind=kLoop, calls=%fused_computation.56
  %custom-call.56 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %get-tuple-element.57, f16[768,768]{0,1} %fusion.56), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.160 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.56), kind=kLoop, calls=%fused_computation.160
  %fusion.55 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.14, f32[12,64]{1,0} %arg44.45), kind=kLoop, calls=%fused_computation.55
  %custom-call.57 = f16[16,12,512,512]{3,2,1,0} custom-call(f16[16,12,512,64]{3,2,1,0} %fusion.160, f16[16,12,64,512]{3,2,1,0} %fusion.55), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.53 = f32[16,12,512,512]{3,2,1,0} fusion(f16[16,12,512,512]{2,3,1,0} %fusion.145, f32[16,12,512]{2,1,0} %reduce.4, f16[16,12,512,512]{3,2,1,0} %custom-call.57, f16[16,12,512,512]{3,2,1,0} %arg3.4), kind=kLoop, calls=%fused_computation.53, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  %reduce.11 = f32[16,12,512]{2,1,0} reduce(f32[16,12,512,512]{3,2,1,0} %fusion.53, f32[] %constant_208), dimensions={2}, to_apply=%gradient_tape_model_bert_pretrainer_bert_encoder_1_transformer_layer_0_self_attention_softmax_Sum-reduction.1566, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/Sum"}
  %fusion.52 = f16[16,12,512,512]{3,2,1,0} fusion(f32[16,12,512]{2,1,0} %reduce.11, f16[16,12,512,512]{2,3,1,0} %fusion.145, f32[16,12,512]{2,1,0} %reduce.4, f16[16,12,512,512]{3,2,1,0} %custom-call.57, f16[16,12,512,512]{3,2,1,0} %arg3.4), kind=kLoop, calls=%fused_computation.52, metadata={op_type="Mul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/softmax/mul_1"}
  %fusion.51 = f16[16,12,512,64]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.12, f32[12,64]{1,0} %arg42.43), kind=kLoop, calls=%fused_computation.51
  %custom-call.58 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.52, f16[16,12,512,64]{3,2,1,0} %fusion.51), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.186 = f32[16,12,64]{2,1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.58), kind=kInput, calls=%fused_computation.186, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Sum"}
  %fusion.49 = f32[12,64]{1,0} fusion(f32[16,12,64]{2,1,0} %fusion.186), kind=kLoop, calls=%fused_computation.49, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1590 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) custom-call(f32[12,64]{1,0} %arg40.41, f32[12,64]{1,0} %arg156.157, f32[12,64]{1,0} %arg157.158, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64]{1,0} %fusion.49), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_8/update_0/ResourceApplyAdam"}
  %get-tuple-element.1591 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1590), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_8/update_0/ResourceApplyAdam"}
  %arg154.155 = f32[768,12,64]{2,1,0} parameter(154), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg155.156 = f32[768,12,64]{2,1,0} parameter(155), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.48 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.58), kind=kLoop, calls=%fused_computation.48
  %custom-call.59 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.48, f16[8192,768]{1,0} %fusion.150), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"99\"}"
  %fusion.47 = f32[768,12,64]{0,2,1} fusion(f16[768,768]{1,0} %custom-call.59), kind=kLoop, calls=%fused_computation.47, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/key/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1600 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) custom-call(f32[768,12,64]{2,1,0} %arg41.42, f32[768,12,64]{2,1,0} %arg154.155, f32[768,12,64]{2,1,0} %arg155.156, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768,12,64]{0,2,1} %fusion.47), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_7/update_0/ResourceApplyAdam"}
  %get-tuple-element.1601 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1600), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_7/update_0/ResourceApplyAdam"}
  %arg160.161 = f32[12,64]{1,0} parameter(160), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg161.162 = f32[12,64]{1,0} parameter(161), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.60 = f16[16,12,512,64]{3,2,1,0} custom-call(f16[16,12,512,512]{3,2,1,0} %fusion.52, f16[16,12,512,64]{3,2,1,0} %fusion.149), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum/Einsum_1"}, backend_config="{\"alpha_real\":0.125,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.187 = f32[16,12,64]{2,1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.60), kind=kInput, calls=%fused_computation.187, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Sum"}
  %fusion.45 = f32[12,64]{1,0} fusion(f32[16,12,64]{2,1,0} %fusion.187), kind=kLoop, calls=%fused_computation.45, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1620 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) custom-call(f32[12,64]{1,0} %arg42.43, f32[12,64]{1,0} %arg160.161, f32[12,64]{1,0} %arg161.162, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64]{1,0} %fusion.45), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_6/update_0/ResourceApplyAdam"}
  %get-tuple-element.1621 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1620), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_6/update_0/ResourceApplyAdam"}
  %arg158.159 = f32[768,12,64]{2,1,0} parameter(158), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg159.160 = f32[768,12,64]{2,1,0} parameter(159), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.44 = f16[768,8192]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.60), kind=kLoop, calls=%fused_computation.44
  %custom-call.61 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.44, f16[8192,768]{1,0} %fusion.150), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"99\"}"
  %fusion.43 = f32[768,12,64]{0,2,1} fusion(f16[768,768]{1,0} %custom-call.61), kind=kLoop, calls=%fused_computation.43, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/query/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1630 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) custom-call(f32[768,12,64]{2,1,0} %arg43.44, f32[768,12,64]{2,1,0} %arg158.159, f32[768,12,64]{2,1,0} %arg159.160, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768,12,64]{0,2,1} %fusion.43), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_5/update_0/ResourceApplyAdam"}
  %get-tuple-element.1631 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1630), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_5/update_0/ResourceApplyAdam"}
  %arg150.151 = f32[12,64]{1,0} parameter(150), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg151.152 = f32[12,64]{1,0} parameter(151), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.42 = f16[16,12,64,512]{3,2,1,0} fusion(f16[8192,768]{1,0} %custom-call.56), kind=kLoop, calls=%fused_computation.42
  %custom-call.62 = f16[16,12,64,512]{3,2,1,0} custom-call(f16[16,12,64,512]{3,2,1,0} %fusion.42, f16[16,12,512,512]{3,2,1,0} %fusion.142), custom_call_target="__cublas$gemm", metadata={op_type="Einsum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/einsum_1/Einsum_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"batch_size\":\"192\"}"
  %fusion.188 = f32[12,64]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.62), kind=kInput, calls=%fused_computation.188, metadata={op_type="Sum" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Sum"}
  %fusion.40 = f32[12,64]{1,0} fusion(f32[12,64]{1,0} %fusion.188), kind=kLoop, calls=%fused_computation.40, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/add/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1647 = (f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) custom-call(f32[12,64]{1,0} %arg44.45, f32[12,64]{1,0} %arg150.151, f32[12,64]{1,0} %arg151.152, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64]{1,0} %fusion.40), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_10/update_0/ResourceApplyAdam"}
  %get-tuple-element.1648 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1647), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_10/update_0/ResourceApplyAdam"}
  %arg170.171 = f32[512,768]{1,0} parameter(170), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg171.172 = f32[512,768]{1,0} parameter(171), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.35 = f16[8192,768]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.62), kind=kLoop, calls=%fused_computation.35
  %fusion.34 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg50.51), kind=kLoop, calls=%fused_computation.34
  %custom-call.65 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.35, f16[768,768]{0,1} %fusion.34), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.37 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.60), kind=kLoop, calls=%fused_computation.37
  %fusion.36 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg43.44), kind=kLoop, calls=%fused_computation.36
  %custom-call.64 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.37, f16[768,768]{0,1} %fusion.36), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %get-tuple-element.63 = f16[16,512,768]{2,1,0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.79), index=6
  %fusion.39 = f16[8192,768]{1,0} fusion(f16[16,12,512,64]{3,2,1,0} %custom-call.58), kind=kLoop, calls=%fused_computation.39
  %fusion.38 = f16[768,768]{0,1} fusion(f32[768,12,64]{2,1,0} %arg41.42), kind=kLoop, calls=%fused_computation.38
  %custom-call.63 = f16[8192,768]{1,0} custom-call(f16[8192,768]{1,0} %fusion.39, f16[768,768]{0,1} %fusion.38), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.33 = f32[1,768,16,512]{1,3,2,0} fusion(f16[8192,768]{1,0} %custom-call.65, f16[8192,768]{1,0} %custom-call.64, f16[16,512,768]{2,1,0} %get-tuple-element.63, f16[8192,768]{1,0} %custom-call.63, f16[16,512,768]{2,1,0} %arg8.9), kind=kLoop, calls=%fused_computation.33, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast_1/ArithmeticOptimizer/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.9 = (f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) custom-call(f32[1,768,16,512]{1,3,2,0} %fusion.151, f32[768]{0} %arg46.47, f32[768]{0} %get-tuple-element.2, f32[768]{0} %get-tuple-element, f32[1,768,16,512]{1,3,2,0} %fusion.33, f32[] %constant, s64[] %constant_196), custom_call_target="__cudnn$batchNormalizationBackward"
  %get-tuple-element.27 = f32[1,768,16,512]{1,3,2,0} get-tuple-element((f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.9), index=0, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormGradV3"}
  %fusion.32 = f16[1,16,512,768]{3,2,1,0} fusion(f32[1,768,16,512]{1,3,2,0} %get-tuple-element.27), kind=kLoop, calls=%fused_computation.32, metadata={op_type="Transpose" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/Cast/Cast-0-0-TransposeNCHWToNHWC-LayoutOptimizer"}
  %fusion.31 = f32[512,768]{1,0} fusion(f16[1,16,512,768]{3,2,1,0} %fusion.32), kind=kLoop, calls=%fused_computation.31, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/position_embedding/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1709 = (f32[512,768]{1,0}, f32[512,768]{1,0}, f32[512,768]{1,0}) custom-call(f32[512,768]{1,0} %arg45.46, f32[512,768]{1,0} %arg170.171, f32[512,768]{1,0} %arg171.172, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[512,768]{1,0} %fusion.31), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_1/update_0/ResourceApplyAdam"}
  %get-tuple-element.1710 = f32[512,768]{1,0} get-tuple-element((f32[512,768]{1,0}, f32[512,768]{1,0}, f32[512,768]{1,0}) %custom-call.1709), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_1/update_0/ResourceApplyAdam"}
  %arg162.163 = f32[768]{0} parameter(162), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg163.164 = f32[768]{0} parameter(163), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.28 = f32[768]{0} get-tuple-element((f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.9), index=1, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormGradV3"}
  %custom-call.94 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg46.47, f32[768]{0} %arg162.163, f32[768]{0} %arg163.164, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.28), custom_call_target="NdpAdam$Batchnorm$custom-call.9", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_3/update_0/ResourceApplyAdam"}
  %get-tuple-element.1681 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.94), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_3/update_0/ResourceApplyAdam"}
  %arg164.165 = f32[768]{0} parameter(164), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg165.166 = f32[768]{0} parameter(165), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.29 = f32[768]{0} get-tuple-element((f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.9), index=2, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormGradV3"}
  %custom-call.95 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg47.48, f32[768]{0} %arg164.165, f32[768]{0} %arg165.166, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.29), custom_call_target="NdpAdam$Batchnorm$custom-call.9", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_4/update_0/ResourceApplyAdam"}
  %get-tuple-element.1686 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.95), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_4/update_0/ResourceApplyAdam"}
  %get-tuple-element.102 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.193), index=3, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %get-tuple-element.106 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.194), index=2, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/embeddings/layer_norm/FusedBatchNormV3"}
  %arg152.153 = f32[768,12,64]{2,1,0} parameter(152), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg153.154 = f32[768,12,64]{2,1,0} parameter(153), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.28 = f16[768,8192]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.62), kind=kLoop, calls=%fused_computation.28
  %custom-call.66 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.28, f16[8192,768]{1,0} %fusion.150), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"99\"}"
  %fusion.27 = f32[768,12,64]{0,2,1} fusion(f16[768,768]{1,0} %custom-call.66), kind=kLoop, calls=%fused_computation.27, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1737 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) custom-call(f32[768,12,64]{2,1,0} %arg50.51, f32[768,12,64]{2,1,0} %arg152.153, f32[768,12,64]{2,1,0} %arg153.154, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768,12,64]{0,2,1} %fusion.27), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_9/update_0/ResourceApplyAdam"}
  %get-tuple-element.1738 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1737), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_9/update_0/ResourceApplyAdam"}
  %arg146.147 = f32[12,64,768]{2,1,0} parameter(146), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg147.148 = f32[12,64,768]{2,1,0} parameter(147), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.58 = f16[768,8192]{0,1} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.79), index=1
  %custom-call.67 = f16[768,768]{1,0} custom-call(f16[768,8192]{0,1} %get-tuple-element.58, f16[8192,768]{1,0} %fusion.140), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"111\"}"
  %fusion.26 = f32[12,64,768]{1,0,2} fusion(f16[768,768]{1,0} %custom-call.67), kind=kLoop, calls=%fused_computation.26, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1745 = (f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) custom-call(f32[12,64,768]{2,1,0} %arg51.52, f32[12,64,768]{2,1,0} %arg146.147, f32[12,64,768]{2,1,0} %arg147.148, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64,768]{1,0,2} %fusion.26), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_11/update_0/ResourceApplyAdam"}
  %get-tuple-element.1746 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %custom-call.1745), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_11/update_0/ResourceApplyAdam"}
  %arg142.143 = f32[768]{0} parameter(142), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg143.144 = f32[768]{0} parameter(143), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.61 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.79), index=4
  %custom-call.96 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg52.53, f32[768]{0} %arg142.143, f32[768]{0} %arg143.144, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.61), custom_call_target="NdpAdam$Batchnorm$custom-call.79", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_13/update_0/ResourceApplyAdam"}
  %get-tuple-element.1524 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.96), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_13/update_0/ResourceApplyAdam"}
  %arg144.145 = f32[768]{0} parameter(144), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg145.146 = f32[768]{0} parameter(145), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.62 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.79), index=5
  %custom-call.97 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg53.54, f32[768]{0} %arg144.145, f32[768]{0} %arg145.146, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.62), custom_call_target="NdpAdam$Batchnorm$custom-call.79", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_14/update_0/ResourceApplyAdam"}
  %get-tuple-element.1529 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.97), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_14/update_0/ResourceApplyAdam"}
  %get-tuple-element.103 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.193), index=4, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %get-tuple-element.104 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.194), index=0, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/self_attention_layer_norm/FusedBatchNormV3"}
  %arg130.131 = f32[768]{0} parameter(130), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg131.132 = f32[768]{0} parameter(131), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.74 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.81), index=4
  %custom-call.98 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg56.57, f32[768]{0} %arg130.131, f32[768]{0} %arg131.132, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.74), custom_call_target="NdpAdam$Batchnorm$custom-call.81", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_19/update_0/ResourceApplyAdam"}
  %get-tuple-element.1417 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.98), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_19/update_0/ResourceApplyAdam"}
  %arg132.133 = f32[768]{0} parameter(132), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg133.134 = f32[768]{0} parameter(133), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.75 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}) %custom-call.81), index=5
  %custom-call.99 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg57.58, f32[768]{0} %arg132.133, f32[768]{0} %arg133.134, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.75), custom_call_target="NdpAdam$Batchnorm$custom-call.81", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_20/update_0/ResourceApplyAdam"}
  %get-tuple-element.1422 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.99), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_20/update_0/ResourceApplyAdam"}
  %get-tuple-element.99 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.193), index=0, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %get-tuple-element.108 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.194), index=4, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_0/output_layer_norm/FusedBatchNormV3"}
  %arg118.119 = f32[768,12,64]{2,1,0} parameter(118), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg119.120 = f32[768,12,64]{2,1,0} parameter(119), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.21 = f16[768,8192]{1,0} fusion(f16[16,12,64,512]{3,2,1,0} %custom-call.48), kind=kLoop, calls=%fused_computation.21
  %custom-call.68 = f16[768,768]{1,0} custom-call(f16[768,8192]{1,0} %fusion.21, f16[8192,768]{1,0} %get-tuple-element.35), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"99\"}"
  %fusion.20 = f32[768,12,64]{0,2,1} fusion(f16[768,768]{1,0} %custom-call.68), kind=kLoop, calls=%fused_computation.20, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/value/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1753 = (f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) custom-call(f32[768,12,64]{2,1,0} %arg60.61, f32[768,12,64]{2,1,0} %arg118.119, f32[768,12,64]{2,1,0} %arg119.120, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768,12,64]{0,2,1} %fusion.20), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_25/update_0/ResourceApplyAdam"}
  %get-tuple-element.1754 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1753), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_25/update_0/ResourceApplyAdam"}
  %arg114.115 = f32[12,64,768]{2,1,0} parameter(114), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg115.116 = f32[12,64,768]{2,1,0} parameter(115), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.51 = f16[768,8192]{0,1} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.78), index=1
  %custom-call.69 = f16[768,768]{1,0} custom-call(f16[768,8192]{0,1} %get-tuple-element.51, f16[8192,768]{1,0} %fusion.128), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"111\"}"
  %fusion.19 = f32[12,64,768]{1,0,2} fusion(f16[768,768]{1,0} %custom-call.69), kind=kLoop, calls=%fused_computation.19, metadata={op_type="Cast" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention/attention_output/einsum/Einsum/Cast/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_float_Cast"}
  %custom-call.1761 = (f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) custom-call(f32[12,64,768]{2,1,0} %arg61.62, f32[12,64,768]{2,1,0} %arg114.115, f32[12,64,768]{2,1,0} %arg115.116, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[12,64,768]{1,0,2} %fusion.19), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_27/update_0/ResourceApplyAdam"}
  %get-tuple-element.1762 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %custom-call.1761), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_27/update_0/ResourceApplyAdam"}
  %get-tuple-element.100 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.193), index=1, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %get-tuple-element.107 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}) %fusion.194), index=3, metadata={op_type="FusedBatchNormV3" op_name="model/bert_pretrainer/bert_encoder_1/transformer/layer_1/self_attention_layer_norm/FusedBatchNormV3"}
  %arg110.111 = f32[768]{0} parameter(110), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg111.112 = f32[768]{0} parameter(111), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.54 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.78), index=4
  %custom-call.100 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg64.65, f32[768]{0} %arg110.111, f32[768]{0} %arg111.112, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.54), custom_call_target="NdpAdam$Batchnorm$custom-call.78", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_29/update_0/ResourceApplyAdam"}
  %get-tuple-element.1264 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.100), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_29/update_0/ResourceApplyAdam"}
  %arg112.113 = f32[768]{0} parameter(112), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg113.114 = f32[768]{0} parameter(113), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.55 = f32[768]{0} get-tuple-element((f16[8192,768]{1,0}, f16[768,8192]{0,1}, f32[768]{0}, f32[1,768,16,512]{1,3,2,0}, f32[768]{0}, f32[768]{0}, f16[16,512,768]{2,1,0}) %custom-call.78), index=5
  %custom-call.101 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg65.66, f32[768]{0} %arg112.113, f32[768]{0} %arg113.114, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.55), custom_call_target="NdpAdam$Batchnorm$custom-call.78", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_30/update_0/ResourceApplyAdam"}
  %get-tuple-element.1269 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.101), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_30/update_0/ResourceApplyAdam"}
  %arg166.167 = f32[2,768]{1,0} parameter(166), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg167.168 = f32[2,768]{1,0} parameter(167), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.133 = f16[8192,768]{1,0} bitcast(f16[1,16,512,768]{3,2,1,0} %fusion.32), metadata={op_type="Reshape" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/word_embeddings/Reshape"}
  %custom-call.70 = f16[8,768]{1,0} custom-call(f16[8192,8]{1,0} %fusion.153, f16[8192,768]{1,0} %bitcast.133), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/bert_encoder_1/type_embeddings/MatMul/MatMul"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"109\"}"
  %custom-call.102 = (f32[2,768]{1,0}, f32[2,768]{1,0}, f32[2,768]{1,0}) custom-call(f32[2,768]{1,0} %arg66.67, f32[2,768]{1,0} %arg166.167, f32[2,768]{1,0} %arg167.168, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[8,768]{1,0} %custom-call.70), custom_call_target="NdpAdam$Gemm$custom-call.70", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_2/update_0/ResourceApplyAdam"}
  %get-tuple-element.1719 = f32[2,768]{1,0} get-tuple-element((f32[2,768]{1,0}, f32[2,768]{1,0}, f32[2,768]{1,0}) %custom-call.102), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_2/update_0/ResourceApplyAdam"}
  %arg77.78 = f32[2]{0} parameter(77), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg78.79 = f32[2]{0} parameter(78), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.16 = f32[2]{0} fusion(f16[16,2]{1,0} %fusion.117), kind=kLoop, calls=%fused_computation.16, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/BiasAdd/BiasAddGrad"}
  %custom-call.103 = (f32[2]{0}, f32[2]{0}, f32[2]{0}) custom-call(f32[2]{0} %arg67.68, f32[2]{0} %arg77.78, f32[2]{0} %arg78.79, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[2]{0} %fusion.16), custom_call_target="NdpAdam$GemmBias$reduce.797", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_40/update_0/ResourceApplyAdam"}
  %get-tuple-element.802 = f32[2]{0} get-tuple-element((f32[2]{0}, f32[2]{0}, f32[2]{0}) %custom-call.103), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_40/update_0/ResourceApplyAdam"}
  %arg79.80 = f32[768,2]{1,0} parameter(79), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg80.81 = f32[768,2]{1,0} parameter(80), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.71 = f16[768,8]{1,0} custom-call(f16[16,768]{1,0} %tanh.694, f16[16,8]{1,0} %pad.4), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/classification/predictions/transform/logits/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"106\"}"
  %custom-call.104 = (f32[768,2]{1,0}, f32[768,2]{1,0}, f32[768,2]{1,0}) custom-call(f32[768,2]{1,0} %arg68.69, f32[768,2]{1,0} %arg79.80, f32[768,2]{1,0} %arg80.81, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[768,8]{1,0} %custom-call.71), custom_call_target="NdpAdam$Gemm$custom-call.71", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_39/update_0/ResourceApplyAdam"}
  %get-tuple-element.848 = f32[768,2]{1,0} get-tuple-element((f32[768,2]{1,0}, f32[768,2]{1,0}, f32[768,2]{1,0}) %custom-call.104), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_39/update_0/ResourceApplyAdam"}
  %arg86.87 = f32[30522]{0} parameter(86), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg87.88 = f32[30522]{0} parameter(87), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.15 = f32[30522]{0} fusion(s32[16,76]{1,0} %arg10.11, f32[1216,30522]{1,0} %get-tuple-element.90, f32[1216]{0} %get-tuple-element.89, f32[16,76]{1,0} %arg12.13, f32[] %get-tuple-element.87), kind=kInput, calls=%fused_computation.15, metadata={op_type="BiasAddGrad" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/BiasAdd/BiasAddGrad"}
  %custom-call.105 = (f32[30522]{0}, f32[30522]{0}, f32[30522]{0}) custom-call(f32[30522]{0} %arg69.70, f32[30522]{0} %arg86.87, f32[30522]{0} %arg87.88, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[30522]{0} %fusion.15), custom_call_target="NdpAdam$GemmBias$reduce.1023", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_41/update_0/ResourceApplyAdam"}
  %get-tuple-element.1028 = f32[30522]{0} get-tuple-element((f32[30522]{0}, f32[30522]{0}, f32[30522]{0}) %custom-call.105), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_41/update_0/ResourceApplyAdam"}
  %arg168.169 = f32[30522,768]{1,0} parameter(168), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg169.170 = f32[30522,768]{1,0} parameter(169), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.72 = f16[30528,768]{1,0} custom-call(f16[1216,30528]{1,0} %fusion.182, f16[1216,768]{1,0} %fusion.104), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/MatMul/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %fusion.14 = f32[30522,768]{1,0} fusion(f16[30528,768]{1,0} %custom-call.72, f16[1,16,512,768]{3,2,1,0} %fusion.32, s32[16,512]{1,0} %arg14.15), kind=kInput, calls=%fused_computation.14, metadata={op_type="UnsortedSegmentSum" op_name="AddN_10/inputs_1"}
  %custom-call.1809 = (f32[30522,768]{1,0}, f32[30522,768]{1,0}, f32[30522,768]{1,0}) custom-call(f32[30522,768]{1,0} %arg70.71, f32[30522,768]{1,0} %arg168.169, f32[30522,768]{1,0} %arg169.170, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[30522,768]{1,0} %fusion.14), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update/update_0/ResourceApplyAdam"}
  %get-tuple-element.1810 = f32[30522,768]{1,0} get-tuple-element((f32[30522,768]{1,0}, f32[30522,768]{1,0}, f32[30522,768]{1,0}) %custom-call.1809), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update/update_0/ResourceApplyAdam"}
  %arg89.90 = f32[768]{0} parameter(89), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg90.91 = f32[768]{0} parameter(90), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.81 = f32[768]{0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.11), index=2
  %custom-call.1800 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg71.72, f32[768]{0} %arg89.90, f32[768]{0} %arg90.91, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.81), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_45/update_0/ResourceApplyAdam"}
  %get-tuple-element.1801 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.1800), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_45/update_0/ResourceApplyAdam"}
  %arg92.93 = f32[768]{0} parameter(92), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg93.94 = f32[768]{0} parameter(93), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.82 = f32[768]{0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.11), index=3
  %custom-call.1786 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg72.73, f32[768]{0} %arg92.93, f32[768]{0} %arg93.94, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.82), custom_call_target="NdpAdam", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_44/update_0/ResourceApplyAdam"}
  %get-tuple-element.1787 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.1786), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_44/update_0/ResourceApplyAdam"}
  %arg96.97 = f32[768]{0} parameter(96), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg97.98 = f32[768]{0} parameter(97), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.79 = f32[768]{0} get-tuple-element((f32[768]{0}, f16[1216,768]{1,0}, f32[768]{0}, f32[768]{0}) %fusion.11), index=0
  %custom-call.106 = (f32[768]{0}, f32[768]{0}, f32[768]{0}) custom-call(f32[768]{0} %arg73.74, f32[768]{0} %arg96.97, f32[768]{0} %arg97.98, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f32[768]{0} %get-tuple-element.79), custom_call_target="NdpAdam$GemmBias$reduce.1117", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_43/update_0/ResourceApplyAdam"}
  %get-tuple-element.1122 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.106), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_43/update_0/ResourceApplyAdam"}
  %arg94.95 = f32[768,768]{1,0} parameter(94), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg95.96 = f32[768,768]{1,0} parameter(95), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.73 = f16[768,768]{1,0} custom-call(f16[1216,768]{1,0} %fusion.180, f16[1216,768]{1,0} %get-tuple-element.80), custom_call_target="__cublas$gemm", metadata={op_type="MatMul" op_name="gradient_tape/model/bert_pretrainer/cls/predictions/transform/dense/MatMul_1"}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"103\"}"
  %custom-call.107 = (f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) custom-call(f32[768,768]{1,0} %arg74.75, f32[768,768]{1,0} %arg94.95, f32[768,768]{1,0} %arg95.96, f32[] %get-tuple-element.98, f32[] %get-tuple-element.97, f32[] %get-tuple-element.96, f32[] %arg16.17, f32[] %arg17.18, f32[] %constant_800, f16[768,768]{1,0} %custom-call.73), custom_call_target="NdpAdam$Gemm$custom-call.73", metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_42/update_0/ResourceApplyAdam"}
  %get-tuple-element.1770 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) %custom-call.107), index=0, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_42/update_0/ResourceApplyAdam"}
  %arg91.92 = f32[] parameter(91), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.94 = f32[16]{0} get-tuple-element((f32[16]{0}, f32[16]{0}) %fusion.192), index=0
  %fusion.10 = f32[] fusion(f32[16]{0} %get-tuple-element.94, s32[16,1]{1,0} %arg13.14, f32[16]{0} %fusion.120, f32[2]{0} %arg67.68, f16[16,8]{1,0} %custom-call.29), kind=kInput, calls=%fused_computation.10, metadata={op_type="Mean" op_name="model/bert_pretrain_loss_and_metric_layer/Mean"}
  %get-tuple-element.88 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.2), index=2
  %arg85.86 = f32[] parameter(85), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg75.76 = f32[] parameter(75), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg88.89 = f32[] parameter(88), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.86 = f32[] get-tuple-element((f32[], f32[], f32[]) %fusion.2), index=0
  %fusion = (f32[], f32[], f32[], f32[]) fusion(f32[] %arg91.92, f32[] %fusion.10, f32[] %get-tuple-element.88, f32[] %get-tuple-element.87, f32[] %arg85.86, f32[] %arg75.76, f32[] %arg88.89, f32[] %get-tuple-element.86), kind=kLoop, calls=%fused_computation, metadata={op_type="AssignAddVariableOp" op_name="AssignAddVariableOp"}
  %get-tuple-element.78 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=2
  %arg76.77 = f32[] parameter(76), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.9 = s64[16]{0} fusion(f32[16]{0} %fusion.120, f32[2]{0} %arg67.68, f16[16,8]{1,0} %custom-call.29), kind=kLoop, calls=%fused_computation.9, metadata={op_type="ArgMax" op_name="model/bert_pretrain_loss_and_metric_layer/ArgMax_1"}
  %fusion.8 = f32[] fusion(f32[] %arg76.77, s32[16,1]{1,0} %arg13.14, s64[16]{0} %fusion.9), kind=kLoop, calls=%fused_computation.8, metadata={op_type="AssignAddVariableOp" op_name="model/bert_pretrain_loss_and_metric_layer/AssignAddVariableOp_4"}
  %get-tuple-element.803 = f32[2]{0} get-tuple-element((f32[2]{0}, f32[2]{0}, f32[2]{0}) %custom-call.103), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_40/update_0/ResourceApplyAdam"}
  %get-tuple-element.804 = f32[2]{0} get-tuple-element((f32[2]{0}, f32[2]{0}, f32[2]{0}) %custom-call.103), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_40/update_0/ResourceApplyAdam"}
  %get-tuple-element.849 = f32[768,2]{1,0} get-tuple-element((f32[768,2]{1,0}, f32[768,2]{1,0}, f32[768,2]{1,0}) %custom-call.104), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_39/update_0/ResourceApplyAdam"}
  %get-tuple-element.850 = f32[768,2]{1,0} get-tuple-element((f32[768,2]{1,0}, f32[768,2]{1,0}, f32[768,2]{1,0}) %custom-call.104), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_39/update_0/ResourceApplyAdam"}
  %get-tuple-element.826 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.82), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_38/update_0/ResourceApplyAdam"}
  %get-tuple-element.827 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.82), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_38/update_0/ResourceApplyAdam"}
  %get-tuple-element.841 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) %custom-call.83), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_37/update_0/ResourceApplyAdam"}
  %get-tuple-element.842 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) %custom-call.83), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_37/update_0/ResourceApplyAdam"}
  %get-tuple-element.77 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=1
  %get-tuple-element.1029 = f32[30522]{0} get-tuple-element((f32[30522]{0}, f32[30522]{0}, f32[30522]{0}) %custom-call.105), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_41/update_0/ResourceApplyAdam"}
  %get-tuple-element.1030 = f32[30522]{0} get-tuple-element((f32[30522]{0}, f32[30522]{0}, f32[30522]{0}) %custom-call.105), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_41/update_0/ResourceApplyAdam"}
  %get-tuple-element.85 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=3
  %get-tuple-element.1802 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.1800), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_45/update_0/ResourceApplyAdam"}
  %get-tuple-element.1803 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.1800), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_45/update_0/ResourceApplyAdam"}
  %get-tuple-element.76 = f32[] get-tuple-element((f32[], f32[], f32[], f32[]) %fusion), index=0
  %get-tuple-element.1788 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.1786), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_44/update_0/ResourceApplyAdam"}
  %get-tuple-element.1789 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.1786), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_44/update_0/ResourceApplyAdam"}
  %get-tuple-element.1771 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) %custom-call.107), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_42/update_0/ResourceApplyAdam"}
  %get-tuple-element.1772 = f32[768,768]{1,0} get-tuple-element((f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768,768]{1,0}) %custom-call.107), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_42/update_0/ResourceApplyAdam"}
  %get-tuple-element.1123 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.106), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_43/update_0/ResourceApplyAdam"}
  %get-tuple-element.1124 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.106), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_43/update_0/ResourceApplyAdam"}
  %get-tuple-element.1158 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.87), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_35/update_0/ResourceApplyAdam"}
  %get-tuple-element.1159 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.87), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_35/update_0/ResourceApplyAdam"}
  %get-tuple-element.1163 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.88), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_36/update_0/ResourceApplyAdam"}
  %get-tuple-element.1164 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.88), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_36/update_0/ResourceApplyAdam"}
  %get-tuple-element.1241 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) %custom-call.86), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_33/update_0/ResourceApplyAdam"}
  %get-tuple-element.1242 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) %custom-call.86), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_33/update_0/ResourceApplyAdam"}
  %get-tuple-element.1185 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.84), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_34/update_0/ResourceApplyAdam"}
  %get-tuple-element.1186 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.84), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_34/update_0/ResourceApplyAdam"}
  %get-tuple-element.1233 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) %custom-call.85), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_31/update_0/ResourceApplyAdam"}
  %get-tuple-element.1234 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) %custom-call.85), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_31/update_0/ResourceApplyAdam"}
  %get-tuple-element.1222 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) %custom-call.1220), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_32/update_0/ResourceApplyAdam"}
  %get-tuple-element.1223 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) %custom-call.1220), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_32/update_0/ResourceApplyAdam"}
  %get-tuple-element.1265 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.100), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_29/update_0/ResourceApplyAdam"}
  %get-tuple-element.1266 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.100), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_29/update_0/ResourceApplyAdam"}
  %get-tuple-element.1270 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.101), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_30/update_0/ResourceApplyAdam"}
  %get-tuple-element.1271 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.101), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_30/update_0/ResourceApplyAdam"}
  %get-tuple-element.1763 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %custom-call.1761), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_27/update_0/ResourceApplyAdam"}
  %get-tuple-element.1764 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %custom-call.1761), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_27/update_0/ResourceApplyAdam"}
  %get-tuple-element.1292 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.89), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_28/update_0/ResourceApplyAdam"}
  %get-tuple-element.1293 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.89), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_28/update_0/ResourceApplyAdam"}
  %get-tuple-element.1755 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1753), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_25/update_0/ResourceApplyAdam"}
  %get-tuple-element.1756 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1753), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_25/update_0/ResourceApplyAdam"}
  %get-tuple-element.1389 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1387), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_26/update_0/ResourceApplyAdam"}
  %get-tuple-element.1390 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1387), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_26/update_0/ResourceApplyAdam"}
  %get-tuple-element.1342 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1340), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_23/update_0/ResourceApplyAdam"}
  %get-tuple-element.1343 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1340), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_23/update_0/ResourceApplyAdam"}
  %get-tuple-element.1332 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1330), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_24/update_0/ResourceApplyAdam"}
  %get-tuple-element.1333 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1330), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_24/update_0/ResourceApplyAdam"}
  %get-tuple-element.1372 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1370), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_21/update_0/ResourceApplyAdam"}
  %get-tuple-element.1373 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1370), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_21/update_0/ResourceApplyAdam"}
  %get-tuple-element.1362 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1360), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_22/update_0/ResourceApplyAdam"}
  %get-tuple-element.1363 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1360), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_22/update_0/ResourceApplyAdam"}
  %get-tuple-element.1418 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.98), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_19/update_0/ResourceApplyAdam"}
  %get-tuple-element.1419 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.98), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_19/update_0/ResourceApplyAdam"}
  %get-tuple-element.1423 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.99), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_20/update_0/ResourceApplyAdam"}
  %get-tuple-element.1424 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.99), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_20/update_0/ResourceApplyAdam"}
  %get-tuple-element.1501 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) %custom-call.92), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_17/update_0/ResourceApplyAdam"}
  %get-tuple-element.1502 = f32[3072,768]{1,0} get-tuple-element((f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}) %custom-call.92), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_17/update_0/ResourceApplyAdam"}
  %get-tuple-element.1445 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.90), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_18/update_0/ResourceApplyAdam"}
  %get-tuple-element.1446 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.90), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_18/update_0/ResourceApplyAdam"}
  %get-tuple-element.1493 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) %custom-call.91), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_15/update_0/ResourceApplyAdam"}
  %get-tuple-element.1494 = f32[768,3072]{1,0} get-tuple-element((f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}) %custom-call.91), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_15/update_0/ResourceApplyAdam"}
  %get-tuple-element.1482 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) %custom-call.1480), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_16/update_0/ResourceApplyAdam"}
  %get-tuple-element.1483 = f32[3072]{0} get-tuple-element((f32[3072]{0}, f32[3072]{0}, f32[3072]{0}) %custom-call.1480), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_16/update_0/ResourceApplyAdam"}
  %get-tuple-element.1525 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.96), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_13/update_0/ResourceApplyAdam"}
  %get-tuple-element.1526 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.96), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_13/update_0/ResourceApplyAdam"}
  %get-tuple-element.1530 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.97), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_14/update_0/ResourceApplyAdam"}
  %get-tuple-element.1531 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.97), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_14/update_0/ResourceApplyAdam"}
  %get-tuple-element.1747 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %custom-call.1745), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_11/update_0/ResourceApplyAdam"}
  %get-tuple-element.1748 = f32[12,64,768]{2,1,0} get-tuple-element((f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}) %custom-call.1745), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_11/update_0/ResourceApplyAdam"}
  %get-tuple-element.1552 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.93), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_12/update_0/ResourceApplyAdam"}
  %get-tuple-element.1553 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.93), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_12/update_0/ResourceApplyAdam"}
  %get-tuple-element.1649 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1647), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_10/update_0/ResourceApplyAdam"}
  %get-tuple-element.1650 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1647), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_10/update_0/ResourceApplyAdam"}
  %get-tuple-element.1739 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1737), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_9/update_0/ResourceApplyAdam"}
  %get-tuple-element.1740 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1737), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_9/update_0/ResourceApplyAdam"}
  %get-tuple-element.1602 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1600), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_7/update_0/ResourceApplyAdam"}
  %get-tuple-element.1603 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1600), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_7/update_0/ResourceApplyAdam"}
  %get-tuple-element.1592 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1590), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_8/update_0/ResourceApplyAdam"}
  %get-tuple-element.1593 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1590), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_8/update_0/ResourceApplyAdam"}
  %get-tuple-element.1632 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1630), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_5/update_0/ResourceApplyAdam"}
  %get-tuple-element.1633 = f32[768,12,64]{2,1,0} get-tuple-element((f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}) %custom-call.1630), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_5/update_0/ResourceApplyAdam"}
  %get-tuple-element.1622 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1620), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_6/update_0/ResourceApplyAdam"}
  %get-tuple-element.1623 = f32[12,64]{1,0} get-tuple-element((f32[12,64]{1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}) %custom-call.1620), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_6/update_0/ResourceApplyAdam"}
  %get-tuple-element.1682 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.94), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_3/update_0/ResourceApplyAdam"}
  %get-tuple-element.1683 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.94), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_3/update_0/ResourceApplyAdam"}
  %get-tuple-element.1687 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.95), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_4/update_0/ResourceApplyAdam"}
  %get-tuple-element.1688 = f32[768]{0} get-tuple-element((f32[768]{0}, f32[768]{0}, f32[768]{0}) %custom-call.95), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_4/update_0/ResourceApplyAdam"}
  %get-tuple-element.1720 = f32[2,768]{1,0} get-tuple-element((f32[2,768]{1,0}, f32[2,768]{1,0}, f32[2,768]{1,0}) %custom-call.102), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_2/update_0/ResourceApplyAdam"}
  %get-tuple-element.1721 = f32[2,768]{1,0} get-tuple-element((f32[2,768]{1,0}, f32[2,768]{1,0}, f32[2,768]{1,0}) %custom-call.102), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_2/update_0/ResourceApplyAdam"}
  %get-tuple-element.1811 = f32[30522,768]{1,0} get-tuple-element((f32[30522,768]{1,0}, f32[30522,768]{1,0}, f32[30522,768]{1,0}) %custom-call.1809), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update/update_0/ResourceApplyAdam"}
  %get-tuple-element.1812 = f32[30522,768]{1,0} get-tuple-element((f32[30522,768]{1,0}, f32[30522,768]{1,0}, f32[30522,768]{1,0}) %custom-call.1809), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update/update_0/ResourceApplyAdam"}
  %get-tuple-element.1711 = f32[512,768]{1,0} get-tuple-element((f32[512,768]{1,0}, f32[512,768]{1,0}, f32[512,768]{1,0}) %custom-call.1709), index=1, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_1/update_0/ResourceApplyAdam"}
  %get-tuple-element.1712 = f32[512,768]{1,0} get-tuple-element((f32[512,768]{1,0}, f32[512,768]{1,0}, f32[512,768]{1,0}) %custom-call.1709), index=2, metadata={op_type="ResourceApplyAdam" op_name="Adam/Adam/update_1/update_0/ResourceApplyAdam"}
  ROOT %tuple.27 = (f32[], f32[768]{0}, f32[768,768]{1,0}, f32[768]{0}, f32[3072]{0}, f32[768,3072]{1,0}, f32[3072,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[768]{0}, f32[3072]{0}, f32[768,3072]{1,0}, f32[3072,768]{1,0}, f32[768]{0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[512,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,12,64]{2,1,0}, f32[12,64,768]{2,1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,12,64]{2,1,0}, f32[12,64,768]{2,1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[2,768]{1,0}, f32[2]{0}, f32[768,2]{1,0}, f32[30522]{0}, f32[30522,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768,768]{1,0}, f32[], f32[], f32[2]{0}, f32[2]{0}, f32[768,2]{1,0}, f32[768,2]{1,0}, f32[768]{0}, f32[768]{0}, f32[768,768]{1,0}, f32[768,768]{1,0}, f32[], f32[30522]{0}, f32[30522]{0}, f32[], f32[768]{0}, f32[768]{0}, f32[], f32[768]{0}, f32[768]{0}, f32[768,768]{1,0}, f32[768,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[3072]{0}, f32[3072]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[768]{0}, f32[768]{0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[3072,768]{1,0}, f32[3072,768]{1,0}, f32[768]{0}, f32[768]{0}, f32[768,3072]{1,0}, f32[768,3072]{1,0}, f32[3072]{0}, f32[3072]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[12,64,768]{2,1,0}, f32[12,64,768]{2,1,0}, f32[768]{0}, f32[768]{0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[768,12,64]{2,1,0}, f32[768,12,64]{2,1,0}, f32[12,64]{1,0}, f32[12,64]{1,0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[768]{0}, f32[2,768]{1,0}, f32[2,768]{1,0}, f32[30522,768]{1,0}, f32[30522,768]{1,0}, f32[512,768]{1,0}, f32[512,768]{1,0}) tuple(f32[] %copy.118, f32[768]{0} %get-tuple-element.825, f32[768,768]{1,0} %get-tuple-element.840, f32[768]{0} %get-tuple-element.1184, f32[3072]{0} %get-tuple-element.1221, f32[768,3072]{1,0} %get-tuple-element.1232, f32[3072,768]{1,0} %get-tuple-element.1240, f32[768]{0} %get-tuple-element.101, f32[768]{0} %get-tuple-element.105, f32[768]{0} %get-tuple-element.1157, f32[768]{0} %get-tuple-element.1162, f32[768]{0} %get-tuple-element.1291, f32[12,64]{1,0} %get-tuple-element.1331, f32[768,12,64]{2,1,0} %get-tuple-element.1341, f32[12,64]{1,0} %get-tuple-element.1361, f32[768,12,64]{2,1,0} %get-tuple-element.1371, f32[12,64]{1,0} %get-tuple-element.1388, f32[768]{0} %get-tuple-element.1444, f32[3072]{0} %get-tuple-element.1481, f32[768,3072]{1,0} %get-tuple-element.1492, f32[3072,768]{1,0} %get-tuple-element.1500, f32[768]{0} %get-tuple-element.1551, f32[12,64]{1,0} %get-tuple-element.1591, f32[768,12,64]{2,1,0} %get-tuple-element.1601, f32[12,64]{1,0} %get-tuple-element.1621, f32[768,12,64]{2,1,0} %get-tuple-element.1631, f32[12,64]{1,0} %get-tuple-element.1648, f32[512,768]{1,0} %get-tuple-element.1710, f32[768]{0} %get-tuple-element.1681, f32[768]{0} %get-tuple-element.1686, f32[768]{0} %get-tuple-element.102, f32[768]{0} %get-tuple-element.106, f32[768,12,64]{2,1,0} %get-tuple-element.1738, f32[12,64,768]{2,1,0} %get-tuple-element.1746, f32[768]{0} %get-tuple-element.1524, f32[768]{0} %get-tuple-element.1529, f32[768]{0} %get-tuple-element.103, f32[768]{0} %get-tuple-element.104, f32[768]{0} %get-tuple-element.1417, f32[768]{0} %get-tuple-element.1422, f32[768]{0} %get-tuple-element.99, f32[768]{0} %get-tuple-element.108, f32[768,12,64]{2,1,0} %get-tuple-element.1754, f32[12,64,768]{2,1,0} %get-tuple-element.1762, f32[768]{0} %get-tuple-element.100, f32[768]{0} %get-tuple-element.107, f32[768]{0} %get-tuple-element.1264, f32[768]{0} %get-tuple-element.1269, f32[2,768]{1,0} %get-tuple-element.1719, f32[2]{0} %get-tuple-element.802, f32[768,2]{1,0} %get-tuple-element.848, f32[30522]{0} %get-tuple-element.1028, f32[30522,768]{1,0} %get-tuple-element.1810, f32[768]{0} %get-tuple-element.1801, f32[768]{0} %get-tuple-element.1787, f32[768]{0} %get-tuple-element.1122, f32[768,768]{1,0} %get-tuple-element.1770, f32[] %get-tuple-element.78, f32[] %fusion.8, f32[2]{0} %get-tuple-element.803, f32[2]{0} %get-tuple-element.804, f32[768,2]{1,0} %get-tuple-element.849, f32[768,2]{1,0} %get-tuple-element.850, f32[768]{0} %get-tuple-element.826, f32[768]{0} %get-tuple-element.827, f32[768,768]{1,0} %get-tuple-element.841, f32[768,768]{1,0} %get-tuple-element.842, f32[] %get-tuple-element.77, f32[30522]{0} %get-tuple-element.1029, f32[30522]{0} %get-tuple-element.1030, f32[] %get-tuple-element.85, f32[768]{0} %get-tuple-element.1802, f32[768]{0} %get-tuple-element.1803, f32[] %get-tuple-element.76, f32[768]{0} %get-tuple-element.1788, f32[768]{0} %get-tuple-element.1789, f32[768,768]{1,0} %get-tuple-element.1771, f32[768,768]{1,0} %get-tuple-element.1772, f32[768]{0} %get-tuple-element.1123, f32[768]{0} %get-tuple-element.1124, f32[768]{0} %get-tuple-element.1158, f32[768]{0} %get-tuple-element.1159, f32[768]{0} %get-tuple-element.1163, f32[768]{0} %get-tuple-element.1164, f32[3072,768]{1,0} %get-tuple-element.1241, f32[3072,768]{1,0} %get-tuple-element.1242, f32[768]{0} %get-tuple-element.1185, f32[768]{0} %get-tuple-element.1186, f32[768,3072]{1,0} %get-tuple-element.1233, f32[768,3072]{1,0} %get-tuple-element.1234, f32[3072]{0} %get-tuple-element.1222, f32[3072]{0} %get-tuple-element.1223, f32[768]{0} %get-tuple-element.1265, f32[768]{0} %get-tuple-element.1266, f32[768]{0} %get-tuple-element.1270, f32[768]{0} %get-tuple-element.1271, f32[12,64,768]{2,1,0} %get-tuple-element.1763, f32[12,64,768]{2,1,0} %get-tuple-element.1764, f32[768]{0} %get-tuple-element.1292, f32[768]{0} %get-tuple-element.1293, f32[768,12,64]{2,1,0} %get-tuple-element.1755, f32[768,12,64]{2,1,0} %get-tuple-element.1756, f32[12,64]{1,0} %get-tuple-element.1389, f32[12,64]{1,0} %get-tuple-element.1390, f32[768,12,64]{2,1,0} %get-tuple-element.1342, f32[768,12,64]{2,1,0} %get-tuple-element.1343, f32[12,64]{1,0} %get-tuple-element.1332, f32[12,64]{1,0} %get-tuple-element.1333, f32[768,12,64]{2,1,0} %get-tuple-element.1372, f32[768,12,64]{2,1,0} %get-tuple-element.1373, f32[12,64]{1,0} %get-tuple-element.1362, f32[12,64]{1,0} %get-tuple-element.1363, f32[768]{0} %get-tuple-element.1418, f32[768]{0} %get-tuple-element.1419, f32[768]{0} %get-tuple-element.1423, f32[768]{0} %get-tuple-element.1424, f32[3072,768]{1,0} %get-tuple-element.1501, f32[3072,768]{1,0} %get-tuple-element.1502, f32[768]{0} %get-tuple-element.1445, f32[768]{0} %get-tuple-element.1446, f32[768,3072]{1,0} %get-tuple-element.1493, f32[768,3072]{1,0} %get-tuple-element.1494, f32[3072]{0} %get-tuple-element.1482, f32[3072]{0} %get-tuple-element.1483, f32[768]{0} %get-tuple-element.1525, f32[768]{0} %get-tuple-element.1526, f32[768]{0} %get-tuple-element.1530, f32[768]{0} %get-tuple-element.1531, f32[12,64,768]{2,1,0} %get-tuple-element.1747, f32[12,64,768]{2,1,0} %get-tuple-element.1748, f32[768]{0} %get-tuple-element.1552, f32[768]{0} %get-tuple-element.1553, f32[12,64]{1,0} %get-tuple-element.1649, f32[12,64]{1,0} %get-tuple-element.1650, f32[768,12,64]{2,1,0} %get-tuple-element.1739, f32[768,12,64]{2,1,0} %get-tuple-element.1740, f32[768,12,64]{2,1,0} %get-tuple-element.1602, f32[768,12,64]{2,1,0} %get-tuple-element.1603, f32[12,64]{1,0} %get-tuple-element.1592, f32[12,64]{1,0} %get-tuple-element.1593, f32[768,12,64]{2,1,0} %get-tuple-element.1632, f32[768,12,64]{2,1,0} %get-tuple-element.1633, f32[12,64]{1,0} %get-tuple-element.1622, f32[12,64]{1,0} %get-tuple-element.1623, f32[768]{0} %get-tuple-element.1682, f32[768]{0} %get-tuple-element.1683, f32[768]{0} %get-tuple-element.1687, f32[768]{0} %get-tuple-element.1688, f32[2,768]{1,0} %get-tuple-element.1720, f32[2,768]{1,0} %get-tuple-element.1721, f32[30522,768]{1,0} %get-tuple-element.1811, f32[30522,768]{1,0} %get-tuple-element.1812, f32[512,768]{1,0} %get-tuple-element.1711, f32[512,768]{1,0} %get-tuple-element.1712)
}

